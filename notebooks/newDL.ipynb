{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a00b0acc-8ada-404c-bd20-a65e77bd9694",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['incident_energies', 'showers']>\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import torch\n",
    "\n",
    "# Replace 'your_file.h5' with the path to your HDF5 file\n",
    "file_path = '/raid/javier/Datasets/CaloVAE/data/atlas_dataset2and3_scaled/dataset_2_1.hdf5'\n",
    "data_dict = {}\n",
    "# Open the HDF5 file\n",
    "with h5py.File(file_path, 'r') as file:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % file.keys())\n",
    "    for key in file.keys():\n",
    "        data_dict[key] = torch.tensor(file[key][:])\n",
    "        \n",
    "    a_group_key = list(file.keys())[0]\n",
    "\n",
    "    # Get the data\n",
    "    data = list(file[a_group_key])\n",
    "\n",
    "    # If you know the specific dataset you want to access, you can use\n",
    "    # dataset = file['dataset_name']  # 'dataset_name' is the name of your dataset\n",
    "    # data = dataset[:]  # Read the entire dataset\n",
    "\n",
    "    # Now, 'data' is a NumPy array containing the dataset's contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4de239ad-e53c-49fb-99f6-57380d8ceca1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/javier/anaconda3/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config.yaml': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import hydra\n",
    "from hydra.utils import instantiate\n",
    "from hydra import initialize, compose\n",
    "\n",
    "\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(version_base=None, config_path=\"../configs\")\n",
    "\n",
    "config=compose(config_name=\"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "78a22374-a808-4824-a4f8-beea01d6454e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataManager(object):\n",
    "    def __init__(self,train_loader=None,test_loader=None,val_loader=None, cfg=None):\n",
    "        self._config=cfg\n",
    "        \n",
    "        self._train_loader=train_loader\n",
    "        self._test_loader=test_loader\n",
    "        self._val_loader=val_loader\n",
    "\n",
    "        #this is a list of tensor.shape tuples (i.e.[(28,28)] for MNIST) \n",
    "        self._input_dimensions=None\n",
    "        #list of flattened tensor.shape tuples (i.e. [784] for mnist)\n",
    "        self._flat_input_sizes=None \n",
    "\n",
    "        self._train_dataset_means=None\n",
    "        \n",
    "        # Variables to be used in the scaling and inverse scaling\n",
    "        self._amin_array = None\n",
    "        self._transformer = None\n",
    "        # return\n",
    "        self.particle_type = [self._config.data.particle_type]\n",
    "        self.frac_train_dataset=self._config.data.frac_train_dataset\n",
    "        self.frac_test_dataset=self._config.data.frac_test_dataset\n",
    "        self.load_dataset_directories()\n",
    "        \n",
    "        \n",
    "    def load_dataset_directories(self):\n",
    "        if self._config.data.data_type.lower()==\"atlas\":\n",
    "            self.inFiles={\n",
    "            'photon1':    self._config.data.atlas_input_photon1,\n",
    "                'photonEn0':    self._config.data.atlas_input_photonEn0,\n",
    "                'photonEn1':    self._config.data.atlas_input_photonEn1,\n",
    "                'photonEn2':    self._config.data.atlas_input_photonEn2,\n",
    "                'photonEn3':    self._config.data.atlas_input_photonEn3,\n",
    "                'photonEn4':    self._config.data.atlas_input_photonEn4,\n",
    "                'photonEn5':    self._config.data.atlas_input_photonEn5,\n",
    "                'photonEn6':    self._config.data.atlas_input_photonEn6,\n",
    "                'photonEn7':    self._config.data.atlas_input_photonEn7,\n",
    "            'pion1':   self._config.data.atlas_input_pion1,\n",
    "                'pionEn0':   self._config.data.atlas_input_pionEn0,\n",
    "                'pionEn1':   self._config.data.atlas_input_pionEn1,\n",
    "                'pionEn2':   self._config.data.atlas_input_pionEn2,\n",
    "                'pionEn3':   self._config.data.atlas_input_pionEn3,\n",
    "                'pionEn4':   self._config.data.atlas_input_pionEn4,\n",
    "                'pionEn5':   self._config.data.atlas_input_pionEn5,\n",
    "                'pionEn6':   self._config.data.atlas_input_pionEn6,\n",
    "                'pionEn7':   self._config.data.atlas_input_pionEn7,\n",
    "            'electron-ds2': self._config.data.atlas_input_electron,\n",
    "        }\n",
    "        \n",
    "    def load_data(self):\n",
    "        \n",
    "        #read in all input files for all jet types and layers\n",
    "        datastore={}\n",
    "        for key,fpath in self.inFiles.items(): \n",
    "            if key in self.particle_type: \n",
    "                with h5py.File(fpath, 'r') as file:\n",
    "                    # List all groups\n",
    "                    # print(\"Keys: %s\" % file.keys())\n",
    "                    for other_key in file.keys():\n",
    "                        datastore[other_key] = torch.tensor(file[other_key][:])\n",
    "                \n",
    "                # in_data=h5py.File(fpath,'r')\n",
    "                # #for each particle_type, create a Container instance for our needs   \n",
    "                # dataStore[key]=CaloImageContainer(  particle_type=key,\n",
    "                #                                     input_data=in_data,\n",
    "                #                                     layer_subset=layer_subset)\n",
    "                # #convert image dataframes to tensors and get energies\n",
    "                # dataStore[key].process_data(input_data=in_data)\n",
    "\n",
    "        assert len(self.particle_type)==1, f\"Currently one particle type at a time\\\n",
    "             can be retrieved. Requested {self.particle_type}\"\n",
    "        ptype=self.particle_type[0]\n",
    "\n",
    "        #let's split our datasets\n",
    "        #get total num evts\n",
    "        num_evts_total=datastore['showers'].shape[0]\n",
    "        print(num_evts_total)\n",
    "\n",
    "        #create a sequential list of indices\n",
    "        # idx_list = [i for i in range(0, num_evts_total)]\n",
    "\n",
    "        # compute number of split evts from fraction\n",
    "        num_evts_train = int(self.frac_train_dataset*num_evts_total)\n",
    "        num_evts_test = int(self.frac_test_dataset*num_evts_total)\n",
    "\n",
    "        #create lists of split indices\n",
    "        # train_idx_list = idx_list[:num_evts_train]\n",
    "        # test_idx_list = idx_list[num_evts_train:(num_evts_train+num_evts_test)]\n",
    "        # val_idx_list = idx_list[(num_evts_train+num_evts_test):]\n",
    "\n",
    "        # train_dataset = dataStore[ptype].create_subset(idx_list=train_idx_list, label=\"train\")\n",
    "        # test_dataset = dataStore[ptype].create_subset(idx_list=test_idx_list, label=\"test\")\n",
    "        # val_dataset = dataStore[ptype].create_subset(idx_list=val_idx_list, label=\"val\")\n",
    "        \n",
    "        self.train_dataset =  TensorDataset(datastore['showers'][:num_evts_train,:], datastore['incident_energies'][:num_evts_train,:])\n",
    "        self.test_dataset =  TensorDataset(datastore['showers'][num_evts_train:num_evts_train+num_evts_test,:], datastore['incident_energies'][num_evts_train:num_evts_train+num_evts_test,:])\n",
    "        self.val_dataset =  TensorDataset(datastore['showers'][num_evts_train+num_evts_test:,:], datastore['incident_energies'][num_evts_train+num_evts_test:,:])\n",
    "\n",
    "        return self.train_dataset, self.test_dataset, self.val_dataset\n",
    "        \n",
    "    def create_dataLoader(self):\n",
    "        assert abs(self._config.data.frac_train_dataset-1)>=0, \"Cfg option frac_train_dataset must be within (0,1]\"\n",
    "        assert abs(self._config.data.frac_test_dataset-0.99)>1.e-5, \"Cfg option frac_test_dataset must be within (0,99]. 0.01 minimum for validation set\"\n",
    "\n",
    "        self.load_data()\n",
    "                \n",
    "        #create the DataLoader for the training dataset\n",
    "        train_loader=DataLoader(   \n",
    "            self.train_dataset,\n",
    "            batch_size=self._config.engine.n_train_batch_size, \n",
    "            num_workers=self._config.num_workers,\n",
    "            shuffle=True)\n",
    "\n",
    "        #create the DataLoader for the testing/validation datasets\n",
    "        #set batch size to full test/val dataset size - limitation only by hardware\n",
    "        test_loader = DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self._config.engine.n_test_batch_size, \n",
    "            num_workers=self._config.num_workers,\n",
    "            shuffle=False)\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self._config.engine.n_valid_batch_size, \n",
    "            num_workers=self._config.num_workers,\n",
    "            shuffle=False)\n",
    "\n",
    "        # logger.info(\"{0}: {2} events, {1} batches\".format(train_loader,len(train_loader),len(train_loader.dataset)))\n",
    "        # logger.info(\"{0}: {2} events, {1} batches\".format(test_loader,len(test_loader),len(test_loader.dataset)))\n",
    "        # logger.info(\"{0}: {2} events, {1} batches\".format(val_loader,len(val_loader),len(val_loader.dataset)))\n",
    "\n",
    "        return train_loader,test_loader,val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4346b27a-39e2-4e59-8bb5-6f5ceee4e7da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataMgr = DataManager(cfg=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2a6e8516-508a-41b2-8202-600c6e2ce3c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "tr, te, va = dataMgr.create_dataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2bae56f3-744f-4cd3-b4be-09a5538bf322",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x7f34d24c7b10>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datastore['incident_energies']\n",
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "56e43a68-b76a-4e3d-a677-f4f125d90c40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1841, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.5740, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.1179, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datastore['showers'][0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d00baaf9-af64-4766-82ab-338bf438d5b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "923e653d-83c7-43d5-8af4-72dfa7a6fade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = TensorDataset(datastore['showers'], datastore['incident_energies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c43d9044-b32d-4df0-9b29-b5578e0576b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    ds,\n",
    "    batch_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01a86199-aa75-4add-8fd6-735700ef5b97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/javier/anaconda3/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:347: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "xx = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95363855-fdde-4c1b-8a9e-f9894d71df68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebea0cb-8758-4f26-aa0a-4892fd1f22cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
