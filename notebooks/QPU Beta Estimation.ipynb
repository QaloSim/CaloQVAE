{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b516566",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109b14f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Add the path to the parent directory to augment search for module\n",
    "sys.path.append(os.getcwd())\n",
    "# Add the path to the parent directory to augment search for module\n",
    "par_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "if par_dir not in sys.path:\n",
    "    sys.path.append(par_dir)\n",
    "    \n",
    "# ML imports\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# DiVAE imports\n",
    "from models.rbm.chimeraRBM import ChimeraRBM\n",
    "from models.rbm.qimeraRBM import QimeraRBM\n",
    "from models.rbm.rbm import RBM\n",
    "from models.samplers.pcd import PCD\n",
    "from models.autoencoders.gumboltCaloCRBM import GumBoltCaloCRBM\n",
    "\n",
    "from nbutils import *\n",
    "\n",
    "# DWave imports\n",
    "from dwave.system import DWaveSampler, LeapHybridSampler\n",
    "import neal\n",
    "import dimod\n",
    "\n",
    "# Extra imports for image and data processing\n",
    "from PIL import Image, ImageDraw\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b7cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(cfg):\n",
    "    model = GumBoltCaloCRBM(flat_input_size=[504],\n",
    "                            train_ds_mean=0.,\n",
    "                            activation_fct=torch.nn.ReLU(),\n",
    "                            cfg=cfg)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97495cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(config_path=\"../configs\"):\n",
    "    cfg = compose(config_name=\"config-backup-oct-7-22\")\n",
    "    # this config file uses the chimera architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b4533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_model(cfg)\n",
    "model.create_networks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6656c90b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qpu_sampler = DWaveSampler(solver={\"topology__type\":\"chimera\", \"chip_id\":\"DW_2000Q_6\"})\n",
    "aux_crbm = QimeraRBM(n_visible=model.prior._n_visible, n_hidden=model.prior._n_hidden)\n",
    "aux_crbm_sampler = PCD(batch_size=850, RBM=aux_crbm, n_gibbs_sampling_steps=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48390169",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code to use GPU. Must specify which GPU unit to use here:\n",
    "\"\"\"\n",
    "GPU_NUM = 5\n",
    "torch.cuda.set_device(5)\n",
    "print(\"Using GPU {0}\".format(torch.cuda.current_device()))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609efe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send sampler to GPU and define ZERO and MINUS_ONE in GPU\n",
    "aux_crbm_sampler = aux_crbm_sampler.to(device)\n",
    "ZERO = torch.tensor(0., dtype=torch.float).to(device)\n",
    "MINUS_ONE = torch.tensor(-1., dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f55c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_ising(n_vis, n_hid):\n",
    "    \"\"\"\n",
    "    This function randomly initializes an Ising model with random J and h in the given ranges\n",
    "    Inputs: nunmbers of visible and hidden nodes\n",
    "    Output: Ising Weights and Biases\n",
    "    * UPDATE: J is now drawn from a Gaussian distribution instead of a Uniform distribution\n",
    "    \"\"\"\n",
    "    #wlim = [-0.5,0.5]\n",
    "    #ising_weights = torch.nn.Parameter((wlim[1]-wlim[0])*torch.rand(n_vis, n_hid) + wlim[0], requires_grad=False) \n",
    "    ising_weights = torch.normal(0, 0.2, size=(n_hid, n_hid))\n",
    "    hlim = [-2,2]\n",
    "    ising_vbias = torch.nn.Parameter((hlim[1]-hlim[0])*torch.rand(n_vis)+hlim[0], requires_grad=False)\n",
    "    ising_hbias = torch.nn.Parameter((hlim[1]-hlim[0])*torch.rand(n_hid)+hlim[0], requires_grad=False)\n",
    "    return ising_weights, ising_vbias, ising_hbias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2be1916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run everytime you want to get new weights and biases for the forward beta estimation procedure\n",
    "ising_weights, ising_vbias, ising_hbias = initialize_ising(1000,1000)\n",
    "\n",
    "# Check to ensure J is in the valid DWAVE coupling range\n",
    "min_J = np.min(ising_weights.cpu().detach().numpy())\n",
    "max_J = np.max(ising_weights.cpu().detach().numpy())\n",
    "print(\"J is between: {0} and {1}\".format(min_J,max_J))\n",
    "\n",
    "# the initialization below is necessary to run the function in\n",
    "# the following block\n",
    "dwave_energies = 0\n",
    "dwave_samples = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502844c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_fixed_qpu(num_iterations:int=10, lr:float=0.01, beta_init=10., n_reads:int=100, qpu_sampler=None, \n",
    "                   aux_crbm_sampler=None, ising_weights=None, ising_vbias=None, ising_hbias=None, \n",
    "                   dwave_energies=0, dwave_samples=0, newDwave = 0):\n",
    "    \"\"\"Estimate the temperature associated with a given QPU configuration\n",
    "    \n",
    "    :param num_iterations (int) : Number of iterations\n",
    "    :param lr (float) : Learning rate\n",
    "    :param beta_init (float) : Initial estimate of the QPU temperature\n",
    "    :param n_qpu_samples (int) : Number of QPU samples\n",
    "    :param qpu_sampler\n",
    "    \n",
    "    \"\"\"\n",
    "    assert qpu_sampler is not None\n",
    "    assert aux_crbm_sampler is not None\n",
    "    \n",
    "    beta = beta_init\n",
    "    betas = [beta]\n",
    "    \n",
    "    # Auxiliary scaled RBM\n",
    "    aux_crbm = aux_crbm_sampler.rbm\n",
    "    aux_crbm_edgelist = aux_crbm.pruned_edge_list\n",
    "    \n",
    "    # Setup QPU control variables\n",
    "    n_vis = len(aux_crbm.visible_qubit_idxs)\n",
    "    n_hid = len(aux_crbm.visible_qubit_idxs)\n",
    "    print(\"n_vis is {0} and n_hid is {1}\\n\".format(n_vis,n_hid))\n",
    "    qubit_idxs = aux_crbm.visible_qubit_idxs+aux_crbm.hidden_qubit_idxs\n",
    "    \n",
    "    visible_idx_map = {visible_qubit_idx:i for i, visible_qubit_idx in enumerate(aux_crbm.visible_qubit_idxs)}\n",
    "    hidden_idx_map = {hidden_qubit_idx:i for i, hidden_qubit_idx in enumerate(aux_crbm.hidden_qubit_idxs)}\n",
    "    \"\"\"\n",
    "    Send Ising weights and biases to GPU is available\n",
    "    \"\"\"\n",
    "    ising_weights = ising_weights.to(device)\n",
    "    ising_vbias = ising_vbias.to(device)\n",
    "    ising_hbias = ising_hbias.to(device)    \n",
    "    \"\"\"\n",
    "    Also note that we are using a Chimera RBM architecture. Thus we have to transform \n",
    "    our Ising model by modifying the couplings to get the Chimera RBM architecture.\n",
    "    We do this by masking the corresponding RBM weights of our ising_model\n",
    "    and then reverting the rbm model back to the ising model. \n",
    "    \"\"\"\n",
    "    aux_crbm_weights = 4.*ising_weights # weights of the corresponding RBM\n",
    "    aux_crbm_weights = aux_crbm_weights*aux_crbm.weights_mask # mask to get Chimera RBM architecture\n",
    "    ising_weights = aux_crbm_weights*0.25 # revert back to ising model\n",
    "    \n",
    "    dwave_weights_np = -ising_weights.detach().cpu().numpy()\n",
    "    print(\"J range = ({0}, {1})\".format(np.min(dwave_weights_np),\n",
    "                                        np.max(dwave_weights_np)))\n",
    "    \n",
    "    vbias_list = list(ising_vbias.detach().cpu().numpy())\n",
    "    hbias_list = list(ising_hbias.detach().cpu().numpy())\n",
    "    hVis = {v_qubit_idx:-vbias_list[visible_idx_map[v_qubit_idx]] for v_qubit_idx in aux_crbm.visible_qubit_idxs}\n",
    "    hHid = {h_qubit_idx:-hbias_list[hidden_idx_map[h_qubit_idx]] for h_qubit_idx in aux_crbm.hidden_qubit_idxs}\n",
    "    h = {**hVis,**hHid}\n",
    "    J = {}\n",
    "    for edge in aux_crbm_edgelist:\n",
    "        if edge[0] in aux_crbm.visible_qubit_idxs:\n",
    "            J[edge] = dwave_weights_np[visible_idx_map[edge[0]]][hidden_idx_map[edge[1]]]\n",
    "        else:\n",
    "            J[edge] = dwave_weights_np[visible_idx_map[edge[1]]][hidden_idx_map[edge[0]]]\n",
    "    \n",
    "    if (newDwave==1):\n",
    "        response = qpu_sampler.sample_ising(h, J, num_reads=n_reads, auto_scale=False)\n",
    "        dwave_samples, dwave_energies = batch_dwave_samples(response, qubit_idxs) \n",
    "    dwave_samples = torch.tensor(dwave_samples, dtype=torch.float)  \n",
    "    \n",
    "    dwave_energy_exp = np.mean(dwave_energies, axis=0) # we use energies returned by DWAVE API to get expectation\n",
    "\n",
    "    print(\"dwave_energy_exp : {0}\".format(dwave_energy_exp))\n",
    "    \n",
    "    \"\"\"\n",
    "    Recall that in each iteration of the beta_eff* estimation we change the RBM parameters. However\n",
    "    we still sample from the same RBM which has been initialized above. Hence, we keep track \n",
    "    of the original parameters by the following:\n",
    "    \"\"\"\n",
    "\n",
    "    rbm_orig_vis = torch.nn.Parameter(2.*(ising_vbias - torch.sum(ising_weights, dim=1)), requires_grad=False).to(device)\n",
    "    rbm_orig_hid = torch.nn.Parameter(2.*(ising_hbias - torch.sum(ising_weights, dim=0)), requires_grad=False).to(device)\n",
    "    rbm_orig_weights = ising_weights*4\n",
    " \n",
    "    classical_energies = [0]*num_iterations # classical energies is a 2D array containing \n",
    "                                            # classical energy distributions computed at each iteration\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        aux_crbm.weights = rbm_orig_weights*beta\n",
    "        aux_crbm._visible_bias = torch.nn.Parameter(rbm_orig_vis*beta, requires_grad=False) \n",
    "        aux_crbm._hidden_bias = torch.nn.Parameter(rbm_orig_hid*beta, requires_grad=False)\n",
    "        \n",
    "        \n",
    "        aux_crbm_sampler.rbm = aux_crbm\n",
    "        \n",
    "        aux_crbm_vis, aux_crbm_hid = aux_crbm_sampler.block_gibbs_sampling()\n",
    "        \n",
    "        \"\"\"\n",
    "        Finding energy using RBM method\n",
    "        The code below computes the same ising energy energy expectation using RBM weights and biases.\n",
    "        rbm_energy_exp is basically the same as aux_crbm_energy_exp and thus has been commented out\n",
    "        \"\"\"\n",
    "        # ising_energy_rbm_params = ising_energy_rbm(rbm_orig_weights, rbm_orig_vis, rbm_orig_hid, aux_crbm_vis, aux_crbm_hid)\n",
    "        # rbm_energy_exp = torch.mean(ising_energy_rbm_params, axis=0)        \n",
    "        \n",
    "        aux_crbm_vis = torch.where(aux_crbm_vis == ZERO, MINUS_ONE, aux_crbm_vis)\n",
    "        aux_crbm_hid = torch.where(aux_crbm_hid == ZERO, MINUS_ONE, aux_crbm_hid)\n",
    "        \n",
    "        aux_crbm_energy_exp = ising_energies_exp(ising_weights, ising_vbias, ising_hbias, aux_crbm_vis, aux_crbm_hid)\n",
    "        aux_crbm_energy_exps = -aux_crbm_energy_exp.detach().cpu().numpy()\n",
    "        classical_energies[i] = aux_crbm_energy_exps\n",
    "        aux_crbm_energy_exp = -torch.mean(aux_crbm_energy_exp, axis=0)\n",
    "        \n",
    "        print(\"aux_crbm_energy_exp : {0}, beta : {1} and epoch {2}\".format(aux_crbm_energy_exp, beta, i+1))\n",
    "        beta = beta - lr*(-float(aux_crbm_energy_exp)+float(dwave_energy_exp))\n",
    "        betas.append(beta)\n",
    "        \n",
    "    return betas, dwave_energies, aux_crbm_energy_exps, classical_energies, dwave_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597dad51",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# newDwave = 1 means we are getting new dwave samples and energies\n",
    "# newDwave = 0 means we are using the previously computed dwave samples and energies\n",
    "betas, dwave_energies,  aux_crbm_energy_exps, classical_energies, dwave_samples = beta_fixed_qpu(num_iterations=1,\n",
    "                       lr=0.095,\n",
    "                       beta_init=9.6,\n",
    "                       n_reads=850,\n",
    "                       qpu_sampler=qpu_sampler,\n",
    "                       aux_crbm_sampler=aux_crbm_sampler,\n",
    "                       ising_weights = ising_weights,\n",
    "                       ising_vbias = ising_vbias,\n",
    "                       ising_hbias = ising_hbias,\n",
    "                       dwave_energies = dwave_energies,\n",
    "                       dwave_samples = dwave_samples,\n",
    "                       newDwave = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2b0329",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_betas(betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b775db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(classical_energies)):\n",
    "    plot_energies(dwave_energies, classical_energies[i], 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2362c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will give dwave energy distributions in 58 second intervals \n",
    "def getDwavePlots(num_iter, n_reads, ising_weights, ising_vbias, ising_hbias):\n",
    "    dwave_array = [0]*num_iter\n",
    "    for i in range(num_iter):\n",
    "        betas, dwave_energies,  aux_crbm_energy_exps, ising_weights, J, h, qubit_idxs, classical_energies, dwave_samples = beta_fixed_qpu(num_iterations=1,\n",
    "                               lr=0.05,\n",
    "                               beta_init=9.27, \n",
    "                               n_reads=n_reads,\n",
    "                               qpu_sampler=qpu_sampler,\n",
    "                               aux_crbm_sampler=aux_crbm_sampler,\n",
    "                               ising_weights = ising_weights,\n",
    "                               ising_vbias = ising_vbias,\n",
    "                               ising_hbias = ising_hbias,\n",
    "                               dwave_energies = 0,\n",
    "                               dwave_samples = 0,\n",
    "                               newDwave = 1) \n",
    "        dwave_array[i] = dwave_energies\n",
    "        if (i!=num_iter-1):\n",
    "            time.sleep(58)\n",
    "    return dwave_array\n",
    "#dwave_array = getDwavePlots(4,2000, ising_weights, ising_vbias, ising_hbias) # uncomment to run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21742628",
   "metadata": {},
   "source": [
    "So far we have been trying to fit classical RBM parameters to model\n",
    "DWAVE distribution. This was done by scaling the classical RBM parameters\n",
    "\n",
    "Now going forward we aim to fit dwave parameters to model a classical RBM distribution.\n",
    "This is done by scaling the QPU control parameters repeatedly to iteratively computer a beta\n",
    "according to equation $\\textbf{C3}$ of the \"Advantage...\" paper. Some discussion regarding the\n",
    "results obtained using this reverse procedure is given below:\n",
    "\n",
    "It was noticed that when $J\\sim N(0,std=0.2)$ we generally had $J\\in[-1,1]$. In such a case, $\\beta_{eff}^{*}=10.4$ seems optimal.\n",
    "\n",
    "Furthermore, when $J = U(-1,1)$, we had $\\beta_{eff}^{*}=9.9$. In this case many more qubit pars have higher couplings as opposed to the $J\\approx N(0,std=0.2)$ case. \n",
    "\n",
    "Additionally, when $J= U(-1.5,1)$ (i.e. using the full range of J), beta still converged to $\\beta_{eff}^{*}=9.6$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4027569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_reverse(num_iterations:int=10, lr:float=0.01, beta_init=10., n_reads:int=100, qpu_sampler=None, \n",
    "                 aux_crbm_sampler=None, ising_weights=None, ising_vbias=None, ising_hbias=None):\n",
    "    \"\"\"Estimate the temperature associated with a given QPU configuration\n",
    "    \n",
    "    :param num_iterations (int) : Number of iterations\n",
    "    :param lr (float) : Learning rate\n",
    "    :param beta_init (float) : Initial estimate of the QPU temperature\n",
    "    :param n_qpu_samples (int) : Number of QPU samples\n",
    "    :param qpu_sampler\n",
    "    \n",
    "    \"\"\"\n",
    "    assert qpu_sampler is not None\n",
    "    assert aux_crbm_sampler is not None\n",
    "    \n",
    "    beta = beta_init\n",
    "    betas = [beta]\n",
    "    \n",
    "    # Auxiliary scaled RBM\n",
    "    aux_crbm = aux_crbm_sampler.rbm\n",
    "    aux_crbm_edgelist = aux_crbm.pruned_edge_list\n",
    "    \n",
    "    # Setup QPU control variables\n",
    "    n_vis = len(aux_crbm.visible_qubit_idxs)\n",
    "    n_hid = len(aux_crbm.visible_qubit_idxs)\n",
    "    print(\"n_vis is {0} and n_hid is {1}\\n\".format(n_vis,n_hid))\n",
    "    qubit_idxs = aux_crbm.visible_qubit_idxs+aux_crbm.hidden_qubit_idxs\n",
    "    \n",
    "    visible_idx_map = {visible_qubit_idx:i for i, visible_qubit_idx in enumerate(aux_crbm.visible_qubit_idxs)}\n",
    "    hidden_idx_map = {hidden_qubit_idx:i for i, hidden_qubit_idx in enumerate(aux_crbm.hidden_qubit_idxs)}\n",
    "    \"\"\"\n",
    "    Send Ising weights and biases to GPU if available\n",
    "    \"\"\"\n",
    "    ising_weights = ising_weights.to(device)\n",
    "    ising_vbias = ising_vbias.to(device)\n",
    "    ising_hbias = ising_hbias.to(device) \n",
    "    \"\"\"\n",
    "    Also note that we are using a Chimera RBM architecture. Thus we have to transform \n",
    "    our Ising model by modifying the couplings to get the Chimera RBM architecture.\n",
    "    We do this by masking the corresponding RBM weights of our ising_model\n",
    "    and then reverting the rbm model back to the ising model. \n",
    "    \"\"\"\n",
    "    aux_crbm_weights = 4.*ising_weights # weights of the corresponding RBM\n",
    "    aux_crbm_weights = aux_crbm_weights*aux_crbm.weights_mask # mask to get Chimera RBM architecture\n",
    "    ising_weights = aux_crbm_weights*0.25 # revert back to ising model\n",
    " \n",
    "    dwave_weights_np = -ising_weights.detach().cpu().numpy()\n",
    "    print(\"J range = ({0}, {1})\".format(np.min(dwave_weights_np),\n",
    "                                        np.max(dwave_weights_np)))\n",
    "    vbias_list = list(ising_vbias.detach().cpu().numpy())\n",
    "    hbias_list = list(ising_hbias.detach().cpu().numpy())\n",
    "    hVis = {v_qubit_idx:-vbias_list[visible_idx_map[v_qubit_idx]] for v_qubit_idx in aux_crbm.visible_qubit_idxs}\n",
    "    hHid = {h_qubit_idx:-hbias_list[hidden_idx_map[h_qubit_idx]] for h_qubit_idx in aux_crbm.hidden_qubit_idxs}\n",
    "    h = {**hVis,**hHid}\n",
    "    J = {}\n",
    "    for edge in aux_crbm_edgelist:\n",
    "        if edge[0] in aux_crbm.visible_qubit_idxs:\n",
    "            J[edge] = dwave_weights_np[visible_idx_map[edge[0]]][hidden_idx_map[edge[1]]]\n",
    "        else:\n",
    "            J[edge] = dwave_weights_np[visible_idx_map[edge[1]]][hidden_idx_map[edge[0]]]\n",
    "    \"\"\"\n",
    "    Whe convert the Ising parameters to RBM parameters here\n",
    "    We have rbm_orig_... variables to keep track of the original RBM parameters.\n",
    "    Note that in this reverse process we do not need to keep track of the original RBM\n",
    "    parameters as we are not changing the RBM parameters.\n",
    "    \"\"\"\n",
    "    rbm_orig_vis = torch.nn.Parameter(2.*(ising_vbias - torch.sum(ising_weights, dim=1)), requires_grad=False).to(device)\n",
    "    rbm_orig_hid = torch.nn.Parameter(2.*(ising_hbias - torch.sum(ising_weights, dim=0)), requires_grad=False).to(device)\n",
    "    rbm_orig_weights = ising_weights*4\n",
    "    \n",
    "    aux_crbm.weights = rbm_orig_weights\n",
    "    aux_crbm._visible_bias = rbm_orig_vis\n",
    "    aux_crbm._hidden_bias = rbm_orig_hid\n",
    "    \n",
    "    aux_crbm_sampler.rbm = aux_crbm\n",
    "    aux_crbm_vis, aux_crbm_hid = aux_crbm_sampler.block_gibbs_sampling()\n",
    "    aux_crbm_vis = torch.where(aux_crbm_vis == ZERO, MINUS_ONE, aux_crbm_vis)\n",
    "    aux_crbm_hid = torch.where(aux_crbm_hid == ZERO, MINUS_ONE, aux_crbm_hid)\n",
    "    aux_crbm_energy_exp = ising_energies_exp(ising_weights, ising_vbias, ising_hbias, aux_crbm_vis, aux_crbm_hid)\n",
    "    aux_crbm_energy_exps = -aux_crbm_energy_exp.detach().cpu().numpy()\n",
    "    aux_crbm_energy_exp = -torch.mean(aux_crbm_energy_exp, axis=0)\n",
    "    print(\"Ising energy with RBM samples: {0}\\n\".format(aux_crbm_energy_exp))\n",
    "\n",
    "    \"\"\"\n",
    "    Now we are done processing the classical energies and\n",
    "    now get energies and samples from dwave\n",
    "    \"\"\"\n",
    "    dwave_energies = [0]*num_iterations\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_iterations):\n",
    "            scaled_h = h.copy()\n",
    "            scaled_J = J.copy()\n",
    "            scaled_h.update((key, value/beta) for key, value in scaled_h.items())\n",
    "            scaled_J.update((key, value/beta) for key, value in scaled_J.items())\n",
    "\n",
    "            scaled_response = qpu_sampler.sample_ising(scaled_h, scaled_J, num_reads=n_reads, auto_scale=False)\n",
    "            scaled_dwave_samples, scaled_dwave_energies, dict_samples = batch_dwave_samples(scaled_response, qubit_idxs)\n",
    "            dwave_vis, dwave_hid = scaled_dwave_samples[:, :n_vis], scaled_dwave_samples[:, n_vis:]\n",
    "\n",
    "            # using torch.from_numpy(...)... instead of \n",
    "            # 'scaled_dwave_samples = torch.tensor(scaled_dwave_samples, dtype=torch.float).to(device)'\n",
    "            # to suppress UserWarning.\n",
    "            dwave_vis = torch.from_numpy(dwave_vis).float().to(device)\n",
    "            dwave_hid = torch.from_numpy(dwave_hid).float().to(device)\n",
    "\n",
    "            scaled_dwave_energies = ising_energies_exp(ising_weights, ising_vbias, ising_hbias, dwave_vis, dwave_hid)\n",
    "            energy_exp_dwave_ising = torch.mean(scaled_dwave_energies, axis = 0)\n",
    "            #print(\"Ising energy with Dwave samples is {0}\".format(energy_exp_dwave_ising))\n",
    "            \n",
    "            dimod_ising_energies = [0]*len(dict_samples)\n",
    "            \n",
    "            for j in range(len(dict_samples)):\n",
    "                dimod_ising_energies[j] = dimod.ising_energy(dict_samples[j], h, J)\n",
    "            \n",
    "            scaled_dwave_samples = torch.tensor(scaled_dwave_samples, dtype=torch.float)  \n",
    "            dwave_energy_exp = np.mean(dimod_ising_energies, axis=0)\n",
    "            dwave_energies[i] = dimod_ising_energies\n",
    "            print(\"aux_crbm_energy_exp : {0}, beta : {1} and {2}\".format(dwave_energy_exp, beta, i))\n",
    "            beta = beta - lr*(-float(aux_crbm_energy_exp)+float(dwave_energy_exp))\n",
    "            betas.append(beta)\n",
    "        \n",
    "    return betas, dwave_energies, aux_crbm_energy_exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92df648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run each time you want to generate new weights and biases\n",
    "\n",
    "ising_weights, ising_vbias, ising_hbias = initialize_ising(1000,1000)\n",
    "\n",
    "# Check to ensure J is in the valid DWAVE coupling range\n",
    "min_J = np.min(ising_weights.cpu().detach().numpy())\n",
    "max_J = np.max(ising_weights.cpu().detach().numpy())\n",
    "print(\"J is between: {0} and {1}\".format(min_J,max_J))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d35fff7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "betas, dwave_energies,  aux_crbm_energy_exps = beta_reverse(num_iterations=3,\n",
    "                       lr=0.015,\n",
    "                       beta_init=6.7,\n",
    "                       n_reads=400,\n",
    "                       qpu_sampler=qpu_sampler,\n",
    "                       aux_crbm_sampler=aux_crbm_sampler,\n",
    "                       ising_weights = ising_weights,\n",
    "                       ising_vbias = ising_vbias,\n",
    "                       ising_hbias = ising_hbias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71accea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the run\n",
    "save_run_info('N(0,0.2)', ising_weights, ising_vbias, ising_hbias, aux_crbm_energy_exps, dwave_energies, betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdea21e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recover data\n",
    "ising_weights, ising_vbias, ising_hbias, aux_crbm_energy_exps, dwave_energies, betas= recover_saved_parameters('N(0,0.2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44c58bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_energy_plots('N(0,0.2)', dwave_energies, aux_crbm_energy_exps, betas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
