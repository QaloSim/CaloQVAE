{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b1616c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: DWAVE_API_TOKEN=4Fq8-98e991a786ff04f4f4b6ab5466629411358ce418\n",
      "Thu Apr 17 18:45:31 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               Off |   00000000:1B:00.0 Off |                  Off |\n",
      "| 30%   43C    P2             75W /  300W |    9468MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000               Off |   00000000:1C:00.0 Off |                  Off |\n",
      "| 30%   32C    P8             28W /  300W |   47460MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000               Off |   00000000:4F:00.0 Off |                  Off |\n",
      "| 30%   32C    P8             20W /  300W |       4MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000               Off |   00000000:50:00.0 Off |                  Off |\n",
      "| 30%   33C    P8             23W /  300W |       4MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA RTX A6000               Off |   00000000:9C:00.0 Off |                  Off |\n",
      "| 30%   27C    P8             27W /  300W |       5MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA RTX A6000               Off |   00000000:9D:00.0 Off |                  Off |\n",
      "| 31%   23C    P8             27W /  300W |       5MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    467857      C   /usr/bin/python                              9458MiB |\n",
      "|    1   N/A  N/A   2955450      C   python                                      47450MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "# %env CUDA_VISIBLE_DEVICES=5\n",
    "%env DWAVE_API_TOKEN=4Fq8-98e991a786ff04f4f4b6ab5466629411358ce418\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d039e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m[18:45:36.357]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mCaloQVAE                                          \u001b[0mWillkommen!\n",
      "\u001b[1m[18:45:36.358]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mCaloQVAE                                          \u001b[0mLoading configuration.\n"
     ]
    }
   ],
   "source": [
    "import hydra\n",
    "from hydra.utils import instantiate\n",
    "from hydra import initialize, compose\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "os.chdir('/home/' + getpass.getuser() + '/CaloQVAE/')\n",
    "sys.path.insert(1, '/home/' + getpass.getuser() + '/CaloQVAE/')\n",
    "\n",
    "#external libraries\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import sys\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits\n",
    "torch.manual_seed(32)\n",
    "import numpy as np\n",
    "np.random.seed(32)\n",
    "import matplotlib.pyplot as plt\n",
    "import hydra\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# PyTorch imports\n",
    "from torch import device, load, save\n",
    "from torch.nn import DataParallel\n",
    "from torch.cuda import is_available\n",
    "\n",
    "# Add the path to the parent directory to augment search for module\n",
    "sys.path.append(os.getcwd())\n",
    "    \n",
    "# Weights and Biases\n",
    "import wandb\n",
    "\n",
    "#self defined imports\n",
    "from CaloQVAE import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2650440b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m[18:45:37.932]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mnumexpr.utils                                     \u001b[0mNote: detected 96 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "\u001b[1m[18:45:37.934]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mnumexpr.utils                                     \u001b[0mNote: NumExpr detected 96 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "\u001b[1m[18:45:37.935]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mnumexpr.utils                                     \u001b[0mNumExpr defaulting to 8 threads.\n",
      "2025-04-17 18:45:38,648 dwave.cloud \u001b[1;95mINFO \u001b[1;0m MainThread Log level for 'dwave.cloud' namespace set to 0\n",
      "\u001b[1m[18:45:38.648]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud                                       \u001b[0mLog level for 'dwave.cloud' namespace set to 0\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from data.dataManager import DataManager\n",
    "from utils.plotting.plotProvider import PlotProvider\n",
    "from engine.engine import Engine\n",
    "from models.modelCreator import ModelCreator\n",
    "\n",
    "from utils.plotting.HighLevelFeatures import HighLevelFeatures as HLF\n",
    "HLF_1_photons = HLF('photon', filename='/fast_scratch_1/caloqvae/data/atlas/binning_dataset_1_photons.xml', wandb=False)\n",
    "HLF_1_pions = HLF('pion', filename='/fast_scratch_1/caloqvae/data/atlas/binning_dataset_1_pions.xml', wandb=False)\n",
    "HLF_1_electron = HLF('electron', filename='/fast_scratch_1/caloqvae/data/atlas_dataset2and3/binning_dataset_2.xml', wandb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80f27433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hydra.initialize()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(version_base=None, config_path=\"configs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4ba19d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config.yaml': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n",
      "\u001b[1m[18:45:41.490]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdata.dataManager                                  \u001b[0mLoading Data\n",
      "\u001b[1m[18:46:03.366]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdata.dataManager                                  \u001b[0m<torch.utils.data.dataloader.DataLoader object at 0x7f998a34cb80>: 80000 events, 625 batches\n",
      "\u001b[1m[18:46:03.369]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdata.dataManager                                  \u001b[0m<torch.utils.data.dataloader.DataLoader object at 0x7f9a48bd37c0>: 10000 events, 10 batches\n",
      "\u001b[1m[18:46:03.370]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdata.dataManager                                  \u001b[0m<torch.utils.data.dataloader.DataLoader object at 0x7f9a474f9c60>: 10000 events, 10 batches\n",
      "\u001b[1m[18:46:05.305]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mmodels.modelCreator                               \u001b[0mInitialising Model Type AtlasConditionalQVAE3DHD\n",
      "2025-04-17 18:46:05,320 dwave.cloud.client.base \u001b[1;95mINFO \u001b[1;0m MainThread Fetching definitions of all available solvers\n",
      "\u001b[1m[18:46:05.320]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mFetching definitions of all available solvers\n",
      "2025-04-17 18:46:06,687 dwave.cloud.client.base \u001b[1;95mINFO \u001b[1;0m MainThread Received solver data for 8 solver(s).\n",
      "\u001b[1m[18:46:06.687]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mReceived solver data for 8 solver(s).\n",
      "2025-04-17 18:46:06,716 dwave.cloud.client.base \u001b[1;95mINFO \u001b[1;0m MainThread Adding solver StructuredSolver(id='Advantage_system4.1')\n",
      "\u001b[1m[18:46:06.716]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mAdding solver StructuredSolver(id='Advantage_system4.1')\n",
      "2025-04-17 18:46:06,752 dwave.cloud.client.base \u001b[1;95mINFO \u001b[1;0m MainThread Adding solver StructuredSolver(id='Advantage_system6.4')\n",
      "\u001b[1m[18:46:06.752]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mAdding solver StructuredSolver(id='Advantage_system6.4')\n",
      "2025-04-17 18:46:06,767 dwave.cloud.client.base \u001b[1;95mINFO \u001b[1;0m MainThread Adding solver StructuredSolver(id='Advantage2_prototype2.6')\n",
      "\u001b[1m[18:46:06.767]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mAdding solver StructuredSolver(id='Advantage2_prototype2.6')\n",
      "2025-04-17 18:46:06,795 dwave.cloud.client.base \u001b[1;95mINFO \u001b[1;0m MainThread Adding solver StructuredSolver(id='Advantage_system7.1')\n",
      "\u001b[1m[18:46:06.795]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mAdding solver StructuredSolver(id='Advantage_system7.1')\n",
      "2025-04-17 18:46:06,858 dwave.cloud.client.base \u001b[1;95mINFO \u001b[1;0m MainThread Fetching definitions of all available solvers\n",
      "\u001b[1m[18:46:06.858]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mFetching definitions of all available solvers\n",
      "2025-04-17 18:46:08,189 dwave.cloud.client.base \u001b[1;95mINFO \u001b[1;0m MainThread Received solver data for 8 solver(s).\n",
      "\u001b[1m[18:46:08.189]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mReceived solver data for 8 solver(s).\n",
      "2025-04-17 18:46:08,219 dwave.cloud.client.base \u001b[1;95mINFO \u001b[1;0m MainThread Adding solver StructuredSolver(id='Advantage_system4.1')\n",
      "\u001b[1m[18:46:08.219]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mAdding solver StructuredSolver(id='Advantage_system4.1')\n",
      "2025-04-17 18:46:08,255 dwave.cloud.client.base \u001b[1;95mINFO \u001b[1;0m MainThread Adding solver StructuredSolver(id='Advantage_system6.4')\n",
      "\u001b[1m[18:46:08.255]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mAdding solver StructuredSolver(id='Advantage_system6.4')\n",
      "2025-04-17 18:46:08,274 dwave.cloud.client.base \u001b[1;95mINFO \u001b[1;0m MainThread Adding solver StructuredSolver(id='Advantage2_prototype2.6')\n",
      "\u001b[1m[18:46:08.274]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mAdding solver StructuredSolver(id='Advantage2_prototype2.6')\n",
      "2025-04-17 18:46:08,303 dwave.cloud.client.base \u001b[1;95mINFO \u001b[1;0m MainThread Adding solver StructuredSolver(id='Advantage_system7.1')\n",
      "\u001b[1m[18:46:08.303]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mAdding solver StructuredSolver(id='Advantage_system7.1')\n",
      "\u001b[1m[18:46:08.524]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mmodels.autoencoders.AtlasConditionalQVAE3DHD      \u001b[0mGumBoltAtlasCRBMCNN::decoder SmallPBHDMIRRORv1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing NetworkV3\n",
      "Layer Inputs:  [1208, 3368, 3368, 3368]\n",
      "Layer Outputs:  [2160, 2160, 2160, 6480]\n",
      "Raw Layer Indices:  [0, 2160, 2160, 2160, 6480]\n",
      "Initializing NetworkV3\n",
      "Initializing NetworkV3\n",
      "Initializing NetworkV3\n",
      "Initializing NetworkV3\n"
     ]
    }
   ],
   "source": [
    "# config=compose(config_name=\"config.yaml\")\n",
    "config=compose(config_name=\"config.yaml\")\n",
    "wandb.init(project=\"caloqvae\", entity=\"qvae\", config=config, mode='disabled')\n",
    "modelCreator = ModelCreator(cfg=config)\n",
    "dataMgr = DataManager(cfg=config)\n",
    "#initialise data loaders\n",
    "dataMgr.init_dataLoaders()\n",
    "#run pre processing: get/set input dimensions and mean of train dataset\n",
    "dataMgr.pre_processing()\n",
    "\n",
    "if config.model.activation_fct.lower()==\"relu\":\n",
    "    modelCreator.default_activation_fct=torch.nn.ReLU()\n",
    "elif config.model.activation_fct.lower()==\"tanh\":\n",
    "    modelCreator.default_activation_fct=torch.nn.Tanh()\n",
    "else:\n",
    "    logger.warning(\"Setting identity as default activation fct\")\n",
    "    modelCreator.default_activation_fct=torch.nn.Identity()\n",
    "\n",
    "#instantiate the chosen model\n",
    "#loads from file \n",
    "model=modelCreator.init_model(dataMgr=dataMgr)\n",
    "\n",
    "#create the NN infrastructure\n",
    "model.create_networks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1d17214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m[18:46:09.229]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mwandb                                             \u001b[0mWatching\n",
      "/home/luian1/.local/lib/python3.10/site-packages/coffea/util.py:154: FutureWarning: In coffea version v0.8.0 (target date: 31 Dec 2022), this will be an error.\n",
      "(Set coffea.deprecations_as_errors = True to get a stack trace now.)\n",
      "ImportError: coffea.hist is deprecated\n",
      "  warnings.warn(message, FutureWarning)\n",
      "\u001b[1m[18:46:09.900]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mengine.engineAtlas                                \u001b[0mSetting up engine Atlas.\n",
      "\u001b[1m[18:46:09.901]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mengine.engineCaloV3                               \u001b[0mSetting up engine Calo.\n",
      "\u001b[1m[18:46:09.902]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mengine.engine                                     \u001b[0mSetting up default engine.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# dev = torch.device(\"cuda:0\")\n",
    "dev = \"cuda:{0}\".format(config.gpu_list[0])\n",
    "wandb.watch(model)\n",
    "engine=instantiate(config.engine, config)\n",
    "engine._config=config\n",
    "#add dataMgr instance to engine namespace\n",
    "engine.data_mgr=dataMgr\n",
    "#add device instance to engine namespace\n",
    "engine.device=dev    \n",
    "#instantiate and register optimisation algorithm\n",
    "engine.optimiser = torch.optim.Adam(model.parameters(),\n",
    "                                    lr=config.engine.learning_rate)\n",
    "#add the model instance to the engine namespace\n",
    "engine.model = model\n",
    "# add the modelCreator instance to engine namespace\n",
    "engine.model_creator = modelCreator\n",
    "engine.model = engine.model.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48c8f3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m[18:46:30.875]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdata.dataManager                                  \u001b[0m<torch.utils.data.dataloader.DataLoader object at 0x7f9a2d42a080>: 80000 events, 625 batches\n",
      "\u001b[1m[18:46:30.877]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdata.dataManager                                  \u001b[0m<torch.utils.data.dataloader.DataLoader object at 0x7f9a2d42a020>: 10000 events, 10 batches\n",
      "\u001b[1m[18:46:30.878]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdata.dataManager                                  \u001b[0m<torch.utils.data.dataloader.DataLoader object at 0x7f990c73b9a0>: 10000 events, 10 batches\n"
     ]
    }
   ],
   "source": [
    "train_loader,test_loader,val_loader = engine.data_mgr.create_dataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e207e29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_state(model, run_path, device):\n",
    "        model_loc = run_path\n",
    "        \n",
    "        # Open a file in read-binary mode\n",
    "        with open(model_loc, 'rb') as f:\n",
    "            # Interpret the file using torch.load()\n",
    "            checkpoint=torch.load(f, map_location=device)\n",
    "            \n",
    "            local_module_keys=list(model._modules.keys())\n",
    "            print(local_module_keys)\n",
    "            for module in checkpoint.keys():\n",
    "                if module in local_module_keys:\n",
    "                    print(\"Loading weights for module = \", module)\n",
    "                    getattr(model, module).load_state_dict(checkpoint[module])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3163cd34-5177-4d36-8865-8f458ecffeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/path/to/project')\n",
    "\n",
    "import utils.plotting.HighLevelFeatures as HLF\n",
    "import h5py\n",
    "from ipynb.fs.full.utils.plotting.HEPMetrics import get_fpd_kpd_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fe7a120-ee54-4930-b76a-fe6fbc807142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_HEPMetric(path_head, model_name, start_epoch, end_epoch, epoch_int):\n",
    "    en_list = []\n",
    "    fpd_recon, fpd_sample = [], []\n",
    "    kpd_recon, kpd_sample = [], []\n",
    "    hlf, ref_hlf = None, None\n",
    "    for en in range(start_epoch, end_epoch, epoch_int):\n",
    "        run_path = path_head + str(en) + \".pth\"\n",
    "        modelname = model_name\n",
    "        datascaled = 'reduced'\n",
    "        open_path = path_head.replace(path_head.split('/')[-1], '') + \"config.yaml\"\n",
    "        with open(open_path, 'r') as file:\n",
    "            model_config = yaml.safe_load(file)\n",
    "            R = json.loads(model_config[\"_content\"][\"value\"]['engine'].replace(\"'\", \"\\\"\"))['r_param']\n",
    "            reducedata = True if model_config[\"_content\"][\"value\"][\"reducedata\"] == 'True' else False\n",
    "            scaled=False\n",
    "            \n",
    "        arch = config['model']['model_type']\n",
    "        part = config['data']['particle_type']\n",
    "        modelCreator.load_state(run_path, dev)\n",
    "        engine.model.eval();\n",
    "\n",
    "        # get the samples\n",
    "        xtarget_samples = []\n",
    "        xrecon_samples = []\n",
    "        xgen_samples = []\n",
    "        xgen_samples_qpu = []\n",
    "        n_samples4_qpu = 200\n",
    "        \n",
    "        # xrecon_samples_2 = []\n",
    "        \n",
    "        # labelstarget_samples = []\n",
    "        # labelsrecon_samples = []\n",
    "        entarget_samples = []\n",
    "        with torch.no_grad():\n",
    "            for xx in train_loader:\n",
    "            # for xx in train_loader:\n",
    "                in_data, true_energy, in_data_flat = engine._preprocess(xx[0],xx[1])\n",
    "                ###############################################\n",
    "                # true_energy = true_energy[:n_samples4_qpu,:]\n",
    "                # in_data = in_data[:n_samples4_qpu,:]\n",
    "                ##############################################\n",
    "                # print(in_data.shape)\n",
    "                if reducedata:\n",
    "                    in_data = engine._reduce(in_data, true_energy, R=R)\n",
    "                fwd_output = engine.model((in_data, true_energy), False)\n",
    "                if reducedata:\n",
    "                    in_data = engine._reduceinv(in_data, true_energy, R=R)\n",
    "                    recon_data = engine._reduceinv(fwd_output.output_activations, true_energy, R=R)\n",
    "                    engine._model.sampler._batch_size = true_energy.shape[0]\n",
    "                    if True:\n",
    "                        sample_energies, sample_data = engine._model.generate_samples_cond(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True)\n",
    "                        # sample_energies_qpu, sample_data_qpu = engine.model.generate_samples_qpu_cond(true_energy=true_energy, num_samples=1, thrsh=30, beta=1/beta0)\n",
    "                    else:\n",
    "                        sample_energies, sample_data = engine._model.generate_samples(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True)\n",
    "                        # sample_energies_qpu, sample_data_qpu = engine._model.generate_samples_qpu(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True, beta=1/beta0)\n",
    "                    engine._model.sampler._batch_size = engine._config.engine.rbm_batch_size\n",
    "                    sample_data = engine._reduceinv(sample_data, sample_energies, R=R)\n",
    "                    # sample_data_qpu = engine._reduceinv(sample_data_qpu, sample_energies_qpu, R=R)\n",
    "                elif scaled:\n",
    "                    in_data = torch.tensor(engine._data_mgr.inv_transform(in_data.detach().cpu().numpy()))\n",
    "                    recon_data = torch.tensor(engine._data_mgr.inv_transform(fwd_output.output_activations.detach().cpu().numpy()))\n",
    "                    # recon_data_2 = torch.tensor(engine._data_mgr.inv_transform(fwd_just_act.detach().cpu().numpy()))\n",
    "                    # recon_data_2 = torch.tensor(engine._data_mgr.inv_transform(fwd_energy_shift.detach().cpu().numpy()))\n",
    "                    engine._model.sampler._batch_size = true_energy.shape[0]\n",
    "                    \n",
    "                    if True:\n",
    "                        sample_energies, sample_data = engine._model.generate_samples_cond(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True)\n",
    "                        # sample_energies_qpu, sample_data_qpu = engine.model.generate_samples_qpu_cond(true_energy=true_energy[:100,:], num_samples=1, thrsh=30, beta=1/beta0)\n",
    "                    else:\n",
    "                        sample_energies, sample_data = engine._model.generate_samples(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True)\n",
    "                        # sample_energies_qpu, sample_data_qpu = engine._model.generate_samples_qpu(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True, beta=1/beta0)\n",
    "                    engine._model.sampler._batch_size = engine._config.engine.rbm_batch_size\n",
    "                    sample_data = torch.tensor(engine._data_mgr.inv_transform(sample_data.detach().cpu().numpy()))\n",
    "                    # sample_data_qpu = torch.tensor(engine._data_mgr.inv_transform(sample_data_qpu.detach().cpu().numpy()))\n",
    "                else:\n",
    "                    in_data = in_data.detach().cpu()*1000\n",
    "                    recon_data = fwd_output.output_activations.detach().cpu()*1000\n",
    "                    engine._model.sampler._batch_size = true_energy.shape[0]\n",
    "                    sample_energies, sample_data = engine._model.generate_samples(num_samples=true_energy.shape[0], true_energy=true_energy)\n",
    "                    engine._model.sampler._batch_size = engine._config.engine.rbm_batch_size\n",
    "                    # sample_energies, sample_data = engine._model.generate_samples(num_samples=2048)\n",
    "                    sample_data = sample_data.detach().cpu()*1000\n",
    "        \n",
    "        \n",
    "                xtarget_samples.append(in_data.detach().cpu())\n",
    "                xrecon_samples.append( recon_data.detach().cpu())\n",
    "                xgen_samples.append( sample_data.detach().cpu())\n",
    "                # xgen_samples_qpu.append( sample_data_qpu.detach().cpu())\n",
    "                entarget_samples.append(true_energy.detach().cpu())\n",
    "        \n",
    "                # xrecon_samples_2.append( recon_data_2.detach().cpu())\n",
    "            for xx in val_loader:\n",
    "            # for xx in train_loader:\n",
    "                in_data, true_energy, in_data_flat = engine._preprocess(xx[0],xx[1])\n",
    "                ###############################################\n",
    "                # true_energy = true_energy[:n_samples4_qpu,:]\n",
    "                # in_data = in_data[:n_samples4_qpu,:]\n",
    "                ##############################################\n",
    "                # print(in_data.shape)\n",
    "                if reducedata:\n",
    "                    in_data = engine._reduce(in_data, true_energy, R=R)\n",
    "                fwd_output = engine.model((in_data, true_energy), False)\n",
    "                if reducedata:\n",
    "                    in_data = engine._reduceinv(in_data, true_energy, R=R)\n",
    "                    recon_data = engine._reduceinv(fwd_output.output_activations, true_energy, R=R)\n",
    "                    engine._model.sampler._batch_size = true_energy.shape[0]\n",
    "                    if True:\n",
    "                        sample_energies, sample_data = engine._model.generate_samples_cond(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True)\n",
    "                        # sample_energies_qpu, sample_data_qpu = engine.model.generate_samples_qpu_cond(true_energy=true_energy, num_samples=1, thrsh=30, beta=1/beta0)\n",
    "                    else:\n",
    "                        sample_energies, sample_data = engine._model.generate_samples(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True)\n",
    "                        # sample_energies_qpu, sample_data_qpu = engine._model.generate_samples_qpu(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True, beta=1/beta0)\n",
    "                    engine._model.sampler._batch_size = engine._config.engine.rbm_batch_size\n",
    "                    sample_data = engine._reduceinv(sample_data, sample_energies, R=R)\n",
    "                    # sample_data_qpu = engine._reduceinv(sample_data_qpu, sample_energies_qpu, R=R)\n",
    "                elif scaled:\n",
    "                    in_data = torch.tensor(engine._data_mgr.inv_transform(in_data.detach().cpu().numpy()))\n",
    "                    recon_data = torch.tensor(engine._data_mgr.inv_transform(fwd_output.output_activations.detach().cpu().numpy()))\n",
    "                    # recon_data_2 = torch.tensor(engine._data_mgr.inv_transform(fwd_just_act.detach().cpu().numpy()))\n",
    "                    # recon_data_2 = torch.tensor(engine._data_mgr.inv_transform(fwd_energy_shift.detach().cpu().numpy()))\n",
    "                    engine._model.sampler._batch_size = true_energy.shape[0]\n",
    "                    \n",
    "                    if True:\n",
    "                        sample_energies, sample_data = engine._model.generate_samples_cond(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True)\n",
    "                        # sample_energies_qpu, sample_data_qpu = engine.model.generate_samples_qpu_cond(true_energy=true_energy[:100,:], num_samples=1, thrsh=30, beta=1/beta0)\n",
    "                    else:\n",
    "                        sample_energies, sample_data = engine._model.generate_samples(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True)\n",
    "                        # sample_energies_qpu, sample_data_qpu = engine._model.generate_samples_qpu(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True, beta=1/beta0)\n",
    "                    engine._model.sampler._batch_size = engine._config.engine.rbm_batch_size\n",
    "                    sample_data = torch.tensor(engine._data_mgr.inv_transform(sample_data.detach().cpu().numpy()))\n",
    "                    # sample_data_qpu = torch.tensor(engine._data_mgr.inv_transform(sample_data_qpu.detach().cpu().numpy()))\n",
    "                else:\n",
    "                    in_data = in_data.detach().cpu()*1000\n",
    "                    recon_data = fwd_output.output_activations.detach().cpu()*1000\n",
    "                    engine._model.sampler._batch_size = true_energy.shape[0]\n",
    "                    sample_energies, sample_data = engine._model.generate_samples(num_samples=true_energy.shape[0], true_energy=true_energy)\n",
    "                    engine._model.sampler._batch_size = engine._config.engine.rbm_batch_size\n",
    "                    # sample_energies, sample_data = engine._model.generate_samples(num_samples=2048)\n",
    "                    sample_data = sample_data.detach().cpu()*1000\n",
    "        \n",
    "        \n",
    "                xtarget_samples.append(in_data.detach().cpu())\n",
    "                xrecon_samples.append( recon_data.detach().cpu())\n",
    "                xgen_samples.append( sample_data.detach().cpu())\n",
    "                # xgen_samples_qpu.append( sample_data_qpu.detach().cpu())\n",
    "                entarget_samples.append(true_energy.detach().cpu())\n",
    "        \n",
    "                # xrecon_samples_2.append( recon_data_2.detach().cpu())\n",
    "            for xx in test_loader:\n",
    "            # for xx in train_loader:\n",
    "                in_data, true_energy, in_data_flat = engine._preprocess(xx[0],xx[1])\n",
    "                ###############################################\n",
    "                # true_energy = true_energy[:n_samples4_qpu,:]\n",
    "                # in_data = in_data[:n_samples4_qpu,:]\n",
    "                ##############################################\n",
    "                # print(in_data.shape)\n",
    "                if reducedata:\n",
    "                    in_data = engine._reduce(in_data, true_energy, R=R)\n",
    "                fwd_output = engine.model((in_data, true_energy), False)\n",
    "                if reducedata:\n",
    "                    in_data = engine._reduceinv(in_data, true_energy, R=R)\n",
    "                    recon_data = engine._reduceinv(fwd_output.output_activations, true_energy, R=R)\n",
    "                    engine._model.sampler._batch_size = true_energy.shape[0]\n",
    "                    if True:\n",
    "                        sample_energies, sample_data = engine._model.generate_samples_cond(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True)\n",
    "                        # sample_energies_qpu, sample_data_qpu = engine.model.generate_samples_qpu_cond(true_energy=true_energy, num_samples=1, thrsh=30, beta=1/beta0)\n",
    "                    else:\n",
    "                        sample_energies, sample_data = engine._model.generate_samples(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True)\n",
    "                        # sample_energies_qpu, sample_data_qpu = engine._model.generate_samples_qpu(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True, beta=1/beta0)\n",
    "                    engine._model.sampler._batch_size = engine._config.engine.rbm_batch_size\n",
    "                    sample_data = engine._reduceinv(sample_data, sample_energies, R=R)\n",
    "                    # sample_data_qpu = engine._reduceinv(sample_data_qpu, sample_energies_qpu, R=R)\n",
    "                elif scaled:\n",
    "                    in_data = torch.tensor(engine._data_mgr.inv_transform(in_data.detach().cpu().numpy()))\n",
    "                    recon_data = torch.tensor(engine._data_mgr.inv_transform(fwd_output.output_activations.detach().cpu().numpy()))\n",
    "                    # recon_data_2 = torch.tensor(engine._data_mgr.inv_transform(fwd_just_act.detach().cpu().numpy()))\n",
    "                    # recon_data_2 = torch.tensor(engine._data_mgr.inv_transform(fwd_energy_shift.detach().cpu().numpy()))\n",
    "                    engine._model.sampler._batch_size = true_energy.shape[0]\n",
    "                    \n",
    "                    if True:\n",
    "                        sample_energies, sample_data = engine._model.generate_samples_cond(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True)\n",
    "                        # sample_energies_qpu, sample_data_qpu = engine.model.generate_samples_qpu_cond(true_energy=true_energy[:100,:], num_samples=1, thrsh=30, beta=1/beta0)\n",
    "                    else:\n",
    "                        sample_energies, sample_data = engine._model.generate_samples(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True)\n",
    "                        # sample_energies_qpu, sample_data_qpu = engine._model.generate_samples_qpu(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True, beta=1/beta0)\n",
    "                    engine._model.sampler._batch_size = engine._config.engine.rbm_batch_size\n",
    "                    sample_data = torch.tensor(engine._data_mgr.inv_transform(sample_data.detach().cpu().numpy()))\n",
    "                    # sample_data_qpu = torch.tensor(engine._data_mgr.inv_transform(sample_data_qpu.detach().cpu().numpy()))\n",
    "                else:\n",
    "                    in_data = in_data.detach().cpu()*1000\n",
    "                    recon_data = fwd_output.output_activations.detach().cpu()*1000\n",
    "                    engine._model.sampler._batch_size = true_energy.shape[0]\n",
    "                    sample_energies, sample_data = engine._model.generate_samples(num_samples=true_energy.shape[0], true_energy=true_energy)\n",
    "                    engine._model.sampler._batch_size = engine._config.engine.rbm_batch_size\n",
    "                    # sample_energies, sample_data = engine._model.generate_samples(num_samples=2048)\n",
    "                    sample_data = sample_data.detach().cpu()*1000\n",
    "        \n",
    "        \n",
    "                xtarget_samples.append(in_data.detach().cpu())\n",
    "                xrecon_samples.append( recon_data.detach().cpu())\n",
    "                xgen_samples.append( sample_data.detach().cpu())\n",
    "                # xgen_samples_qpu.append( sample_data_qpu.detach().cpu())\n",
    "                entarget_samples.append(true_energy.detach().cpu())\n",
    "        \n",
    "                # xrecon_samples_2.append( recon_data_2.detach().cpu())\n",
    "            \n",
    "            \n",
    "        xtarget_samples = torch.cat(xtarget_samples, dim=0)\n",
    "        xrecon_samples = torch.cat(xrecon_samples, dim=0)\n",
    "        xgen_samples = torch.cat(xgen_samples, dim=0)\n",
    "        # xgen_samples_qpu = torch.cat(xgen_samples_qpu, dim=0)\n",
    "        entarget_samples = torch.cat(entarget_samples, dim=0)\n",
    "        \n",
    "        # xrecon_samples_2 = torch.cat(xrecon_samples_2, dim=0)\n",
    "        if en == start_epoch:\n",
    "            print(\"First epoch\")\n",
    "            hlf = HLF.HighLevelFeatures('electron', filename='/fast_scratch_1/caloqvae/data/atlas_dataset2and3/binning_dataset_2.xml', wandb=False)\n",
    "            ref_hlf = HLF.HighLevelFeatures('electron', filename='/fast_scratch_1/caloqvae/data/atlas_dataset2and3/binning_dataset_2.xml', wandb=False)\n",
    "            hlf.Einc = entarget_samples\n",
    "\n",
    "        recon_HEPMetrics = get_fpd_kpd_metrics(np.array(xtarget_samples), np.array(xrecon_samples), False, hlf, ref_hlf)\n",
    "        sample_HEPMetrics = get_fpd_kpd_metrics(np.array(xtarget_samples), np.array(xgen_samples), False, hlf, ref_hlf)\n",
    "\n",
    "        en_list.append(en)\n",
    "        fpd_recon.append(recon_HEPMetrics[0])\n",
    "        kpd_recon.append(recon_HEPMetrics[2])\n",
    "        fpd_sample.append(sample_HEPMetrics[0])\n",
    "        kpd_sample.append(sample_HEPMetrics[2])\n",
    "        print(\"Finished generating HEP Metrics for epoch \" + str(en) + \" ...\")\n",
    "    return en_list, fpd_recon, kpd_recon, fpd_sample, kpd_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00348d61-0080-4736-bb2a-7182aaeec5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_head = \"/fast_scratch_1/caloqvae/luian1/wandb/run-20240716_092439-ja6l8prq/files/AtlasConditionalQVAE3D_atlas_default_\"\n",
    "# # path_head = \"/fast_scratch_1/caloqvae/luian1/wandb/run-20240722_235537-ws2su3jq/files/AtlasConditionalQVAE3DHD_atlas_default_\"\n",
    "# path_head = \"/fast_scratch_1/caloqvae/luian1/wandb/run-20240717_230102-6jq8dbkm/files/AtlasConditionalQVAE3D_atlas_default_\"\n",
    "# path_head = \"/fast_scratch_1/caloqvae/luian1/wandb/run-20240809_192828-bk3n30hp/files/AtlasConditionalQVAE3DHD_atlas_default_\"\n",
    "# model_name = 'robust-serenity-216'\n",
    "# path_head = \"/fast_scratch_1/caloqvae/luian1/wandb/run-20240717_230102-6jq8dbkm/files/AtlasConditionalQVAE3D_atlas_default_\"\n",
    "# model_name = 'mild-pond-223'\n",
    "# HEPMetric_output = search_HEPMetric(path_head, model_name, 10, 151, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58fc5d44-6cd1-4197-9f94-ac280faaa396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# en_list, fpd_recon, kpd_recon, fpd_sample, kpd_sample = HEPMetric_output[0], HEPMetric_output[1], HEPMetric_output[2], HEPMetric_output[3], HEPMetric_output[4]\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# # Plot fpd_recon and fpd_sample on ax1\n",
    "# ax1.scatter(en_list, fpd_recon, color='blue', label='FPD Recon')\n",
    "# ax1.scatter(en_list, fpd_sample, color='green', label='FPD Sample')\n",
    "# ax1.set_xlabel('Epoch Number')\n",
    "# ax1.set_ylabel('FPD Values')\n",
    "# ax1.set_title('FPD Recon vs FPD Sample')\n",
    "# ax1.legend()\n",
    "\n",
    "# # Plot kpd_recon and kpd_sample on ax2\n",
    "# ax2.scatter(en_list, kpd_recon, color='blue', label='KPD Recon')\n",
    "# ax2.scatter(en_list, kpd_sample, color='green', label='KPD Sample')\n",
    "# ax2.set_xlabel('Epoch Number')\n",
    "# ax2.set_ylabel('KPD Values')\n",
    "# ax2.set_title('KPD Recon vs KPD Sample')\n",
    "# ax2.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8feca45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m[18:46:31.410]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mmodels.modelCreator                               \u001b[0mLoading state\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AtlasConditionalQVAE3DHD\n",
      "electron-ds2\n",
      "False True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m[18:46:32.359]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mmodels.modelCreator                               \u001b[0mLoading weights from file : /fast_scratch_1/caloqvae/luian1/wandb/run-20250326_172648-1jifk4nt/files/AtlasConditionalQVAE3DHD_atlas_default_150.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights for module =  _activation_fct\n",
      "Loading weights for module =  _bce_loss\n",
      "Loading weights for module =  _energy_activation_fct\n",
      "Loading weights for module =  _hit_activation_fct\n",
      "Loading weights for module =  _output_loss\n",
      "Loading weights for module =  _hit_loss\n",
      "Loading weights for module =  _hit_smoothing_dist_mod\n",
      "Loading weights for module =  _inference_energy_activation_fct\n",
      "Loading weights for module =  encoder\n",
      "Loading weights for module =  prior\n",
      "Loading weights for module =  decoder\n"
     ]
    }
   ],
   "source": [
    "# winter-snowflake-24: GumBoltAtlasPRBMCNN\n",
    "# run_path = \"/fast_scratch/sgonzalez/wandb/run-20240228_120832-q78nzust/files/GumBoltAtlasPRBMCNN_atlas_default_best.pth\"\n",
    "\n",
    "# run_path = \"/fast_scratch/sgonzalez/wandb/run-20240313_155921-4q9i8pnt/files/GumBoltAtlasPRBMCNN_atlas_default_best.pth\"\n",
    "# modelname = 'macabre-candle-1372'\n",
    "# datascaled = 'scaled'\n",
    "# with open(\"/fast_scratch/sgonzalez/wandb/run-20240313_155921-4q9i8pnt/files/config.yaml\", 'r') as file:\n",
    "#     model_config = yaml.safe_load(file)\n",
    "#     #R = json.loads(model_config[\"_content\"][\"value\"]['engine'].replace(\"'\", \"\\\"\"))['r_param']\n",
    "#     #reducedata = True if model_config[\"_content\"][\"value\"][\"reducedata\"] == 'True' else False\n",
    "\n",
    "# change this directory to load the synthetic data\n",
    "\n",
    "# run_path = \"/fast_scratch/QVAE/syn_data/dataset2_synthetic_macabre-candle-1372v2.hdf5\"\n",
    "\n",
    "# run_path = \"/fast_scratch_1/caloqvae/luian1/wandb/run-20240620_223442-7ggy3s19/files/AtlasConditionalQVAEv2_atlas_default_best.pth\"\n",
    "# modelname = 'dutiful-silence-108'\n",
    "# datascaled = 'scaled'\n",
    "# with open(\"/fast_scratch_1/caloqvae/luian1/wandb/run-20240620_223442-7ggy3s19/files/config.yaml\", 'r') as file:\n",
    "#     model_config = yaml.safe_load(file)\n",
    "#     #R = json.loads(model_config[\"_content\"][\"value\"]['engine'].replace(\"'\", \"\\\"\"))['r_param']\n",
    "#     #reducedata = True if model_config[\"_content\"][\"value\"][\"reducedata\"] == 'True' else False\n",
    "\n",
    "\n",
    "# arch = config['model']['model_type']\n",
    "# part = config['data']['particle_type']\n",
    "# print(arch)\n",
    "# print(part)\n",
    "\n",
    "\n",
    "# # load_state(model, run_path, 'cuda:{0}'.format(cfg.gpu_list[0]))\n",
    "# load_state(model, run_path, dev)\n",
    "# model.eval();\n",
    "\n",
    "#pretty-shape-455 | CNN + cond + scaled data + Cyl EncDec + lin/sqrt/log LONG energy encoded + CRBM 1st Partition Binv2 +scaled\n",
    "# run_path = \"/fast_scratch_1/caloqvae/luian1/wandb/run-20240620_223442-7ggy3s19/files/AtlasConditionalQVAEv2_atlas_default_best.pth\"\n",
    "# # run_path = \"/fast_scratch_1/caloqvae/luian1/wandb/run-20240620_223442-7ggy3s19/files/AtlasConditionalQVAEv2_atlas_default_150.pth\"\n",
    "# modelname = 'dutiful-silence-108'\n",
    "# datascaled = 'reduced'\n",
    "# with open(\"/fast_scratch_1/caloqvae/luian1/wandb/run-20240620_223442-7ggy3s19/files/config.yaml\", 'r') as file:\n",
    "#     model_config = yaml.safe_load(file)\n",
    "#     R = json.loads(model_config[\"_content\"][\"value\"]['engine'].replace(\"'\", \"\\\"\"))['r_param']\n",
    "#     reducedata = True if model_config[\"_content\"][\"value\"][\"reducedata\"] == 'True' else False\n",
    "#     scaled=False\n",
    "\n",
    "# \n",
    "# run_path = \"/fast_scratch_1/caloqvae/luian1/wandb/run-20240628_222152-jbtlda15/files/AtlasConditionalQVAEv2_atlas_default_best.pth\"\n",
    "# Dutiful-Silence-108 4 subdecoders on 0.001\n",
    "# run_path = \"/fast_scratch_1/caloqvae/luian1/wandb/run-20240620_223442-7ggy3s19/files/AtlasConditionalQVAEv2_atlas_default_150.pth\"\n",
    "# Rare-Puddle-159 9 subdecoders on 0.005 on Pegasus Topology\n",
    "# run_path = \"/fast_scratch_1/caloqvae/luian1/wandb/run-20240702_233736-ap4spv1x/files/AtlasConditionalQVAEv2_atlas_default_150.pth\"\n",
    "# denim-smoke-166 9 subdecoders on 0.005 on Zephyr Topology\n",
    "# run_path = \"/fast_scratch_1/caloqvae/luian1/wandb/run-20240705_110546-io27epy3/files/AtlasConditionalQVAEv2_atlas_default_150.pth\"\n",
    "# hearty-energy-168 9 subdecoders on 0.007 on Zephyr Topology\n",
    "# run_path = \"/fast_scratch_1/caloqvae/luian1/wandb/run-20240705_190726-t13cwqm8/files/AtlasConditionalQVAEv2_atlas_default_130.pth\"\n",
    "# smooth-water-178 25,5,5,5,5 heavy-sided subdecoders on 0.005 on Zephyr Topology\n",
    "# run_path = \"/fast_scratch_1/caloqvae/luian1/wandb/run-20240709_191350-r1fwwnrm/files/AtlasConditionalQVAEv2_atlas_default_150.pth\"\n",
    "# rich-glade-179 25,5,5,5,5 heavy-sided subdecoders on 0.01 on Zephyr Topology\n",
    "# run_path = \"/fast_scratch_1/caloqvae/luian1/wandb/run-20240709_192333-p8vm6r9d/files/AtlasConditionalQVAEv2_atlas_default_150.pth\"\n",
    "# worthy-vortex-203 no Hierarchical Decoders - 3D Convolutions on 0.005 on Zephyr Topology\n",
    "# run_path = \"/fast_scratch_1/caloqvae/luian1/wandb/run-20240713_101327-d3gqz2n9/files/AtlasConditionalQVAEv2_atlas_default_best.pth\"\n",
    "# jumping-wave-204 no Hierarchical Decoders - 3D Convolutions on 0.01 on Zephyr Topology\n",
    "# run_path = \"/fast_scratch_1/caloqvae/luian1/wandb/run-20240713_101633-v3azs68p/files/AtlasConditionalQVAEv2_atlas_default_150.pth\"\n",
    "# leafy-spaceship-215 no Hierarchical Decoders - 1 Extra 3D Convolutions with Center Padding on 0.005 on Zephyr Topology\n",
    "# run_path = \"/fast_scratch_1/caloqvae/luian1/wandb/run-20240716_092220-6rii3gnx/files/AtlasConditionalQVAE3D_atlas_default_best.pth\"\n",
    "# robust-serenity-216 no Hierarchical Decoders - 1 Extra 3D Convolutions with Center Padding on 0.01 on Zephyr Topology\n",
    "# run_path = \"/fast_scratch_1/caloqvae/luian1/wandb/run-20240716_092439-ja6l8prq/files/AtlasConditionalQVAE3D_atlas_default_150.pth\"\n",
    "# mild-pond-223 no Hierarchical Decoders - 1 Extra 3D Convolutions with Center Padding on 0.01 on Pegasus \n",
    "# run_path = \"/fast_scratch_1/caloqvae/luian1/wandb/run-20240717_230102-6jq8dbkm/files/AtlasConditionalQVAE3D_atlas_default_best.pth\"\n",
    "# kind-meadow-230 9 Sub Decoders - 1 Extra 3D Convolutions with Center Padding on 0.005 on Zephyr Topology\n",
    "# run_path = \"/fast_scratch_1/caloqvae/luian1/wandb/run-20240722_232630-qs8sl6t1/files/AtlasConditionalQVAE3DHD_atlas_default_100.pth\"\n",
    "# royal-thunder-232 9 Sub Decoders - HD 1 Extra 3D Convolutions with Center Padding on 0.01 on Zephyr \n",
    "# run_path = \"/fast_scratch_1/caloqvae/luian1/wandb/run-20240722_235537-ws2su3jq/files/AtlasConditionalQVAE3DHD_atlas_default_best.pth\"\n",
    "# firm-field-235 9 Sub Decoders - HD drop pos energy BP 1 Extra 3D Convolutions with Center Padding on 0.01 on Zephyr \n",
    "# run_path = \"/fast_scratch_1/caloqvae/luian1/wandb/run-20240726_185231-sqlc60h9/files/AtlasConditionalQVAE3DHD_atlas_default_130.pth\"\n",
    "# playful-sun-263 9 Sub Decoders - HD dropout 0.2 BP 1 Extra 3D Convolutions with Center Padding on 0.01 on Zephyr \n",
    "# run_path = \"/fast_scratch_1/caloqvae/luian1/wandb/run-20240808_083125-kdoniahe/files/AtlasConditionalQVAE3DHD_atlas_default_best.pth\"\n",
    "# playful-sun-265 9 Sub Decoders - HD lr 0.001, weight decay 0.001, dropout 0.2 BP 1 Extra 3D Convolutions with Center Padding on 0.01 on Zephyr \n",
    "# run_path = \"/fast_scratch_1/caloqvae/luian1/wandb/run-20240809_192828-bk3n30hp/files/AtlasConditionalQVAE3DHD_atlas_default_best.pth\"\n",
    "# ethereal-meadow-666 UNET QVAE with size 7 skip connections\n",
    "# run_path = \"/fast_scratch_1/caloqvae/luian1/wandb/run-20250315_231215-nt1vh19r/files/AtlasConditionalQVAE3DHD_atlas_default_best.pth\"\n",
    "# good-haze-675 UNET QVAE (skipcon configurations) with size 15 skip connections\n",
    "run_path = \"/fast_scratch_1/caloqvae/luian1/wandb/run-20250326_172648-1jifk4nt/files/AtlasConditionalQVAE3DHD_atlas_default_150.pth\"\n",
    "\n",
    "\n",
    "modelname = 'good-haze-675'\n",
    "datascaled = 'reduced'\n",
    "with open(\"/fast_scratch_1/caloqvae/luian1/wandb/run-20250326_172648-1jifk4nt/files/config.yaml\", 'r') as file:\n",
    "    model_config = yaml.safe_load(file)\n",
    "    R = json.loads(model_config[\"_content\"][\"value\"]['engine'].replace(\"'\", \"\\\"\"))['r_param']\n",
    "    reducedata = True if model_config[\"_content\"][\"value\"][\"reducedata\"] == 'True' else False\n",
    "    scaled=False\n",
    "    \n",
    "arch = config['model']['model_type']\n",
    "part = config['data']['particle_type']\n",
    "print(arch)\n",
    "print(part)\n",
    "print(scaled, reducedata)\n",
    "\n",
    "\n",
    "# load_state(model, run_path, 'cuda:{0}'.format(cfg.gpu_list[0]))\n",
    "# load_state(model, run_path, dev)\n",
    "modelCreator.load_state(run_path, dev)\n",
    "engine.model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df26a2e0-2ad2-4799-bad3-fbe7fdf7388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "partition_size=config.model.n_latent_nodes_per_p\n",
    "encoded_data = []\n",
    "energy_encoded_data = []\n",
    "n_samples4_qpu = 200\n",
    "\n",
    "# encoded_data_rdm = []\n",
    "# energy_encoded_data_rdm = []\n",
    "engine.model.eval()\n",
    "# engine_2.model.eval()\n",
    "with torch.no_grad():\n",
    "    for xx in val_loader:\n",
    "    # for xx in train_loader:\n",
    "        in_data, true_energy, in_data_flat = engine._preprocess(xx[0],xx[1])\n",
    "        #################################################\n",
    "        # true_energy = true_energy[:n_samples4_qpu,:]\n",
    "        # in_data = in_data[:n_samples4_qpu,:]\n",
    "        #################################################\n",
    "        if reducedata:\n",
    "            in_data = engine._reduce(in_data, true_energy, R=R)\n",
    "        # enIn = torch.cat((in_data, true_energy), dim=1)\n",
    "        # beta, post_logits, post_samples = engine.model.encoder(enIn, False)\n",
    "        beta, post_logits, post_samples = engine.model.encoder(in_data, true_energy, False)\n",
    "        post_samples = torch.cat(post_samples, 1)\n",
    "        post_samples_energy = engine.model.stater.energy_samples(post_samples[:,0:partition_size], post_samples[:,partition_size:2*partition_size], \n",
    "                                                 post_samples[:,2*partition_size:3*partition_size], post_samples[:,3*partition_size:4*partition_size], 1.0 )\n",
    "        encoded_data.append(post_samples.detach().cpu())\n",
    "        energy_encoded_data.append(post_samples_energy.detach().cpu())\n",
    "        \n",
    "        # #Rdm model\n",
    "        # # enIn = torch.cat((in_data, true_energy), dim=1)\n",
    "        # # beta, post_logits, post_samples = engine.model.encoder(enIn, False)\n",
    "        # beta, post_logits, post_samples = engine_2.model.encoder(in_data, true_energy, False)\n",
    "        # post_samples = torch.cat(post_samples, 1)\n",
    "        # post_samples_energy = engine_2.model.stater.energy_samples(post_samples[:,0:partition_size], post_samples[:,partition_size:2*partition_size], \n",
    "        #                                          post_samples[:,2*partition_size:3*partition_size], post_samples[:,3*partition_size:4*partition_size], 1.0 )\n",
    "        # encoded_data_rdm.append(post_samples.detach().cpu())\n",
    "        # energy_encoded_data_rdm.append(post_samples_energy.detach().cpu())\n",
    "\n",
    "encoded_data = torch.cat(encoded_data, dim=0)\n",
    "energy_encoded_data = torch.cat(energy_encoded_data, dim=0)\n",
    "        \n",
    "# encoded_data_rdm = torch.cat(encoded_data_rdm, dim=0)\n",
    "# energy_encoded_data_rdm = torch.cat(energy_encoded_data_rdm, dim=0)\n",
    "\n",
    "p1,p2,p3,p4 = post_samples[:,0:partition_size], post_samples[:,partition_size:2*partition_size], \\\n",
    "                                                 post_samples[:,2*partition_size:3*partition_size], post_samples[:,3*partition_size:4*partition_size]\n",
    "\n",
    "energy_rbm_data = []\n",
    "rbm_data = []\n",
    "# energy_rbm_rdm_data = []\n",
    "with torch.no_grad():\n",
    "    # for i in range(10):\n",
    "    for xx in val_loader:\n",
    "        in_data, true_energy, in_data_flat = engine._preprocess(xx[0],xx[1])\n",
    "        ##################################################\n",
    "        # true_energy = true_energy[:n_samples4_qpu,:]\n",
    "        # in_data = in_data[:n_samples4_qpu,:]\n",
    "        ##################################################\n",
    "        # if i == 0:\n",
    "            # p1, p2, p3, p4 = engine.model.stater.block_gibbs_sampling_ais(1.0)\n",
    "        # else:\n",
    "            # p1, p2, p3, p4 = engine.model.stater.block_gibbs_sampling_ais(1.0, p1, p2, p3, p4)\n",
    "        engine._model.sampler._batch_size = true_energy.shape[0]\n",
    "        if True:\n",
    "            u = engine.model.encoder.binary_energy(true_energy).to(dtype=torch.float32)\n",
    "            p1, p2, p3, p4 = engine.model.sampler.block_gibbs_sampling_cond(u)\n",
    "        else:\n",
    "            p1, p2, p3, p4 = engine.model.sampler.block_gibbs_sampling()\n",
    "        rbm_data.append(torch.cat((p1,p2,p3,p4),1))\n",
    "        rbm_samples_energy = engine.model.stater.energy_samples(p1, p2, p3, p4, 1.0)\n",
    "        energy_rbm_data.append(rbm_samples_energy.detach().cpu())\n",
    "        engine._model.sampler._batch_size = engine._config.engine.rbm_batch_size\n",
    "        \n",
    "        # if i == 0:\n",
    "        #     p1_r, p2_r, p3_r, p4_r = engine_2.model.stater.block_gibbs_sampling_ais(1.0)\n",
    "        # else:\n",
    "        #     p1_r, p2_r, p3_r, p4_r = engine_2.model.stater.block_gibbs_sampling_ais(1.0, p1_r, p2_r, p3_r, p4_r)\n",
    "        # rbm_rdm_samples_energy = engine_2.model.stater.energy_samples(p1_r, p2_r, p3_r, p4_r, 1.0)\n",
    "        # energy_rbm_rdm_data.append(rbm_rdm_samples_energy.detach().cpu())\n",
    "    \n",
    "energy_rbm_data = torch.cat(energy_rbm_data, dim=0)\n",
    "rbm_data = torch.cat(rbm_data,0)\n",
    "# energy_rbm_rdm_data = torch.cat(energy_rbm_rdm_data, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b903b53f-7ddb-494b-9efa-93dd0d10df40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.model.sampling_time_qpu\n",
    "engine.model.sampling_time_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83eb566d-836a-4fa3-a885-b17e65c2d6fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_data_qpu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m     engine\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39msampler\u001b[38;5;241m.\u001b[39m_batch_size \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39mrbm_batch_size\n\u001b[1;32m     37\u001b[0m     sample_data \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39m_reduceinv(sample_data, sample_energies, R\u001b[38;5;241m=\u001b[39mR)\n\u001b[0;32m---> 38\u001b[0m     sample_data_qpu \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39m_reduceinv(\u001b[43msample_data_qpu\u001b[49m, sample_energies_qpu, R\u001b[38;5;241m=\u001b[39mR)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m scaled:\n\u001b[1;32m     40\u001b[0m     in_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(engine\u001b[38;5;241m.\u001b[39m_data_mgr\u001b[38;5;241m.\u001b[39minv_transform(in_data\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_data_qpu' is not defined"
     ]
    }
   ],
   "source": [
    "# en_labels = []\n",
    "\n",
    "xtarget_samples = []\n",
    "xrecon_samples = []\n",
    "xgen_samples = []\n",
    "xgen_samples_qpu = []\n",
    "n_samples4_qpu = 200\n",
    "\n",
    "# xrecon_samples_2 = []\n",
    "\n",
    "# labelstarget_samples = []\n",
    "# labelsrecon_samples = []\n",
    "entarget_samples = []\n",
    "with torch.no_grad():\n",
    "    for xx in val_loader:\n",
    "    # for xx in train_loader:\n",
    "        in_data, true_energy, in_data_flat = engine._preprocess(xx[0],xx[1])\n",
    "        ###############################################\n",
    "        # true_energy = true_energy[:n_samples4_qpu,:]\n",
    "        # in_data = in_data[:n_samples4_qpu,:]\n",
    "        ##############################################\n",
    "        # print(in_data.shape)\n",
    "        if reducedata:\n",
    "            in_data = engine._reduce(in_data, true_energy, R=R)\n",
    "        fwd_output = engine.model((in_data, true_energy), False)\n",
    "        if reducedata:\n",
    "            in_data = engine._reduceinv(in_data, true_energy, R=R)\n",
    "            recon_data = engine._reduceinv(fwd_output.output_activations, true_energy, R=R)\n",
    "            engine._model.sampler._batch_size = true_energy.shape[0]\n",
    "            if True:\n",
    "                sample_energies, sample_data = engine._model.generate_samples_cond(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True)\n",
    "                # sample_energies_qpu, sample_data_qpu = engine.model.generate_samples_qpu_cond(true_energy=true_energy, num_samples=1, thrsh=30, beta=1/beta0)\n",
    "            else:\n",
    "                sample_energies, sample_data = engine._model.generate_samples(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True)\n",
    "                # sample_energies_qpu, sample_data_qpu = engine._model.generate_samples_qpu(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True, beta=1/beta0)\n",
    "            engine._model.sampler._batch_size = engine._config.engine.rbm_batch_size\n",
    "            sample_data = engine._reduceinv(sample_data, sample_energies, R=R)\n",
    "            sample_data_qpu = engine._reduceinv(sample_data_qpu, sample_energies_qpu, R=R)\n",
    "        elif scaled:\n",
    "            in_data = torch.tensor(engine._data_mgr.inv_transform(in_data.detach().cpu().numpy()))\n",
    "            recon_data = torch.tensor(engine._data_mgr.inv_transform(fwd_output.output_activations.detach().cpu().numpy()))\n",
    "            # recon_data_2 = torch.tensor(engine._data_mgr.inv_transform(fwd_just_act.detach().cpu().numpy()))\n",
    "            # recon_data_2 = torch.tensor(engine._data_mgr.inv_transform(fwd_energy_shift.detach().cpu().numpy()))\n",
    "            engine._model.sampler._batch_size = true_energy.shape[0]\n",
    "            \n",
    "            if True:\n",
    "                sample_energies, sample_data = engine._model.generate_samples_cond(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True)\n",
    "                # sample_energies_qpu, sample_data_qpu = engine.model.generate_samples_qpu_cond(true_energy=true_energy[:100,:], num_samples=1, thrsh=30, beta=1/beta0)\n",
    "            else:\n",
    "                sample_energies, sample_data = engine._model.generate_samples(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True)\n",
    "                # sample_energies_qpu, sample_data_qpu = engine._model.generate_samples_qpu(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True, beta=1/beta0)\n",
    "            engine._model.sampler._batch_size = engine._config.engine.rbm_batch_size\n",
    "            sample_data = torch.tensor(engine._data_mgr.inv_transform(sample_data.detach().cpu().numpy()))\n",
    "            # sample_data_qpu = torch.tensor(engine._data_mgr.inv_transform(sample_data_qpu.detach().cpu().numpy()))\n",
    "        else:\n",
    "            in_data = in_data.detach().cpu()*1000\n",
    "            recon_data = fwd_output.output_activations.detach().cpu()*1000\n",
    "            engine._model.sampler._batch_size = true_energy.shape[0]\n",
    "            sample_energies, sample_data = engine._model.generate_samples(num_samples=true_energy.shape[0], true_energy=true_energy)\n",
    "            engine._model.sampler._batch_size = engine._config.engine.rbm_batch_size\n",
    "            # sample_energies, sample_data = engine._model.generate_samples(num_samples=2048)\n",
    "            sample_data = sample_data.detach().cpu()*1000\n",
    "\n",
    "\n",
    "        xtarget_samples.append(in_data.detach().cpu())\n",
    "        xrecon_samples.append( recon_data.detach().cpu())\n",
    "        xgen_samples.append( sample_data.detach().cpu())\n",
    "        # xgen_samples_qpu.append( sample_data_qpu.detach().cpu())\n",
    "        entarget_samples.append(true_energy.detach().cpu())\n",
    "\n",
    "        # xrecon_samples_2.append( recon_data_2.detach().cpu())\n",
    "    \n",
    "    \n",
    "xtarget_samples = torch.cat(xtarget_samples, dim=0)\n",
    "xrecon_samples = torch.cat(xrecon_samples, dim=0)\n",
    "xgen_samples = torch.cat(xgen_samples, dim=0)\n",
    "xgen_samples_qpu = torch.cat(xgen_samples_qpu, dim=0)\n",
    "entarget_samples = torch.cat(entarget_samples, dim=0)\n",
    "\n",
    "# xrecon_samples_2 = torch.cat(xrecon_samples_2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58393531-88aa-44d0-8100-fffad7293fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR QPU Samples Generation\n",
    "# import time\n",
    "# # en_labels = []\n",
    "\n",
    "# xtarget_samples = []\n",
    "# xrecon_samples = []\n",
    "# xgen_samples = []\n",
    "# xgen_samples_qpu = []\n",
    "# n_samples4_qpu = 1000\n",
    "\n",
    "# # xrecon_samples_2 = []\n",
    "\n",
    "# # labelstarget_samples = []\n",
    "# # labelsrecon_samples = []\n",
    "# entarget_samples = []\n",
    "# with torch.no_grad():\n",
    "#     for xx in val_loader:\n",
    "#     # for xx in train_loader:\n",
    "#         in_data, true_energy, in_data_flat = engine._preprocess(xx[0],xx[1])\n",
    "#         ###############################################\n",
    "#         # true_energy = true_energy[:n_samples4_qpu,:]\n",
    "#         # in_data = in_data[:n_samples4_qpu,:]\n",
    "#         ##############################################\n",
    "#         # print(in_data.shape)\n",
    "#         if reducedata:\n",
    "#             in_data = engine._reduce(in_data, true_energy, R=R)\n",
    "#         fwd_output = engine.model((in_data, true_energy), False)\n",
    "#         if reducedata:\n",
    "#             in_data = engine._reduceinv(in_data, true_energy, R=R)\n",
    "#             recon_data = engine._reduceinv(fwd_output.output_activations, true_energy, R=R)\n",
    "#             engine._model.sampler._batch_size = true_energy.shape[0]\n",
    "#             if True:\n",
    "#                 sample_energies, sample_data = engine._model.generate_samples_cond(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True)\n",
    "#                 sample_energies_qpu, sample_data_qpu = engine.model.generate_samples_qpu_cond(true_energy=true_energy, num_samples=1, thrsh=15, beta=1/beta0)\n",
    "#             else:\n",
    "#                 sample_energies, sample_data = engine._model.generate_samples(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True)\n",
    "#                 # sample_energies_qpu, sample_data_qpu = engine._model.generate_samples_qpu(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True, beta=1/beta0)\n",
    "#             engine._model.sampler._batch_size = engine._config.engine.rbm_batch_size\n",
    "#             sample_data = engine._reduceinv(sample_data, sample_energies, R=R)\n",
    "#             sample_data_qpu = engine._reduceinv(sample_data_qpu, sample_energies_qpu, R=R)\n",
    "#         elif scaled:\n",
    "#             in_data = torch.tensor(engine._data_mgr.inv_transform(in_data.detach().cpu().numpy()))\n",
    "#             recon_data = torch.tensor(engine._data_mgr.inv_transform(fwd_output.output_activations.detach().cpu().numpy()))\n",
    "#             # recon_data_2 = torch.tensor(engine._data_mgr.inv_transform(fwd_just_act.detach().cpu().numpy()))\n",
    "#             # recon_data_2 = torch.tensor(engine._data_mgr.inv_transform(fwd_energy_shift.detach().cpu().numpy()))\n",
    "#             engine._model.sampler._batch_size = true_energy.shape[0]\n",
    "            \n",
    "#             if True:\n",
    "#                 sample_energies, sample_data = engine._model.generate_samples_cond(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True)\n",
    "#                 # sample_energies_qpu, sample_data_qpu = engine.model.generate_samples_qpu_cond(true_energy=true_energy[:100,:], num_samples=1, thrsh=30, beta=1/beta0)\n",
    "#             else:\n",
    "#                 sample_energies, sample_data = engine._model.generate_samples(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True)\n",
    "#                 # sample_energies_qpu, sample_data_qpu = engine._model.generate_samples_qpu(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True, beta=1/beta0)\n",
    "#             engine._model.sampler._batch_size = engine._config.engine.rbm_batch_size\n",
    "#             sample_data = torch.tensor(engine._data_mgr.inv_transform(sample_data.detach().cpu().numpy()))\n",
    "#             # sample_data_qpu = torch.tensor(engine._data_mgr.inv_transform(sample_data_qpu.detach().cpu().numpy()))\n",
    "#         else:\n",
    "#             in_data = in_data.detach().cpu()*1000\n",
    "#             recon_data = fwd_output.output_activations.detach().cpu()*1000\n",
    "#             engine._model.sampler._batch_size = true_energy.shape[0]\n",
    "#             sample_energies, sample_data = engine._model.generate_samples(num_samples=true_energy.shape[0], true_energy=true_energy)\n",
    "#             engine._model.sampler._batch_size = engine._config.engine.rbm_batch_size\n",
    "#             # sample_energies, sample_data = engine._model.generate_samples(num_samples=2048)\n",
    "#             sample_data = sample_data.detach().cpu()*1000\n",
    "\n",
    "\n",
    "#         xtarget_samples.append(in_data.detach().cpu())\n",
    "#         xrecon_samples.append( recon_data.detach().cpu())\n",
    "#         xgen_samples.append( sample_data.detach().cpu())\n",
    "#         xgen_samples_qpu.append( sample_data_qpu.detach().cpu())\n",
    "#         entarget_samples.append(true_energy.detach().cpu())\n",
    "\n",
    "#         time.sleep(2.5)\n",
    "\n",
    "#         # xrecon_samples_2.append( recon_data_2.detach().cpu())\n",
    "    \n",
    "    \n",
    "# xtarget_samples = torch.cat(xtarget_samples, dim=0)\n",
    "# xrecon_samples = torch.cat(xrecon_samples, dim=0)\n",
    "# xgen_samples = torch.cat(xgen_samples, dim=0)\n",
    "# xgen_samples_qpu = torch.cat(xgen_samples_qpu, dim=0)\n",
    "# entarget_samples = torch.cat(entarget_samples, dim=0)\n",
    "\n",
    "# # xrecon_samples_2 = torch.cat(xrecon_samples_2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ee7349-508d-4aed-89e2-22e734e8dbc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beta, beta_list, rbm_energy_list, dwave_energy_list, thrsh_met = engine.model.find_beta(num_reads=1024, beta_init=8.1, lr=0.01, num_epochs = 30, delta = 4.0, method = 2, \n",
    "                                                                                        TOL=True, const = 1.0, adaptive = True)\n",
    "beta0 = beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efefdca8-1a4d-4e62-a440-91d540f3c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(beta_list)), beta_list, linewidth=2.5, color=\"b\" )\n",
    "plt.plot(range(len(beta_list)), beta_list, linewidth=1.5, color=\"b\" )\n",
    "plt.xlabel(\"Iterations\", fontsize=15)\n",
    "plt.ylabel(\"Estimated $_{QA}$\", fontsize=15)\n",
    "plt.legend([f'Chip {engine.model._qpu_sampler.properties[\"chip_id\"]}'], fontsize=15)\n",
    "# plt.title(f'{ds[part]}')\n",
    "plt.grid(\"True\")\n",
    "plt.savefig(f'/home/luian1/CaloQVAE/figs/{modelname}/beta_QA_{engine.model._qpu_sampler.properties[\"chip_id\"]}_{modelname}_{arch}_{datascaled}_{part}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b488472-26d2-4a92-9b9f-a70d5f5a875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA = \"$_{QA}$\"\n",
    "Hoffset = -(sum([engine.model.prior.bias_dict[key].sum().detach().cpu().item() for key in engine.model.prior.bias_dict.keys()])/2 \n",
    "            + sum([engine.model.prior.weight_dict[key].sum().detach().cpu().item() for key in engine.model.prior.weight_dict.keys()])/4)\n",
    "print(Hoffset)\n",
    "\n",
    "minVal, maxVal = min(dwave_energy_list[-1] + Hoffset), max(dwave_energy_list[-1] + Hoffset)\n",
    "# minVal, maxVal = min(energy_dwave.detach().cpu().numpy()+ Hoffset/2), max(energy_dwave.detach().cpu().numpy()+ Hoffset/2)\n",
    "binwidth = (maxVal-minVal)/10\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "# plt.hist(energy_encoded_data.numpy(), bins=30, linewidth=2.5, color=\"b\", density=True, log=True)\n",
    "plt.hist(rbm_energy_list[-1] + Hoffset, density=True, color=\"m\", alpha=0.5, bins=np.arange(minVal, maxVal + binwidth, binwidth))\n",
    "plt.hist(dwave_energy_list[-1] + Hoffset, density=True, color=\"#4B0082\", alpha=0.5, bins=np.arange(minVal, maxVal + binwidth, binwidth))\n",
    "plt.xlabel(\"RBM Energy\", fontsize=15)\n",
    "plt.ylabel(\"PDF\", fontsize=15)\n",
    "plt.legend([\"Classical samples\", \"QPU samples\"], fontsize=14)\n",
    "# plt.title(f'ln(Z)[AIS] = {np.round(Zais)}, ln(Z)[RAIS] = {np.round(Zrais)}')\n",
    "plt.figtext(0.735, 0.6, f'Est. {QA} = {np.round(beta0, 2)}', ha='center', va='top', fontsize=17, bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=1'))\n",
    "plt.subplots_adjust(bottom=0.2)\n",
    "plt.grid(\"True\")\n",
    "plt.savefig(f'/home/luian1/CaloQVAE/figs/{modelname}/Ising_energy_{modelname}_{arch}_{datascaled}_{part}.png')\n",
    "plt.show()\n",
    "print(len(rbm_energy_list[-1]))\n",
    "print(len(dwave_energy_list[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1219db57-9d77-44f6-a7c9-40f75ea92a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir = '/home/luian1/CaloQVAE/data/QPUSamples/'\n",
    "# print(xtarget_samples.shape)\n",
    "# torch.save(xgen_samples_qpu, save_dir + modelname + 'qpusamples.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3656b1-e2a2-402f-88d1-ef483c3eb134",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(post_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352518f4-8e0e-469d-9ae1-407cc7f02717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "decoder_time = []\n",
    "for i in range(1000):\n",
    "    # start = time.process_time()\n",
    "    start = time.time()\n",
    "    output_hits, output_activations = engine.model.decoder(post_samples, true_energy)\n",
    "    torch.cuda.current_stream().synchronize()\n",
    "    t1 = time.time()\n",
    "    decoder_time.append(t1 - start)\n",
    "    # decoder_time.append(time.process_time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d89882-2779-4cd6-95ac-185a13664176",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtarget_samples_orig = xtarget_samples\n",
    "entarget_samples_orig = entarget_samples\n",
    "print(xtarget_samples, xtarget_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4c8dd6-0730-4915-adcb-32287076624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoder_time)\n",
    "plt.plot(decoder_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb65277-596d-4a09-a302-6ade635f1916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# New test data\n",
    "test_data = h5py.File('/fast_scratch_1/caloqvae/test_data/dataset_2_2.hdf5', 'r')\n",
    "print(test_data.keys())\n",
    "print(test_data['incident_energies'].shape)\n",
    "print(test_data['showers'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81402cb4-16e9-4e38-83f9-ab961e0efdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_showers = (np.array(test_data['showers']))\n",
    "data_showers = torch.tensor(data_showers)\n",
    "print(data_showers, data_showers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed3f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from good model\n",
    "# gen_samps = h5py.File('/fast_scratch_1/caloqvae/syn_data/wise-tree-1429.hdf5', 'r')\n",
    "gen_samps_orig = h5py.File('/fast_scratch_1/caloqvae/syn_data/dataset2_synthetic_denim-smoke-166en130.hdf5', 'r')\n",
    "# gen_samps = h5py.File('/fast_scratch_1/caloqvae/syn_data/dataset2_synthetic_dutiful-silence-108v2.hdf5', 'r')\n",
    "print(len(gen_samps))\n",
    "print(gen_samps.keys())\n",
    "print((gen_samps['incidence energy']).shape)\n",
    "print((gen_samps['showers']).shape)\n",
    "print(xtarget_samples.numpy().shape)\n",
    "factor = (xtarget_samples.numpy().shape)[0]/((gen_samps['showers']).shape)[0]\n",
    "print(factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b6c5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "entarget_samples = torch.tensor(gen_samps['incidence energy'])\n",
    "# xgen_samples = torch.tensor(gen_samps_orig['showers'])\n",
    "xgen_samples_1429 = torch.tensor(gen_samps['showers'])\n",
    "print(xgen_samples, xgen_samples.shape)\n",
    "print(xgen_samples_1429, xgen_samples_1429.shape)\n",
    "print(np.histogram((xgen_samples_1429 == 0).sum(dim=1) / xtarget_samples.shape[1], bins=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520a9e55-bf00-4841-a17b-6dfc11b3a420",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtarget_samples = data_showers\n",
    "print(\"Ground Truth: \", xtarget_samples.shape)\n",
    "print(\"Recon Size: \", xrecon_samples.shape)\n",
    "print(\"Samples Size: \", xgen_samples.shape)\n",
    "print(\"Comparison Samples Size: \", xgen_samples_1429.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4e2d2b-17ff-4836-bc5a-d64093314488",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.hist(energy_encoded_data.numpy(), bins=30, linewidth=2.5, color=\"b\", density=True, log=True)\n",
    "plt.hist(energy_rbm_data.numpy(), bins=30, color=\"orange\", density=True, fc=(1, 0, 1, 0.5), histtype='step', linewidth=1.5)\n",
    "# plt.hist(energy_rbm_rdm_data.numpy(), bins=20, linewidth=2.5, color=\"cyan\", density=True, fc=(0.5, 1.0, 0.5, 0.8))\n",
    "# plt.hist(energy_encoded_data_rdm.numpy(), bins=70, linewidth=2.5, color=\"r\", density=True)\n",
    "\n",
    "plt.xlabel(\"RBM Energy\", fontsize=15)\n",
    "plt.ylabel(\"PDF\", fontsize=15)\n",
    "plt.legend([\"Trained RBM w/ Encoded Data\", \"Trained RBM w/ Gibbs sampled data\", \"Random RBM w/ Gibbs sampled data\", \"Random RBM w/ Init Encoded Data\"], fontsize=17, loc='lower left')\n",
    "# plt.title(f'ln(Z)[AIS] = {np.round(Zais)}, ln(Z)[RAIS] = {np.round(Zrais)}')\n",
    "# plt.title(f'LL(trained) = {np.round(-energy_encoded_data.mean() - lnZais)}, LL(Rdm) = {np.round(-energy_encoded_data_rdm.mean() - lnZrais_rdm)} \\n \\\n",
    "        # LL(trained RBM data) = {np.round(-energy_rbm_data.mean() - lnZais)}, LL(Rdm RBM data) = {np.round(-energy_rbm_rdm_data.mean() - lnZrais_rdm)}')\n",
    "plt.grid(\"True\")\n",
    "# plt.savefig(f'/home/luian1/CaloQVAE/figs/{modelname}/RBM_energy_{modelname}_{arch}_{datascaled}_{part}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cb7d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hfont = {'fontname':'Helvetica'}\n",
    "ds = {'electron-ds2':'Dataset 2', 'pion1':'Dataset 1: '}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a04443",
   "metadata": {},
   "outputs": [],
   "source": [
    "minVal, maxVal = min(((xtarget_samples == 0).sum(dim=1)/xtarget_samples.shape[1]).numpy()), max(((xtarget_samples == 0).sum(dim=1)/xtarget_samples.shape[1]).numpy())\n",
    "binwidth = (maxVal-minVal)/30\n",
    "\n",
    "plt.hist(((xtarget_samples == 0).sum(dim=1)/xtarget_samples.shape[1]).numpy(), bins=np.arange(minVal, maxVal + binwidth, binwidth), log=True, density=True, histtype='stepfilled', linewidth=2.5, color=\"b\", alpha=0.7)\n",
    "plt.hist(((xrecon_samples == 0).sum(dim=1)/xtarget_samples.shape[1]).numpy(), bins=np.arange(minVal, maxVal + binwidth, binwidth), log=True, density=True, histtype='step', linewidth=2.5, color=\"c\")\n",
    "plt.hist(((xgen_samples == 0).sum(dim=1)/xtarget_samples.shape[1]).numpy(), bins=np.arange(minVal, maxVal + binwidth, binwidth), log=True, density=True, histtype='step', linewidth=2.5, color=\"orange\", linestyle=\"dashdot\")\n",
    "# plt.hist((xgen_samples_1429 == 0).sum(dim=1) / xtarget_samples.shape[1], bins=30, log=True, density=True, histtype='step', linewidth=2.5, color=\"purple\", linestyle=\"dashdot\")\n",
    "plt.hist((xgen_samples_qpu == 0).sum(dim=1) / xtarget_samples.shape[1], bins=np.arange(minVal, maxVal + binwidth, binwidth), log=True, density=True, histtype='step', linewidth=2.5, color=\"purple\", linestyle=\"dashdot\")\n",
    "\n",
    "plt.xlabel(\"Sparsity Index\")\n",
    "plt.ylabel(\"Histogram\")\n",
    "plt.legend([\"Ground Truth\", \"Reconstruction\", \"Classic Samples\", \"QPU Samples\"])\n",
    "# plt.title(f'{ds[part]}')\n",
    "plt.grid(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815a5f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HLF_1_electron.relevantLayers = [0,5,10,15,20,25,30,35,40,45]\n",
    "\n",
    "# Create lists for the histogram values\n",
    "target_counts, recon_counts, sample_counts = [0] * (len(HLF_1_electron.relevantLayers) - 1), [0] * (len(HLF_1_electron.relevantLayers) - 1), [0] * (len(HLF_1_electron.relevantLayers) - 1)\n",
    "target_bins, recon_bins, sample_bins = [0] * (len(HLF_1_electron.relevantLayers) - 1), [0] * (len(HLF_1_electron.relevantLayers) - 1), [0] * (len(HLF_1_electron.relevantLayers) - 1)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "# Create a grid of subplots\n",
    "fig, axes = plt.subplots(3,3, figsize=(12, 12), sharey=True, sharex=True, tight_layout=True)\n",
    "fig.text(0.5, -0.01, 'Sparsity Index', ha='center', fontsize=15)\n",
    "fig.text(-0.01, 0.5, 'Histogram', va='center', rotation='vertical', fontsize=15)\n",
    "\n",
    "# Iterate through the columns of X and plot histograms\n",
    "for i,_ in enumerate(HLF_1_electron.relevantLayers[:-1]):\n",
    "    row_index = i // 3  # Determine the row index\n",
    "    col_index = i % 3   # Determine the column index\n",
    "    \n",
    "    ax = axes[row_index, col_index]  # Get the current subplot\n",
    "    \n",
    "    # Plot histogram for the current column\n",
    "    idx = HLF_1_electron.relevantLayers[i+1]*9*16\n",
    "    idxPrev = (HLF_1_electron.relevantLayers[i])*9*16\n",
    "    l = idx - idxPrev\n",
    "    print(idx, idxPrev)\n",
    "    minVal, maxVal = min(((xtarget_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy()), max(((xtarget_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy())\n",
    "    binwidth = (maxVal-minVal)/30\n",
    "    \n",
    "    # Store the values of the histogram with the same normalization and bin sizes\n",
    "    target_counts[i], target_bins[i] = np.histogram(((xtarget_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), bins=np.arange(minVal, maxVal + binwidth, binwidth))\n",
    "    recon_counts[i], recon_bins[i] = np.histogram(((xrecon_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), bins=np.arange(minVal, maxVal + binwidth, binwidth))\n",
    "    sample_counts[i], sample_bins[i] = np.histogram(((xgen_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), bins=np.arange(minVal, maxVal + binwidth, binwidth))\n",
    "    ax.hist(((xtarget_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(),  bins=np.arange(minVal, maxVal+binwidth, binwidth), log=True, density=True, histtype='stepfilled', linewidth=2.5, color=\"b\", alpha=0.7)\n",
    "    ax.hist(((xrecon_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), bins=np.arange(minVal, maxVal+binwidth, binwidth), log=True, density=True, histtype='step', linewidth=2.5, color=\"c\")\n",
    "    ax.hist(((xgen_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), bins=np.arange(minVal, maxVal+binwidth, binwidth), log=True, density=True, histtype='step', linewidth=2.5, color=\"orange\", linestyle=\"dashdot\")\n",
    "    # ax.hist(((xgen_samples_1429[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), bins=np.arange(minVal, maxVal+binwidth, binwidth), log=True, density=True, histtype='step', linewidth=2.5, color=\"purple\", linestyle=\"dashdot\")\n",
    "    ax.hist(((xgen_samples_qpu[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), bins=np.arange(minVal, maxVal+binwidth, binwidth), log=True, density=True, histtype='step', linewidth=2.5, color=\"purple\", linestyle=\"dashdot\")\n",
    "    ax.legend([\"GT\", \"Recon\", \"Sample\", \"Sample w/ QPU\"], fontsize=14, loc = 'upper left')\n",
    "    ax.grid(\"True\")\n",
    "    \n",
    "    # Set labels and title for the subplot\n",
    "    # ax.set_xlabel(f'Column {i + 1}')\n",
    "    # ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'Layers {HLF_1_electron.relevantLayers[i]} to {HLF_1_electron.relevantLayers[i+1]-1}', fontsize=12)\n",
    "\n",
    "# Adjust layout and display the plots\n",
    "plt.tight_layout()\n",
    "os.makedirs(f'/home/luian1/CaloQVAE/figs/{modelname}/sparsity_per_layer_{modelname}_{arch}_{datascaled}_{part}', exist_ok=True)\n",
    "plt.savefig(f'/home/luian1/CaloQVAE/figs/{modelname}/sparsity_per_layer_{modelname}_{arch}_{datascaled}_{part}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfb28b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HLF_1_electron.relevantLayers = [0,5,10,15,20,25,30,35,40,45]\n",
    "\n",
    "# # Energy Ranges\n",
    "# E_right = 50000000000\n",
    "# E_left = 100000\n",
    "# tmp = (entarget_samples < E_right) * (entarget_samples > E_left)\n",
    "# idxEnFilter = (tmp == True).nonzero(as_tuple=True)[0]\n",
    "\n",
    "# # Create lists for the histogram values\n",
    "# target_counts, recon_counts, sample_counts = [0] * (len(HLF_1_electron.relevantLayers) - 1), [0] * (len(HLF_1_electron.relevantLayers) - 1), [0] * (len(HLF_1_electron.relevantLayers) - 1)\n",
    "# target_bins, recon_bins, sample_bins = [0] * (len(HLF_1_electron.relevantLayers) - 1), [0] * (len(HLF_1_electron.relevantLayers) - 1), [0] * (len(HLF_1_electron.relevantLayers) - 1)\n",
    "\n",
    "# plt.figure(figsize=(8,6))\n",
    "# # Create a grid of subplots\n",
    "# fig, axes = plt.subplots(3,3, figsize=(12, 12), sharey=True, sharex=True, tight_layout=True)\n",
    "# fig.text(0.5, -0.01, 'Sparsity Index', ha='center', fontsize=15)\n",
    "# fig.text(-0.01, 0.5, 'Histogram', va='center', rotation='vertical', fontsize=15)\n",
    "\n",
    "# # Iterate through the columns of X and plot histograms\n",
    "# for i,_ in enumerate(HLF_1_electron.relevantLayers[:-1]):\n",
    "#     row_index = i // 3  # Determine the row index\n",
    "#     col_index = i % 3   # Determine the column index\n",
    "    \n",
    "#     ax = axes[row_index, col_index]  # Get the current subplot\n",
    "    \n",
    "#     # Plot histogram for the current column\n",
    "#     idx = HLF_1_electron.relevantLayers[i+1]*9*16\n",
    "#     idxPrev = (HLF_1_electron.relevantLayers[i])*9*16\n",
    "#     l = idx - idxPrev\n",
    "#     minVal, maxVal = min(((xtarget_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy()), max(((xtarget_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy())\n",
    "#     binwidth = (maxVal-minVal)/30\n",
    "    \n",
    "#     # Store the values of the histogram with the same normalization and bin sizes\n",
    "#     target_counts[i], target_bins[i] = np.histogram(((xtarget_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), bins=np.arange(minVal, maxVal + binwidth, binwidth))\n",
    "#     recon_counts[i], recon_bins[i] = np.histogram(((xrecon_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), bins=np.arange(minVal, maxVal + binwidth, binwidth))\n",
    "#     sample_counts[i], sample_bins[i] = np.histogram(((xgen_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), bins=np.arange(minVal, maxVal + binwidth, binwidth))\n",
    "#     ax.hist((((xtarget_samples[idxEnFilter])[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(),  bins=np.arange(minVal, maxVal, binwidth), log=True, density=True, histtype='stepfilled', linewidth=2.5, color=\"b\", alpha=0.7)\n",
    "#     ax.hist((((xrecon_samples[idxEnFilter])[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), bins=np.arange(minVal, maxVal, binwidth), log=True, density=True, histtype='step', linewidth=2.5, color=\"c\")\n",
    "#     ax.hist((((xgen_samples[idxEnFilter])[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), bins=np.arange(minVal, maxVal, binwidth), log=True, density=True, histtype='step', linewidth=2.5, color=\"orange\", linestyle=\"dashdot\")\n",
    "#     ax.hist((((xgen_samples_1429[idxEnFilter])[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), bins=np.arange(minVal, maxVal, binwidth), log=True, density=True, histtype='step', linewidth=2.5, color=\"purple\", linestyle=\"dashdot\")\n",
    "#     # ax.hist(((xgen_samples_qpu[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), bins=30, log=True, histtype='step', linewidth=2.5, color=\"m\", linestyle=\"dashed\")\n",
    "#     if i == 0:\n",
    "#         ax.legend([\"GT\", \"Recon\", \"Sample\", \"Sample w/ QPU\"], fontsize=14, loc = 'upper left')\n",
    "#     ax.grid(\"True\")\n",
    "    \n",
    "#     # Set labels and title for the subplot\n",
    "#     # ax.set_xlabel(f'Column {i + 1}')\n",
    "#     # ax.set_ylabel('Frequency')\n",
    "#     ax.set_title(f'Layers {HLF_1_electron.relevantLayers[i]} to {HLF_1_electron.relevantLayers[i+1]-1}', fontsize=12)\n",
    "\n",
    "# # Adjust layout and display the plots\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f'/home/luian1/CaloQVAE/figs/{modelname}/sparsity_per_layer_{modelname}_{arch}_{datascaled}_{part}.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f6bab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_div(target, inp):\n",
    "    KL_met = 0\n",
    "    for i in range(min(len(inp), len(target))):\n",
    "        if target[i] == 0:\n",
    "            KL_met += 0\n",
    "        else:\n",
    "            KL_met += target[i] * np.log(abs(target[i]/inp[i]))\n",
    "    return KL_met / min(len(inp), len(target))\n",
    "\n",
    "def JSD(P, Q):\n",
    "    P = np.asarray(P, dtype=np.float)\n",
    "    Q = np.asarray(Q, dtype=np.float)\n",
    "    M = 0.5 * (P + Q)\n",
    "    return 0.5 * (KL_div(P, M) + KL_div(Q, M))\n",
    "\n",
    "def Hellinger_dist(target, inp):\n",
    "    Hel_dist = 0\n",
    "    for i in range(min(len(inp), len(target))):\n",
    "        Hel_dist += ((target[i]) ** 0.5 - (inp[i]) ** 0.5) ** 2\n",
    "    return (1 / (2 ** 0.5)) * ((Hel_dist) ** 0.5)\n",
    "\n",
    "# print(idxPrev, idx)\n",
    "# print(xtarget_samples.size()) # [# of data cylinders, # voxels per cylinder]\n",
    "# print((xtarget_samples[:, idxPrev:idx] == 0))\n",
    "# # l = idx = idxPrev\n",
    "# print((xtarget_samples[:, idxPrev:idx] == 0).sum(dim=1))\n",
    "# print(((xtarget_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), ((xtarget_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy().shape)\n",
    "\n",
    "# plt.plot(((xtarget_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy())\n",
    "# plt.show()\n",
    "\n",
    "# print(target_counts[0], recon_counts[0])\n",
    "# print(\"KL div b/w target and recon: \", KL_div(target_counts[0], recon_counts[0]))\n",
    "# print(\"KL div b/w target and gen: \", KL_div(((xtarget_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), ((xgen_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c99831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a Jensen-Shannon Divergence (JSD) a smoothed version of KL divergence to handle / 0\n",
    "\n",
    "layers = np.linspace(0, 44, 45)\n",
    "JSD_recon = np.zeros(45)\n",
    "JSD_gen1 = np.zeros(45)\n",
    "JSD_gen2 = np.zeros(45)\n",
    "\n",
    "HLD_recon = np.zeros(45)\n",
    "HLD_gen1 = np.zeros(45)\n",
    "HLD_gen2 = np.zeros(45)\n",
    "\n",
    "# Create lists for the histogram values\n",
    "target_counts, recon_counts, gen_counts, gen_counts_1429 = [0] * 44, [0] * 44, [0] * 44, [0] * 44\n",
    "target_bins, recon_bins, gen_bins, gen_bins_1429 = [0] * 44, [0] * 44, [0] * 44, [0] * 44\n",
    "\n",
    "for i in range(44): # of layers in the cylinder - 1\n",
    "    idx = (i+1)*9*16\n",
    "    idxPrev = (i)*9*16\n",
    "    l = idx - idxPrev\n",
    "    minVal, maxVal = min(((xtarget_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy()), max(((xtarget_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy())\n",
    "    binwidth = (maxVal-minVal)/30\n",
    "    target_counts[i], target_bins[i] = np.histogram(((xtarget_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), bins=np.arange(minVal, maxVal + binwidth, binwidth), density=True)\n",
    "    recon_counts[i], recon_bins[i] = np.histogram(((xrecon_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), bins=np.arange(minVal, maxVal + binwidth, binwidth), density=True)\n",
    "    gen_counts[i], gen_bins[i] = np.histogram(((xgen_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), bins=np.arange(minVal, maxVal + binwidth, binwidth), density=True)\n",
    "    gen_counts_1429[i], gen_bins_1429[i] = np.histogram(((xgen_samples_1429[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), bins=np.arange(minVal, maxVal + binwidth, binwidth), density=True)\n",
    "\n",
    "#     gen_counts[i], gen_bins[i] = np.histogram(((xgen_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), bins=np.arange(minVal, maxVal + binwidth, binwidth), weights = np.ones_like((xgen_samples[:, idxPrev:idx] == 0).sum(dim=1)/l) * 0.1)\n",
    "    mid_recon = 0.5 * (target_counts[i] + recon_counts[i])\n",
    "    mid_gen1 = 0.5 * (target_counts[i] + gen_counts[i])\n",
    "    mid_gen2 = 0.5 * (target_counts[i] + gen_counts_1429[i])\n",
    "    \n",
    "#     print(mid_recon, target_counts[0].tolist())\n",
    "    \n",
    "#     print(\"testing: \", KL_div(((xtarget_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), mid_recon))\n",
    "#     print((0.5 * (KL_div(((xtarget_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), mid_recon) + KL_div(((xrecon_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), mid_recon))))\n",
    "   \n",
    "    JSD_recon[i] = (0.5 * (KL_div(target_counts[i].tolist(), mid_recon) + KL_div(recon_counts[i].tolist(), mid_recon)))\n",
    "    JSD_gen1[i] = (0.5 * (KL_div(target_counts[i].tolist(), mid_gen1) + KL_div(gen_counts[i].tolist(), mid_gen1)))\n",
    "    JSD_gen2[i] = (0.5 * (KL_div(target_counts[i].tolist(), mid_gen2) + KL_div(gen_counts_1429[i].tolist(), mid_gen2)))\n",
    "    \n",
    "    HLD_recon[i] = Hellinger_dist(target_counts[i].tolist(), recon_counts[i].tolist())\n",
    "    HLD_gen1[i] = Hellinger_dist(target_counts[i].tolist(), gen_counts[i].tolist())\n",
    "    HLD_gen2[i] = Hellinger_dist(target_counts[i].tolist(), gen_counts_1429[i].tolist())\n",
    "\n",
    "print(\"Layer Size: \", layers.size)\n",
    "print(\"JSD Size: \", len(JSD_recon))\n",
    "plt.scatter(layers[:-1], np.asarray(JSD_recon[:-1]))\n",
    "plt.scatter(layers[:-1], np.asarray(JSD_gen1[:-1]))\n",
    "plt.scatter(layers[:-1], np.asarray(JSD_gen2[:-1]))\n",
    "\n",
    "plt.xlabel(\"Layer Number\")\n",
    "plt.ylabel(\"JSD Divergence Value\")\n",
    "plt.legend([\"Target vs Recon\", \"Target vs Sample 1372\", \"Target vs Sample 454\"])\n",
    "plt.title(\"JSD Divergence Comparison per Layer\")\n",
    "plt.grid(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb405ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(layers[:-1], np.asarray(HLD_recon[:-1]))\n",
    "plt.scatter(layers[:-1], np.asarray(HLD_gen1[:-1]))\n",
    "plt.scatter(layers[:-1], np.asarray(HLD_gen2[:-1]))\n",
    "\n",
    "\n",
    "plt.xlabel(\"Layer Number\")\n",
    "plt.ylabel(\"Hellinger Distance\")\n",
    "plt.legend([\"Target vs Recon\", \"Target vs Sample 1372\", \"Target vs Sample 454\"])\n",
    "plt.title(\"Hellinger Distance Comparison per Layer\")\n",
    "plt.grid(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d879c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(entarget_samples)\n",
    "entarget_samplesnp = entarget_samples.numpy()\n",
    "ind = np.argsort(entarget_samplesnp, axis = 0).flatten()\n",
    "print(ind)\n",
    "print(entarget_samples[ind])\n",
    "sortxtarget_samples = xtarget_samples[ind]\n",
    "print(sortxtarget_samples, sortxtarget_samples.shape)\n",
    "print(max(sortxtarget_samples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2933f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = np.logspace(0, np.log(1000000), 44, base=10)\n",
    "# print(test)\n",
    "\n",
    "# idxPrev = test[0]\n",
    "# print(entarget_samples, entarget_samples.shape)\n",
    "# for i in range(44):\n",
    "#     print(len(entarget_samples[:, int(test[i]): int(test[i+1])]))\n",
    "\n",
    "x=np.zeros([4,4])\n",
    "x[0][0] = 1\n",
    "print(x)\n",
    "\n",
    "samps = np.linspace(0, 44, 12)\n",
    "print(samps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f3d716-f62d-4793-8bee-d07fd4e392a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_samps = h5py.File('/fast_scratch_1/caloqvae/syn_data/dataset2_synthetic_robust-serenity-216en150.hdf5', 'r')\n",
    "xgen_samples_1429 = torch.tensor(gen_samps['showers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6933f8-c7bc-44c4-913b-6b21abf3d4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xtarget_samples.shape, xgen_samples_1429.shape, entarget_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74c90d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "samps = np.linspace(0, len(entarget_samples) - 1, 10)\n",
    "\n",
    "layers = np.linspace(0, 44, 45)\n",
    "print(layers)\n",
    "# JSD_recon = np.zeros([44,44])\n",
    "JSD_gen = np.zeros([45,45])\n",
    "\n",
    "# HLD_recon = np.zeros([44,44])\n",
    "HLD_gen = np.zeros([45,45])\n",
    "\n",
    "# Sort samples\n",
    "entarget_samplesnp = entarget_samples.numpy()\n",
    "ind = np.argsort(entarget_samplesnp, axis=0).flatten()\n",
    "sortentarget_samples = entarget_samples[ind]\n",
    "sortxtarget_samples = xtarget_samples[ind]\n",
    "# sortxrecon_samples = xrecon_samples[ind]\n",
    "sortxgen_samples = xgen_samples_1429[ind]\n",
    "inc_energies = np.linspace(0, len(entarget_samples), 46).astype(int)\n",
    "\n",
    "xenergies = (((sortentarget_samples[samps] / 1).flatten()).numpy().astype(int)).tolist()\n",
    "\n",
    "minVal, maxVal = 0, 1\n",
    "binwidth = (maxVal-minVal)/20\n",
    "\n",
    "for j in range(45):\n",
    "    \n",
    "    # Create lists for the histogram values\n",
    "    target_counts, gen_counts = [0] * 45, [0] * 45\n",
    "    target_bins, gen_bins = [0] * 45, [0] * 45\n",
    "\n",
    "    sub_target_samples = sortxtarget_samples[inc_energies[j] : inc_energies[j+1]]\n",
    "    # sub_recon_samples = sortxrecon_samples[inc_energies[j] : inc_energies[j+1]]\n",
    "    sub_gen_samples = sortxgen_samples[inc_energies[j] : inc_energies[j+1]]\n",
    "\n",
    "    for i in range(45): # of layers in the cylinder - 1\n",
    "\n",
    "        idx = (i+1)*9*16\n",
    "        idxPrev = (i)*9*16\n",
    "        l = idx - idxPrev\n",
    "        \n",
    "        target_counts[i], target_bins[i] = np.histogram(((sub_target_samples[:, idxPrev:idx] == 0).sum(axis=1)/l).numpy(), bins=np.arange(minVal, maxVal + binwidth, binwidth))\n",
    "        # recon_counts[i], recon_bins[i] = np.histogram(((sub_recon_samples[:, idxPrev:idx] == 0).sum(axis=1)/l).numpy(), bins=np.arange(minVal, maxVal + binwidth, binwidth), density=True)\n",
    "        gen_counts[i], gen_bins[i] = np.histogram(((sub_gen_samples[:, idxPrev:idx] == 0).sum(axis=1)/l).numpy(), bins=np.arange(minVal, maxVal + binwidth, binwidth))\n",
    "        # mid_recon = 0.5 * (target_counts[i] + recon_counts[i])\n",
    "        mid_gen = 0.5 * (target_counts[i] + gen_counts[i])\n",
    "\n",
    "        # JSD_recon[j][i] = (0.5 * (KL_div(target_counts[i].tolist(), mid_recon) + KL_div(recon_counts[i].tolist(), mid_recon)))\n",
    "        JSD_gen[j][i] = (0.5 * (KL_div(target_counts[i].tolist(), mid_gen) + KL_div(gen_counts[i].tolist(), mid_gen)))\n",
    "\n",
    "        # HLD_recon[j][i] = Hellinger_dist(target_counts[i].tolist(), recon_counts[i].tolist())\n",
    "        HLD_gen[j][i] = Hellinger_dist(target_counts[i].tolist(), gen_counts[i].tolist())\n",
    "\n",
    "        if i == 0: \n",
    "            print(target_counts, len(target_counts))\n",
    "            print(gen_counts)\n",
    "\n",
    "# print(JSD_recon)\n",
    "# print(min(JSD_recon.flatten()))\n",
    "# print(JSD_recon, JSD_recon.shape)\n",
    "\n",
    "# Plotting for JSD_gen\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "cax = ax.imshow(JSD_gen, interpolation='nearest', cmap='viridis')\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title(\"Ground Truth vs Model Prediction (Sparsity Index)\")\n",
    "ax.set_yticks(np.linspace(0, 44, 10))\n",
    "ax.set_yticklabels([int(x) for x in xenergies])\n",
    "ax.set_ylabel(\"Incident Energy Ranges (MeV)\")\n",
    "ax.set_xlabel(\"Layer Number\")\n",
    "\n",
    "divider = make_axes_locatable(ax)\n",
    "cax_cbar = divider.append_axes(\"right\", size=\"5%\", pad=0.25)\n",
    "cbar = fig.colorbar(cax, cax=cax_cbar)\n",
    "cbar.set_label('JSD Divergence Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb739bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for HLD_recon and HLD_gen\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "fig, ax2 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot for HLD_recon\n",
    "# cax1 = ax1.imshow(HLD_recon, interpolation='nearest', cmap='viridis')\n",
    "# ax1.set_aspect('equal')\n",
    "# ax1.set_yticks(np.linspace(0, 43, 10))\n",
    "# ax1.set_yticklabels([int(x) for x in xenergies])\n",
    "# ax1.set_ylabel(\"Incident Energy Ranges\")\n",
    "# ax1.set_xlabel(\"Layer Number\")\n",
    "# divider1 = make_axes_locatable(ax1)\n",
    "# cax_cbar1 = divider1.append_axes(\"right\", size=\"5%\", pad=0.25)\n",
    "# cbar1 = fig.colorbar(cax1, cax=cax_cbar1)\n",
    "# cbar1.set_label('Hellinger Distance Value')\n",
    "\n",
    "# Plot for HLD_gen\n",
    "cax2 = ax2.imshow(HLD_gen, interpolation='nearest', cmap='viridis')\n",
    "ax2.set_aspect('equal')\n",
    "ax2.set_yticks(np.linspace(0, 43, 10))\n",
    "ax2.set_yticklabels([int(x) for x in xenergies])\n",
    "ax2.set_ylabel(\"Incident Energy Ranges\")\n",
    "ax2.set_xlabel(\"Layer Number\")\n",
    "divider2 = make_axes_locatable(ax2)\n",
    "cax_cbar2 = divider2.append_axes(\"right\", size=\"5%\", pad=0.25)\n",
    "cbar2 = fig.colorbar(cax2, cax=cax_cbar2)\n",
    "cbar2.set_label('Hellinger Distance Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e708a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(entarget_samples, ((xtarget_samples == 0).sum(dim=1)/xtarget_samples.shape[1]).numpy())\n",
    "plt.scatter(entarget_samples, ((xrecon_samples == 0).sum(dim=1)/xtarget_samples.shape[1]).numpy())\n",
    "\n",
    "plt.xlabel(\"Incidence Energy (GeV)\")\n",
    "plt.ylabel(\"Sparsity\")\n",
    "plt.legend([\"GT\", \"Recon\", \"Samples\", \"Sample /w QPU\"])\n",
    "plt.title(f'{ds[part]}')\n",
    "plt.grid(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421e80bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3,2, figsize=(8,8), tight_layout=True)\n",
    "fig.text(0.5, 0.0, 'GT energy per event (GeV)', ha='center')\n",
    "fig.text(0.5, 1.0, f'{ds[part]}', ha='center', fontsize=12)\n",
    "\n",
    "axes[0,0].scatter(xtarget_samples.sum(dim=1).numpy()/1000, xrecon_samples.sum(dim=1).numpy()/1000, marker='.', alpha=.5, color=\"b\")\n",
    "axes[0,0].plot([0,800],[0,800], c='red', lw=2.5, label='y=x')\n",
    "axes[0,0].set_ylabel(\"Recon energy per event (GeV)\")\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(\"True\")\n",
    "axes[0,0].set_yscale('log')\n",
    "axes[0,0].set_xscale('log')\n",
    "\n",
    "axes[0,1].scatter(xtarget_samples.sum(dim=1).numpy()/1000, (xtarget_samples.sum(dim=1).numpy() - xrecon_samples.sum(dim=1).numpy())/1000, marker='.', alpha=.5, color=\"b\")\n",
    "axes[0,1].set_ylabel(\"Abs error (GeV)\")\n",
    "axes[0,1].set_ylim([-40,40])\n",
    "# axes[0,1].legend()\n",
    "# axes[0,1].set_yscale('log')\n",
    "axes[0,1].grid(\"True\")\n",
    "\n",
    "axes[1,0].scatter(1/(entarget_samples.numpy()/1000), np.abs(entarget_samples.reshape(-1).numpy() - xtarget_samples.sum(dim=1).numpy())/(entarget_samples.reshape(-1).numpy()+1e-3), marker='.', alpha=.5, color=\"blue\", label=\"Simulation\")\n",
    "axes[1,0].scatter(1/(entarget_samples.numpy()/1000), np.abs(entarget_samples.reshape(-1).numpy() - xrecon_samples.sum(dim=1).numpy())/(entarget_samples.reshape(-1).numpy()+1e-3), marker='.', alpha=.05, color=\"red\", label=\"Model\")\n",
    "axes[1,0].plot([1e-3,1e0],np.power([1e-3,1e0],0.25)*np.power(10,2.0), c='orange', lw=2.5, label='slope=0.25', linestyle=\"dashdot\")\n",
    "axes[1,0].grid(\"True\")\n",
    "axes[1,0].set_yscale('log')\n",
    "axes[1,0].set_xscale('log')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].set_ylabel(\"Relative Error\")\n",
    "\n",
    "axes[1,1].scatter(entarget_samples.numpy()/1000, np.abs(entarget_samples.reshape(-1).numpy()/1000 - xtarget_samples.sum(dim=1).numpy()/1000), marker='.', alpha=1, color=\"blue\", label=\"Simulation\")\n",
    "axes[1,1].scatter(entarget_samples.numpy()/1000, np.abs(entarget_samples.reshape(-1).numpy()/1000 - xrecon_samples.sum(dim=1).numpy()/1000), marker='.', alpha=.2, color=\"red\", label=\"Model\")\n",
    "axes[1,1].plot([1,800],np.sqrt([1,800]), c='orange', lw=2.5, label='y=sqrt(x)', linestyle=\"dashdot\")\n",
    "axes[1,1].plot([1,800],[1,800], c='c', lw=2.5, label='y=x', linestyle=\"dashed\")\n",
    "axes[1,1].grid(\"True\")\n",
    "axes[1,1].set_yscale('log')\n",
    "axes[1,1].set_xscale('log')\n",
    "axes[1,1].set_ylabel(\"Absolute Error\")\n",
    "axes[1,1].legend()\n",
    "\n",
    "\n",
    "# Merge the first row's axes\n",
    "gs = axes[2, 0].get_gridspec()\n",
    "for ax in axes[2, :]:\n",
    "    ax.remove()\n",
    "ax_big = fig.add_subplot(gs[2, :])\n",
    "ax_big.scatter(1/(xtarget_samples.sum(dim=1).numpy()/1000), np.abs(xtarget_samples.sum(dim=1).numpy() - xrecon_samples.sum(dim=1).numpy())/(xtarget_samples.sum(dim=1).numpy()+1e-3), marker='.', alpha=.5, color=\"blue\")\n",
    "ax_big.plot([1e-3,1e1],np.sqrt([1e-3,1e1])*np.power(1,3.5), c='orange', lw=2.5, label='y=sqrt(x)', linestyle=\"dashdot\")\n",
    "ax_big.grid(\"True\")\n",
    "ax_big.set_yscale('log')\n",
    "ax_big.set_xscale('log')\n",
    "ax_big.legend()\n",
    "ax_big.set_ylim([1e-5,1e1])\n",
    "ax_big.set_yticks([1e-5,1e-4,1e-3,1e-2,1e-1,1e0,1e1])\n",
    "ax_big.set_ylabel(\"Relative Recon Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece6dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d90dd-a7fd-47ff-9fc9-2ed331b49dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "minVal, maxVal = min(xtarget_samples.sum(dim=1).numpy()/1000), max(xtarget_samples.sum(dim=1).numpy()/1000)\n",
    "binwidth = (maxVal-minVal)/30\n",
    "\n",
    "plt.hist(xtarget_samples.sum(dim=1).numpy()/1000, bins=np.arange(minVal, maxVal + binwidth, binwidth), log=True, density=True, histtype='stepfilled', linewidth=2.5, color=\"b\", alpha=0.7)\n",
    "plt.hist(xrecon_samples.sum(dim=1).numpy()/1000, bins=np.arange(minVal, maxVal + binwidth, binwidth), log=True, density=True, histtype='step', linewidth=2.5, color=\"c\")\n",
    "plt.hist(xgen_samples.sum(dim=1).numpy()/1000, bins=np.arange(minVal, maxVal + binwidth, binwidth), log=True, density=True, histtype='step', linewidth=2.5, color=\"orange\", linestyle=\"dashdot\")\n",
    "# plt.hist(xgen_samples_1429.sum(dim=1).numpy()/1000, bins=30, log=True, density=True, histtype='step', linewidth=2.5, color=\"purple\", linestyle=\"dashdot\")\n",
    "plt.hist(xgen_samples_qpu.sum(dim=1).numpy()/1000, bins=np.arange(minVal, maxVal + binwidth, binwidth), log=True, density=True, histtype='step', linewidth=2.5, color=\"purple\", linestyle=\"dashdot\")\n",
    "# plt.hist(((xgen_samples_1429 == 0).sum(axis=1)/xtarget_samples.shape[1]).numpy(), bins=20, log=True, histtype='step', linewidth=2.5, color=\"orange\", linestyle=\"dashdot\")\n",
    "\n",
    "plt.xlabel(\"Energy per Event (GeV)\")\n",
    "plt.ylabel(\"Histogram\")\n",
    "plt.legend([\"Ground Truth\", \"Reconstruction\", \"Classic Samples\", \"QPU Samples\"])\n",
    "# plt.title(\"Energy Histogram\")\n",
    "plt.grid(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c616f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "HLF_1_electron.relevantLayers = [0,5,10,15,20,25,30,35,40,45]\n",
    "\n",
    "# Create lists for the histogram values\n",
    "target_counts, recon_counts, sample_counts = [0] * (len(HLF_1_electron.relevantLayers) - 1), [0] * (len(HLF_1_electron.relevantLayers) - 1), [0] * (len(HLF_1_electron.relevantLayers) - 1)\n",
    "target_bins, recon_bins, sample_bins = [0] * (len(HLF_1_electron.relevantLayers) - 1), [0] * (len(HLF_1_electron.relevantLayers) - 1), [0] * (len(HLF_1_electron.relevantLayers) - 1)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "# Create a grid of subplots\n",
    "fig, axes = plt.subplots(3,3, figsize=(12, 12), sharey=True, sharex=False, tight_layout=True)\n",
    "fig.text(0.5, 0.0, 'Energy per event (GeV)', ha='center', fontsize=15)\n",
    "fig.text(0.0, 0.5, 'Histogram', va='center', rotation='vertical', fontsize=15)\n",
    "\n",
    "# Iterate through the columns of X and plot histograms\n",
    "for i,_ in enumerate(HLF_1_electron.relevantLayers[:-1]):\n",
    "    row_index = i // 3  # Determine the row index\n",
    "    col_index = i % 3   # Determine the column index\n",
    "    \n",
    "    ax = axes[row_index, col_index]  # Get the current subplot\n",
    "\n",
    "    minVal, maxVal = min(xtarget_samples[:, idxPrev:idx].sum(dim=1).numpy()/1000), max(xtarget_samples[:, idxPrev:idx].sum(dim=1).numpy()/1000)\n",
    "    binwidth = (maxVal-minVal)/30\n",
    "    \n",
    "    # Plot histogram for the current column\n",
    "    idx = HLF_1_electron.relevantLayers[i+1]*9*16\n",
    "    idxPrev = (HLF_1_electron.relevantLayers[i])*9*16\n",
    "    l = idx - idxPrev\n",
    "    \n",
    "    # Store the values of the histogram with the same normalization and bin sizes\n",
    "    target_counts[i], target_bins[i] = np.histogram(xtarget_samples[:, idxPrev:idx].sum(dim=1).numpy()/1000, bins=20)\n",
    "    recon_counts[i], recon_bins[i] = np.histogram(xrecon_samples[:, idxPrev:idx].sum(dim=1).numpy()/1000, bins=20)\n",
    "    sample_counts[i], sample_bins[i] = np.histogram(xgen_samples[:, idxPrev:idx].sum(dim=1).numpy()/1000, bins=20)\n",
    "    \n",
    "    ax.hist(xtarget_samples[:, idxPrev:idx].sum(dim=1).numpy()/1000,  bins=np.arange(minVal, maxVal + binwidth, binwidth), log=True, density=True, histtype='stepfilled', linewidth=2.5, color=\"b\", alpha=0.7)\n",
    "    ax.hist(xrecon_samples[:, idxPrev:idx].sum(dim=1).numpy()/1000, bins=np.arange(minVal, maxVal + binwidth, binwidth), log=True, density=True, histtype='step', linewidth=2.5, color=\"c\")\n",
    "    ax.hist(xgen_samples[:, idxPrev:idx].sum(dim=1).numpy()/1000, bins=np.arange(minVal, maxVal + binwidth, binwidth), log=True, density=True, histtype='step', linewidth=2.5, color=\"orange\", linestyle=\"dashdot\")\n",
    "    # ax.hist(xgen_samples_1429[:, idxPrev:idx].sum(dim=1).numpy()/1000, bins=np.arange(minVal, maxVal + binwidth, binwidth), log=True, density=True, histtype='step', linewidth=2.5, color=\"purple\", linestyle=\"dashdot\")\n",
    "    ax.hist(xgen_samples_qpu[:, idxPrev:idx].sum(dim=1).numpy()/1000, bins=np.arange(minVal, maxVal + binwidth, binwidth), log=True, density=True, histtype='step', linewidth=2.5, color=\"purple\", linestyle=\"dashdot\")\n",
    "\n",
    "    if i == 0:\n",
    "        ax.legend([\"GT\", \"Recon\", \"Sample\", \"Sample w/ QPU\"], fontsize=15, loc='upper right')\n",
    "    ax.grid(\"True\")\n",
    "    \n",
    "    # Set labels and title for the subplot\n",
    "    # ax.set_xlabel(f'Column {i + 1}')\n",
    "    # ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'Layers {HLF_1_electron.relevantLayers[i]} to {HLF_1_electron.relevantLayers[i+1]-1}')\n",
    "\n",
    "# Adjust layout and display the plots\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'/home/luian1/CaloQVAE/figs/{modelname}/sparsity_per_layer_{modelname}_{arch}_{datascaled}_{part}.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2576009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xtarget_samples.shape)\n",
    "\n",
    "E_right = 12000 # MeV\n",
    "E_left = 10000 # MeV\n",
    "tmp = (entarget_samples < E_right) * (entarget_samples > E_left)\n",
    "idxEnFilter = (tmp == True).nonzero(as_tuple=True)[0]\n",
    "print(idxEnFilter.shape)\n",
    "print(xtarget_samples, xtarget_samples.shape)\n",
    "# print(entarget_samples[idxEnFilter])\n",
    "print(xtarget_samples[idxEnFilter], xtarget_samples[idxEnFilter].shape)\n",
    "len(idxEnFilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3a7e26-b2df-4aa2-84a2-1b20d830b3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtarget_samples = data_showers\n",
    "print(xtarget_samples, xtarget_samples.shape)\n",
    "print(xgen_samples_1429, xgen_samples_1429.shape)\n",
    "print(entarget_samples, entarget_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dbda19-6880-4252-b263-d5c89cf766c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xrecon_samples, xrecon_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c1318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a Jensen-Shannon Divergence (JSD) a smoothed version of KL divergence to handle / 0\n",
    "\n",
    "layers = np.linspace(0, 44, 45)\n",
    "JSD_recon = np.zeros(45)\n",
    "JSD_gen1 = np.zeros(45)\n",
    "JSD_gen2 = np.zeros(45)\n",
    "\n",
    "HLD_recon = np.zeros(45)\n",
    "HLD_gen1 = np.zeros(45)\n",
    "HLD_gen2 = np.zeros(45)\n",
    "\n",
    "# Create lists for the histogram values\n",
    "target_counts, recon_counts, gen_counts, gen_counts_1429 = [0] * 44, [0] * 44, [0] * 44, [0] * 44\n",
    "target_bins, recon_bins, gen_bins, gen_bins_1429 = [0] * 44, [0] * 44, [0] * 44, [0] * 44\n",
    "\n",
    "# Energy Ranges\n",
    "E_right = 5000000000\n",
    "E_left = 0\n",
    "tmp = (entarget_samples < E_right) * (entarget_samples > E_left)\n",
    "idxEnFilter = (tmp == True).nonzero(as_tuple=True)[0]\n",
    "\n",
    "# Energy filtered samples\n",
    "print(xtarget_samples)\n",
    "print(xtarget_samples.shape, idxEnFilter.shape)\n",
    "# xtarget_samples, xrecon_samples, gen_samples = xtarget_samples[idxEnFilter], xrecon_samples[idxEnFilter], xgen_samples[idxEnFilter] # this line creates problems when running it again \n",
    "\n",
    "for i in range(44): # of layers in the cylinder - 1\n",
    "    idx = (i+1)*9*16\n",
    "    idxPrev = (i)*9*16\n",
    "    l = idx - idxPrev\n",
    "    \n",
    "    # target_counts[i], target_bins[i] = np.histogram((xtarget_samples[idxEnFilter])[:, idxPrev:idx].sum(dim=1).numpy()/1000, bins=20, density=True)\n",
    "    # recon_counts[i], recon_bins[i] = np.histogram((xrecon_samples[idxEnFilter < xrecon_samples.shape[0]])[:, idxPrev:idx].sum(dim=1).numpy()/1000, bins=20,density=True)\n",
    "    # gen_counts[i], gen_bins[i] = np.histogram((xgen_samples[idxEnFilter])[:, idxPrev:idx].sum(dim=1).numpy()/1000, bins=20, density=True)\n",
    "    # gen_counts_1429[i], gen_bins_1429[i] = np.histogram((xgen_samples[idxEnFilter])[:, idxPrev:idx].sum(dim=1).numpy()/1000, bins=20, density=True)\n",
    "\n",
    "    target_counts[i], target_bins[i] = np.histogram(((xtarget_samples)[:, idxPrev:idx]).sum(dim=1).numpy()/1000, bins=20, density=True)\n",
    "    recon_counts[i], recon_bins[i] = np.histogram(((xrecon_samples)[:, idxPrev:idx]).sum(dim=1).numpy()/1000, bins=20,density=True)\n",
    "    gen_counts[i], gen_bins[i] = np.histogram(((xgen_samples)[:, idxPrev:idx]).sum(dim=1).numpy()/1000, bins=20, density=True)\n",
    "    gen_counts_1429[i], gen_bins_1429[i] = np.histogram(((xgen_samples_1429)[:, idxPrev:idx]).sum(dim=1).numpy()/1000, bins=20, density=True)\n",
    "    \n",
    "    mid_recon = 0.5 * (target_counts[i] + recon_counts[i])\n",
    "    mid_gen1 = 0.5 * (target_counts[i] + gen_counts[i])\n",
    "    mid_gen2 = 0.5 * (target_counts[i] + gen_counts_1429[i])\n",
    "    \n",
    "#     print(mid_recon, target_counts[0].tolist())\n",
    "    \n",
    "#     print(\"testing: \", KL_div(((xtarget_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), mid_recon))\n",
    "#     print((0.5 * (KL_div(((xtarget_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), mid_recon) + KL_div(((xrecon_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), mid_recon))))\n",
    "   \n",
    "    JSD_recon[i] = (0.5 * (KL_div(target_counts[i].tolist(), mid_recon) + KL_div(recon_counts[i].tolist(), mid_recon)))\n",
    "    JSD_gen1[i] = (0.5 * (KL_div(target_counts[i].tolist(), mid_gen1) + KL_div(gen_counts[i].tolist(), mid_gen1)))\n",
    "    JSD_gen2[i] = (0.5 * (KL_div(target_counts[i].tolist(), mid_gen2) + KL_div(gen_counts_1429[i].tolist(), mid_gen2)))\n",
    "    \n",
    "    HLD_recon[i] = Hellinger_dist(target_counts[i].tolist(), recon_counts[i].tolist())\n",
    "    HLD_gen1[i] = Hellinger_dist(target_counts[i].tolist(), gen_counts[i].tolist())\n",
    "    HLD_gen2[i] = Hellinger_dist(target_counts[i].tolist(), gen_counts_1429[i].tolist())\n",
    "\n",
    "print(\"Layer Size: \", layers.size)\n",
    "print(\"JSD Size: \", len(JSD_recon))\n",
    "plt.scatter(layers[:-1], np.asarray(JSD_recon[:-1]))\n",
    "plt.scatter(layers[:-1], np.asarray(JSD_gen1[:-1]))\n",
    "plt.scatter(layers[:-1], np.asarray(JSD_gen2[:-1]))\n",
    "\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel(\"Layer Number\")\n",
    "plt.ylabel(\"JSD Divergence Value\")\n",
    "plt.legend([\"Target vs Recon\", \"Target vs Sample 1372\", \"Target vs Sample 454\"])\n",
    "plt.title(\"JSD Divergence Comparison per Layer\")\n",
    "plt.grid(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93fb00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(layers[:-1], np.asarray(HLD_recon[:-1]))\n",
    "plt.scatter(layers[:-1], np.asarray(HLD_gen1[:-1]))\n",
    "plt.scatter(layers[:-1], np.asarray(HLD_gen2[:-1]))\n",
    "\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel(\"Layer Number\")\n",
    "plt.ylabel(\"Hellinger Distance\")\n",
    "plt.legend([\"Target vs Recon\", \"Target vs Sample 1372\", \"Target vs Sample 454\"])\n",
    "plt.title(\"Hellinger Distance Comparison per Layer\")\n",
    "plt.grid(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c16629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a Jensen-Shannon Divergence (JSD) a smoothed version of KL divergence to handle / 0\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "samps = np.linspace(0, len(entarget_samples_orig) - 1, 10)\n",
    "print(entarget_samples_orig, entarget_samples_orig.shape)\n",
    "\n",
    "layers = np.linspace(0, 44, 45)\n",
    "JSD_recon = np.zeros([45,45])\n",
    "JSD_gen = np.zeros([45,45])\n",
    "\n",
    "HLD_recon = np.zeros([45,45])\n",
    "HLD_gen = np.zeros([45,45])\n",
    "\n",
    "# Sort samples\n",
    "entarget_samplesnp = entarget_samples_orig.numpy()\n",
    "ind = np.argsort(entarget_samplesnp, axis = 0).flatten()\n",
    "sortentarget_samples = entarget_samples_orig[ind]\n",
    "sortxtarget_samples = xtarget_samples_orig[ind]\n",
    "sortxrecon_samples = xrecon_samples[ind]\n",
    "sortxgen_samples = xgen_samples[ind]\n",
    "inc_energies = np.linspace(0, len(entarget_samples_orig), 46).astype(int)\n",
    "\n",
    "xenergies = (((sortentarget_samples[samps] / 1).flatten()).numpy().astype(int)).tolist()\n",
    "\n",
    "for j in range(45):\n",
    "    \n",
    "    # Create lists for the histogram values\n",
    "    target_counts, recon_counts, gen_counts = [0] * 45, [0] * 45, [0] * 45\n",
    "    target_bins, recon_bins, gen_bins = [0] * 45, [0] * 45, [0] * 45\n",
    "\n",
    "    \n",
    "    sub_target_samples = sortxtarget_samples[inc_energies[j] : inc_energies[j+1]]\n",
    "    sub_recon_samples = sortxrecon_samples[inc_energies[j] : inc_energies[j+1]]\n",
    "    sub_gen_samples = sortxgen_samples[inc_energies[j] : inc_energies[j+1]]\n",
    "    for i in range(45): # of layers in the cylinder - 1\n",
    "#         print(sub_target_samples, sub_target_samples.shape)\n",
    "        idx = (i+1)*9*16\n",
    "        idxPrev = (i)*9*16\n",
    "        l = idx - idxPrev\n",
    "#         target_counts[i], target_bins[i] = np.histogram(((sub_target_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), bins=np.arange(minVal, maxVal + binwidth, binwidth))\n",
    "#         recon_counts[i], recon_bins[i] = np.histogram(((sub_recon_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), bins=np.arange(minVal, maxVal + binwidth, binwidth))\n",
    "#         gen_counts[i], gen_bins[i] = np.histogram(((sub_gen_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), bins=np.arange(minVal, maxVal + binwidth, binwidth))\n",
    "        \n",
    "        target_counts[i], target_bins[i] = np.histogram((sub_target_samples[:, idxPrev:idx]).sum(dim=1).numpy()/1000, bins=20)\n",
    "        recon_counts[i], recon_bins[i] = np.histogram((sub_recon_samples[:, idxPrev:idx]).sum(dim=1).numpy()/1000, bins=20)\n",
    "        gen_counts[i], gen_bins[i] = np.histogram((sub_gen_samples[:, idxPrev:idx]).sum(dim=1).numpy()/1000, bins=20)\n",
    "\n",
    "        \n",
    "        mid_recon = 0.5 * (target_counts[i] + recon_counts[i])\n",
    "        mid_gen = 0.5 * (target_counts[i] + gen_counts[i])\n",
    "\n",
    "    #     print(mid_recon, target_counts[0].tolist())\n",
    "\n",
    "    #     print(\"testing: \", KL_div(((xtarget_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), mid_recon))\n",
    "    #     print((0.5 * (KL_div(((xtarget_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), mid_recon) + KL_div(((xrecon_samples[:, idxPrev:idx] == 0).sum(dim=1)/l).numpy(), mid_recon))))\n",
    "\n",
    "        JSD_recon[j][i] = (0.5 * (KL_div(target_counts[i].tolist(), mid_recon) + KL_div(recon_counts[i].tolist(), mid_recon)))\n",
    "        JSD_gen[j][i] = (0.5 * (KL_div(target_counts[i].tolist(), mid_gen) + KL_div(gen_counts[i].tolist(), mid_gen)))\n",
    "\n",
    "        HLD_recon[j][i] = Hellinger_dist(target_counts[i].tolist(), recon_counts[i].tolist())\n",
    "        HLD_gen[j][i] = Hellinger_dist(target_counts[i].tolist(), gen_counts[i].tolist())\n",
    "\n",
    "    if j == 43:\n",
    "        print(target_counts, len(target_counts))\n",
    "        \n",
    "# print(JSD_recon)\n",
    "# print(min(JSD_recon.flatten()))\n",
    "print(JSD_recon, JSD_recon.shape)\n",
    "\n",
    "# Plotting for JSD_recon and JSD_gen\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot for JSD_recon\n",
    "cax1 = ax1.imshow(JSD_recon, interpolation='nearest', cmap='viridis')\n",
    "ax1.set_aspect('equal')\n",
    "ax1.set_title('Target vs Recon')\n",
    "ax1.set_yticks(np.linspace(0, 43, 10))\n",
    "ax1.set_yticklabels([int(x) for x in xenergies])\n",
    "ax1.set_ylabel(\"Incident Energy Ranges\")\n",
    "ax1.set_xlabel(\"Layer Number\")\n",
    "divider1 = make_axes_locatable(ax1)\n",
    "cax_cbar1 = divider1.append_axes(\"right\", size=\"5%\", pad=0.25)\n",
    "cbar1 = fig.colorbar(cax1, cax=cax_cbar1)\n",
    "cbar1.set_label('JSD Divergence Value')\n",
    "\n",
    "# Plot for JSD_gen\n",
    "cax2 = ax2.imshow(JSD_gen, interpolation='nearest', cmap='viridis')\n",
    "ax2.set_aspect('equal')\n",
    "ax2.set_title('Target vs Sample')\n",
    "ax2.set_yticks(np.linspace(0, 43, 10))\n",
    "ax2.set_yticklabels([int(x) for x in xenergies])\n",
    "ax2.set_ylabel(\"Incident Energy Ranges\")\n",
    "ax2.set_xlabel(\"Layer Number\")\n",
    "divider2 = make_axes_locatable(ax2)\n",
    "cax_cbar2 = divider2.append_axes(\"right\", size=\"5%\", pad=0.25)\n",
    "cbar2 = fig.colorbar(cax2, cax=cax_cbar2)\n",
    "cbar2.set_label('JSD Divergence Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87318650-8d46-47a9-903d-b14de7f6fcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(entarget_samples, entarget_samples.shape)\n",
    "print(xtarget_samples, xtarget_samples.shape)\n",
    "print(xgen_samples_1429, xgen_samples_1429.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7acb87-b063-4e93-8eb4-bf2bd15fcb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "samps = np.linspace(0, len(entarget_samples) - 1, 10)\n",
    "\n",
    "layers = np.linspace(0, 44, 45)\n",
    "# JSD_recon = np.zeros([44,44])\n",
    "JSD_gen = np.zeros([45,45])\n",
    "\n",
    "# HLD_recon = np.zeros([44,44])\n",
    "HLD_gen = np.zeros([45,45])\n",
    "\n",
    "# Sort samples\n",
    "entarget_samplesnp = entarget_samples.numpy()\n",
    "ind = np.argsort(entarget_samplesnp, axis=0).flatten()\n",
    "sortentarget_samples = entarget_samples[ind]\n",
    "sortxtarget_samples = xtarget_samples[ind]\n",
    "# sortxrecon_samples = xrecon_samples[ind]\n",
    "sortxgen_samples = xgen_samples_1429[ind]\n",
    "inc_energies = np.linspace(0, len(entarget_samples), 46).astype(int)\n",
    "\n",
    "print(sortentarget_samples, sortentarget_samples.shape)\n",
    "print(sortxtarget_samples, sortxtarget_samples.shape)\n",
    "print(sortxgen_samples, sortxgen_samples.shape)\n",
    "\n",
    "xenergies = (((sortentarget_samples[samps] / 1).flatten()).numpy().astype(int)).tolist()\n",
    "\n",
    "minVal, maxVal = 0, 1\n",
    "binwidth = (maxVal-minVal)/20\n",
    "\n",
    "for j in range(45):\n",
    "    \n",
    "    # Create lists for the histogram values\n",
    "    target_counts, gen_counts = [0] * 45, [0] * 45\n",
    "    target_bins, gen_bins = [0] * 45, [0] * 45\n",
    "\n",
    "    sub_target_samples = sortxtarget_samples[inc_energies[j] : inc_energies[j+1]]\n",
    "    # sub_recon_samples = sortxrecon_samples[inc_energies[j] : inc_energies[j+1]]\n",
    "    sub_gen_samples = sortxgen_samples[inc_energies[j] : inc_energies[j+1]]\n",
    "\n",
    "    for i in range(45): # of layers in the cylinder - 1\n",
    "\n",
    "        idx = (i+1)*9*16\n",
    "        idxPrev = (i)*9*16\n",
    "        l = idx - idxPrev\n",
    "        \n",
    "        target_counts[i], target_bins[i] = np.histogram((sub_target_samples[:, idxPrev:idx]).sum(dim=1).numpy()/1000, bins=20)\n",
    "        # recon_counts[i], recon_bins[i] = np.histogram((sub_recon_samples[:, idxPrev:idx]).sum(dim=1).numpy()/1000, bins=20)\n",
    "        gen_counts[i], gen_bins[i] = np.histogram((sub_gen_samples[:, idxPrev:idx]).sum(dim=1).numpy()/1000, bins=20)\n",
    "        # mid_recon = 0.5 * (target_counts[i] + recon_counts[i])\n",
    "        mid_gen = 0.5 * (target_counts[i] + gen_counts[i])\n",
    "\n",
    "        # JSD_recon[j][i] = (0.5 * (KL_div(target_counts[i].tolist(), mid_recon) + KL_div(recon_counts[i].tolist(), mid_recon)))\n",
    "        JSD_gen[j][i] = (0.5 * (KL_div(target_counts[i].tolist(), mid_gen) + KL_div(gen_counts[i].tolist(), mid_gen)))\n",
    "\n",
    "        # HLD_recon[j][i] = Hellinger_dist(target_counts[i].tolist(), recon_counts[i].tolist())\n",
    "        HLD_gen[j][i] = Hellinger_dist(target_counts[i].tolist(), gen_counts[i].tolist())\n",
    "\n",
    "print(JSD_recon)\n",
    "print(min(JSD_recon.flatten()))\n",
    "print(JSD_recon, JSD_recon.shape)\n",
    "\n",
    "# Plotting for JSD_gen\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "cax = ax.imshow(JSD_gen, interpolation='nearest', cmap='rainbow')\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title(\"Ground Truth vs Model Prediction (Energy per Event)\")\n",
    "ax.set_yticks(np.linspace(0, 44, 10))\n",
    "ax.set_yticklabels([int(x) for x in xenergies])\n",
    "ax.set_ylabel(\"Incident Energy Ranges (MeV)\")\n",
    "ax.set_xlabel(\"Layer Number\")\n",
    "\n",
    "divider = make_axes_locatable(ax)\n",
    "cax_cbar = divider.append_axes(\"right\", size=\"5%\", pad=0.25)\n",
    "cbar = fig.colorbar(cax, cax=cax_cbar)\n",
    "cbar.set_label('JSD Divergence Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4203f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for HLD_recon and HLD_gen\n",
    "fig, ax2 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot for HLD_recon\n",
    "# cax1 = ax1.imshow(HLD_recon, interpolation='nearest', cmap='viridis')\n",
    "# ax1.set_aspect('equal')\n",
    "# ax1.set_yticks(np.linspace(0, 43, 10))\n",
    "# ax1.set_yticklabels([int(x) for x in xenergies])\n",
    "# ax1.set_ylabel(\"Incident Energy Ranges\")\n",
    "# ax1.set_xlabel(\"Layer Number\")\n",
    "# divider1 = make_axes_locatable(ax1)\n",
    "# cax_cbar1 = divider1.append_axes(\"right\", size=\"5%\", pad=0.25)\n",
    "# cbar1 = fig.colorbar(cax1, cax=cax_cbar1)\n",
    "# cbar1.set_label('Hellinger Distance Value')\n",
    "\n",
    "# Plot for HLD_gen\n",
    "cax2 = ax2.imshow(HLD_gen, interpolation='nearest', cmap='viridis')\n",
    "ax2.set_aspect('equal')\n",
    "ax2.set_yticks(np.linspace(0, 43, 10))\n",
    "ax2.set_yticklabels([int(x) for x in xenergies])\n",
    "ax2.set_ylabel(\"Incident Energy Ranges\")\n",
    "ax2.set_xlabel(\"Layer Number\")\n",
    "divider2 = make_axes_locatable(ax2)\n",
    "cax_cbar2 = divider2.append_axes(\"right\", size=\"5%\", pad=0.25)\n",
    "cbar2 = fig.colorbar(cax2, cax=cax_cbar2)\n",
    "cbar2.set_label('Hellinger Distance Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df822de-49f9-4aa9-bdcb-e99c16c8c4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "HLF_1_electron.relevantLayers = [0,5,10,15,20,25,30,35,40,45]\n",
    "plt.figure(figsize=(8,6))\n",
    "# Create a grid of subplots\n",
    "fig, axes = plt.subplots(3,3, figsize=(12, 12), sharey=False, sharex=False, tight_layout=True)\n",
    "fig.text(0.5, 0.0, 'Energy per event (GeV)', ha='center', fontsize=15)\n",
    "fig.text(0.0, 0.5, 'Histogram', va='center', rotation='vertical', fontsize=15)\n",
    "\n",
    "# Iterate through the columns of X and plot histograms\n",
    "for i,_ in enumerate(HLF_1_electron.relevantLayers[:-1]):\n",
    "    row_index = i // 3  # Determine the row index\n",
    "    col_index = i % 3   # Determine the column index\n",
    "    \n",
    "    ax = axes[row_index, col_index]  # Get the current subplot\n",
    "    \n",
    "    # Plot histogram for the current column\n",
    "    idx = HLF_1_electron.relevantLayers[i+1]*9*16\n",
    "    idxPrev = (HLF_1_electron.relevantLayers[i])*9*16\n",
    "    l = idx - idxPrev\n",
    "    ax.scatter(xtarget_samples_orig[:, idxPrev:idx].sum(dim=1).numpy()/1000, xrecon_samples[:, idxPrev:idx].sum(dim=1).numpy()/1000, marker='.', alpha=.5, color=\"b\")\n",
    "    ax.scatter(xtarget_samples_orig[:, idxPrev:idx].sum(dim=1).numpy()/1000, xgen_samples[:, idxPrev:idx].sum(dim=1).numpy()/1000, marker='.', alpha=.5, color=\"orange\")    \n",
    "    minVal, maxVal = np.min(xtarget_samples[:, idxPrev:idx].sum(dim=1).numpy()/1000), np.max(xtarget_samples[:, idxPrev:idx].sum(dim=1).numpy()/1000) \n",
    "    ax.plot([minVal, maxVal],[minVal, maxVal], c='red', lw=2.5, label='y=x')\n",
    "    if i == 0:\n",
    "        # ax.legend([\"GT\", \"Recon\", \"Sample\", \"Sample w/ QPU\"], title=f'{ds[part]}')\n",
    "        ax.legend([\"GT\", \"Recon\", \"Sample\", \"Sample w/ QPU\"], fontsize=15)\n",
    "    ax.grid(\"True\")\n",
    "    \n",
    "    ax.set_title(f'Layers {HLF_1_electron.relevantLayers[i]} to {HLF_1_electron.relevantLayers[i+1]-1}')\n",
    "\n",
    "# Adjust layout and display the plots\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f'/home/javier/Projects/CaloQVAE/figs/{modelname}/energy_per_layer_{modelname}_{arch}_{datascaled}_{part}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c38ed89-2860-422d-9734-6b18404dcf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(xtarget_samples_orig.sum(dim=1).numpy()/1000, xrecon_samples.sum(dim=1).numpy()/1000, marker='.', alpha=.5, color='b')\n",
    "plt.scatter(xtarget_samples_orig.sum(dim=1).numpy()/1000, xgen_samples.sum(dim=1).numpy()/1000, marker='.', alpha=.5, color='orange')\n",
    "# plt.scatter(xtarget_samples.sum(dim=1).numpy()/1000, xgen_samples_qpu.sum(dim=1).numpy()/1000, marker=., alpha=.5, color=cyan)\n",
    "plt.plot([0,800],[0,800], c='red', lw=2.5, label='y=x')\n",
    "# plt.plot([0,20],[0,20], c=red, lw=2)\n",
    "# axes[0,1].set_xlabel(GT energy per event (GeV))\n",
    "plt.legend(['Recon', 'MC', 'QPU', 'y=x'], fontsize=17)\n",
    "plt.grid('True')\n",
    "plt.xlabel('GT energy per event (GeV)', fontsize=15)\n",
    "plt.ylabel('Model output energy per \\n event (GeV)', fontsize=15)\n",
    "# plt.savefig(f/home/javier/Projects/CaloQVAE/figs/{modelname}/energy_scatter_{modelname}_{arch}_{datascaled}_{part}.png)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff79f634",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist((xtarget_samples[idxEnFilter,:].sum(dim=1)/entarget_samples[idxEnFilter].view(-1)).numpy(), bins=30, log=True, histtype='stepfilled', linewidth=2.5, color=\"b\", alpha=0.7, density=True)\n",
    "plt.hist((xrecon_samples[idxEnFilter,:].sum(dim=1).numpy()/entarget_samples[idxEnFilter].view(-1)).numpy(), bins=30, log=True, histtype='step', linewidth=2.5, color=\"c\", density=True)\n",
    "plt.hist((xgen_samples[idxEnFilter,:].sum(dim=1).numpy()/entarget_samples[idxEnFilter].view(-1)).numpy(), bins=30, log=True, histtype='step', linewidth=2.5, color=\"orange\", linestyle=\"dashdot\", density=True)\n",
    "\n",
    "plt.xlabel(\"Energy ratio\", fontsize=15)\n",
    "# plt.xscale('log')\n",
    "\n",
    "plt.ylabel(\"Histogram\", fontsize=15)\n",
    "plt.legend([\"GT\", \"Recon\", \"Sample\", \"Sample w/ QPU\"])\n",
    "# plt.savefig(f'/home/javier/Projects/CaloQVAE/figs/{modelname}/energy_ration_{modelname}_{arch}_{datascaled}_{part}.png')\n",
    "plt.show()\n",
    "\n",
    "# Energy across all voxels throughout the cylinder / incidence energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17efa2cd-0481-4325-a1c0-88c9688216c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xtarget_samples.shape, xgen_samples.shape, xgen_samples_1429.shape\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a669858-e0a8-4b24-8097-29b2633edc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jetnet\n",
    "# Refer to Documentation: https://jetnet.readthedocs.io/en/latest/pages/metrics.html#jetnet.evaluation.fpd\n",
    "# Metrics will be implemented once dataset # and sample # matches\n",
    "# jetnet.evaluation.fpd(real_features: Tensor | np.ndarray, gen_features: Tensor | np.ndarray, min_samples: int = 20000, max_samples: int = 50000, num_batches: int = 20, num_points: int = 10, normalise: bool = True, seed: int = 42) tuple[float, float]\n",
    "# jetnet.evaluation.kpd(real_features: Tensor | np.ndarray, gen_features: Tensor | np.ndarray, num_batches: int = 10, batch_size: int = 5000, normalise: bool = True, seed: int = 42, num_threads: int | None = None) tuple[float, float]\n",
    "fpd_met = jetnet.evaluation.fpd(xtarget_samples, xgen_samples_1429)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f9e67-8faf-42aa-8cc8-6ff03d997315",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FPD Metric Value: \", fpd_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774714d1-8c74-4e72-a545-a5eadbb73403",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpd_met = jetnet.evaluation.kpd(xtarget_samples, xgen_samples_1429)\n",
    "print(\"KPD Metric Value: \", kpd_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b40c2a-2407-4930-b749-9bf1fe3cfb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist((xtarget_samples_orig.sum(dim=1)/entarget_samples_orig[idxEnFilter].view(-1)).numpy(), bins=30, log=True, histtype='stepfilled', linewidth=2.5, color=\"b\", alpha=0.7, density=True)\n",
    "# plt.hist((xrecon_samples.sum(dim=1).numpy()/entarget_samples_orig[idxEnFilter].view(-1)).numpy(), bins=30, log=True, histtype='step', linewidth=2.5, color=\"c\", density=True)\n",
    "# plt.hist((xgen_samples.sum(dim=1).numpy()/entarget_samples_orig[idxEnFilter].view(-1)).numpy(), bins=30, log=True, histtype='step', linewidth=2.5, color=\"orange\", linestyle=\"dashdot\", density=True)\n",
    "\n",
    "plt.hist((xtarget_samples.sum(dim=1)/entarget_samples.view(-1)).numpy(), bins=30, log=True, density=True, histtype='stepfilled', linewidth=2.5, color=\"b\", alpha=0.7)\n",
    "plt.hist((xrecon_samples.sum(dim=1).numpy()/entarget_samples_orig.view(-1)).numpy(), bins=30, log=True, density=True, histtype='step', linewidth=2.5, color=\"c\")\n",
    "plt.hist((xgen_samples.sum(dim=1).numpy()/entarget_samples_orig.view(-1)).numpy(), bins=30, log=True, density=True, histtype='step', linewidth=2.5, color=\"orange\", linestyle=\"dashdot\")\n",
    "# plt.hist((xgen_samples_1429.sum(dim=1).numpy()/entarget_samples.view(-1)).numpy(), bins=30, log=True, density=True, histtype='step', linewidth=2.5, color=\"purple\", linestyle=\"dashdot\")\n",
    "plt.hist((xgen_samples_qpu.sum(dim=1).numpy()/entarget_samples.view(-1)).numpy(), bins=30, log=True, density=True, histtype='step', linewidth=2.5, color=\"purple\", linestyle=\"dashdot\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Energy ratio\", fontsize=15)\n",
    "# plt.xscale('log')\n",
    "\n",
    "plt.ylabel(\"Histogram\", fontsize=15)\n",
    "plt.legend([\"GT\", \"Recon\", \"Samples\", \"Samples w/ QPU\"])\n",
    "# plt.savefig(f'/home/javier/Projects/CaloQVAE/figs/{modelname}/energy_ration_{modelname}_{arch}_{datascaled}_{part}.png')\n",
    "plt.show()\n",
    "\n",
    "# Energy across all voxels throughout the cylinder / incidence energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96f9e0f-f36f-49ba-a093-387c4ca4c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(entarget_samples_orig, entarget_samples_orig.shape, entarget_samples_orig.view(-1), entarget_samples_orig.view(-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901403c0-b2ad-4909-9b1f-fc396fce5040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "x, y = xtarget_samples_orig.sum(dim=1), entarget_samples_orig.view(-1)\n",
    "slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "print(f\"Slope: {slope}\")\n",
    "print(f\"Intercept: {intercept}\")\n",
    "print(f\"R-squared: {r_value**2}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "print(f\"Standard error: {std_err}\")\n",
    "plt.scatter(x, y)\n",
    "plt.scatter(x, y, color='blue', label='Data points')\n",
    "plt.plot(x, slope*x + intercept, color='red', label='Fitted line')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "comb = xtarget_samples_orig.sum(dim=1)/entarget_samples_orig.view(-1)\n",
    "print(entarget_samples_orig[comb > 1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde8192a-a811-4022-8fa1-63656d0be1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((xtarget_samples.sum(dim=1)/entarget_samples[idxEnFilter].view(-1)).numpy(), bins=30, log=True, histtype='stepfilled', linewidth=2.5, color=\"b\", alpha=0.7, density=True)\n",
    "plt.hist((xrecon_samples.sum(dim=1).numpy()/entarget_samples[idxEnFilter].view(-1)).numpy(), bins=30, log=True, histtype='step', linewidth=2.5, color=\"c\", density=True)\n",
    "plt.hist((xgen_samples.sum(dim=1).numpy()/entarget_samples[idxEnFilter].view(-1)).numpy(), bins=30, log=True, histtype='step', linewidth=2.5, color=\"orange\", linestyle=\"dashdot\", density=True)\n",
    "\n",
    "plt.xlabel(\"Energy ratio\", fontsize=15)\n",
    "# plt.xscale('log')\n",
    "\n",
    "plt.ylabel(\"Histogram\", fontsize=15)\n",
    "plt.legend([\"GT\", \"Recon\", \"Sample\", \"Sample w/ QPU\"])\n",
    "# plt.savefig(f'/home/javier/Projects/CaloQVAE/figs/{modelname}/energy_ration_{modelname}_{arch}_{datascaled}_{part}.png')\n",
    "plt.show()\n",
    "\n",
    "# Energy across all voxels throughout the cylinder / incidence energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d817776",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(xtarget_samples[idxEnFilter,:].sum(dim=1).numpy()/1000, bins=50, log=True, histtype='stepfilled', linewidth=2.5, color=\"b\", alpha=0.7, density=True)\n",
    "plt.hist(xrecon_samples[idxEnFilter,:].sum(dim=1).numpy()/1000, bins=50, log=True, histtype='step', linewidth=2.5, color=\"c\", density=True)\n",
    "plt.hist(xgen_samples[idxEnFilter,:].sum(dim=1).numpy()/1000, bins=50, log=True, histtype='step', linewidth=2.5, color=\"orange\", linestyle=\"dashdot\", density=True)\n",
    "# plt.hist(xgen_samples_qpu[idxEnFilter,:].sum(dim=1).numpy()/1000, bins=30, log=True, histtype='step', linewidth=2.5, color=\"m\", linestyle=\"dashed\", density=True)\n",
    "\n",
    "plt.xlabel(\"energy per event (GeV)\", fontsize=15)\n",
    "# plt.xscale('log')\n",
    "\n",
    "plt.ylabel(\"Histogram\", fontsize=15)\n",
    "plt.legend([\"GT\", \"Recon\", \"Sample\", \"Sample w/ QPU\"])\n",
    "# plt.title(f'{ds[part]} \\n {E_left/1000}<E_inc<{E_right/1000} (GeV)')\n",
    "# plt.savefig(f'/home/javier/Projects/CaloQVAE/figs/{modelname}/energy_slice_{modelname}_{arch}_{datascaled}_{part}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc66320",
   "metadata": {},
   "outputs": [],
   "source": [
    "HLF_1_electron.relevantLayers = [0,5,10,15,20,25,30,35,40,45]\n",
    "plt.figure(figsize=(8,6))\n",
    "# Create a grid of subplots\n",
    "fig, axes = plt.subplots(3,3, figsize=(12, 12), sharey=True, sharex=False, tight_layout=True)\n",
    "fig.text(0.5, 0.0, 'Energy per event (GeV)', ha='center', fontsize=15)\n",
    "fig.text(0.0, 0.5, 'Histogram', va='center', rotation='vertical', fontsize=15)\n",
    "\n",
    "# Iterate through the columns of X and plot histograms\n",
    "for i,_ in enumerate(HLF_1_electron.relevantLayers[:-1]):\n",
    "    row_index = i // 3  # Determine the row index\n",
    "    col_index = i % 3   # Determine the column index\n",
    "    \n",
    "    ax = axes[row_index, col_index]  # Get the current subplot\n",
    "    \n",
    "    # Plot histogram for the current column\n",
    "    idx = HLF_1_electron.relevantLayers[i+1]*9*16\n",
    "    idxPrev = (HLF_1_electron.relevantLayers[i])*9*16\n",
    "    l = idx - idxPrev\n",
    "    ax.hist(xtarget_samples[idxEnFilter, idxPrev:idx].sum(dim=1).numpy()/1000,  bins=10, log=True, histtype='stepfilled', linewidth=2.5, color=\"b\", alpha=0.7)\n",
    "    ax.hist(xrecon_samples[idxEnFilter, idxPrev:idx].sum(dim=1).numpy()/1000, bins=10, log=True, histtype='step', linewidth=2.5, color=\"c\")\n",
    "    # ax.hist(xrecon_samples_2[:, idxPrev:idx].sum(dim=1).numpy()/1000, bins=20, log=True, histtype='step', linewidth=2.5, color=\"m\", linestyle=\"dashdot\")\n",
    "    ax.hist(xgen_samples[idxEnFilter, idxPrev:idx].sum(dim=1).numpy()/1000, bins=10, log=True, histtype='step', linewidth=2.5, color=\"orange\", linestyle=\"dashdot\")\n",
    "    # ax.hist(xgen_samples_qpu[:, idxPrev:idx].sum(dim=1).numpy()/1000, bins=20, log=True, histtype='step', linewidth=2.5, color=\"m\", linestyle=\"dashed\")\n",
    "    if i == 0:\n",
    "        # ax.legend([\"GT\", \"Recon\", \"Sample\", \"Sample w/ QPU\"], title=f'{ds[part]}')\n",
    "        ax.legend([\"GT\", \"Recon\", \"Sample\", \"Sample w/ QPU\"], fontsize=15)\n",
    "    ax.grid(\"True\")\n",
    "    \n",
    "    # Set labels and title for the subplot\n",
    "    # ax.set_xlabel(f'Column {i + 1}')\n",
    "    # ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'Layers {HLF_1_electron.relevantLayers[i]} to {HLF_1_electron.relevantLayers[i+1]-1}')\n",
    "\n",
    "# Adjust layout and display the plots\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f'/home/javier/Projects/CaloQVAE/figs/{modelname}/energy_per_layer_{modelname}_{arch}_{datascaled}_{part}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133f64b9-747e-47d9-a2dc-44c8ba988a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRANULARITY METRICS\n",
    "def measure_single_granularity(data_tensor_1, index):\n",
    "    \"\"\"\n",
    "    Measure the granularity of a calorimeter image given as a 1D array using PyTorch.\n",
    "    \n",
    "    Parameters:\n",
    "    data (array-like): 1D array representing the calorimeter image.\n",
    "    \n",
    "    Returns:\n",
    "    float: A measure of the granularity.\n",
    "    \"\"\"\n",
    "    # Calculate the differences between consecutive elements\n",
    "#     diffs_1 = (data_tensor_1[:,index:] - data_tensor_1[:,:-index])/torch.mean(data_tensor_1[:,:-index], dim = 1, keepdim=True)\n",
    "    diffs_1 = (data_tensor_1[:,index:] - data_tensor_1[:,:-index])\n",
    "#     std_g = torch.std(diffs_1, dim = 1)\n",
    "    return diffs_1\n",
    "\n",
    "\n",
    "def measure_stochastic_granularity(data_tensor):\n",
    "    segment_length = 144  # Length of each segment\n",
    "    num_segments = 45     # Number of segments per sample\n",
    "    num_samples = data_tensor.size(0)  # Number of samples in the data tensor\n",
    "\n",
    "    # Use PyTorch to generate a random integer array of shape (num_samples,) with values between 0 and 15\n",
    "    random_array = torch.randint(0, 16, (num_samples,), dtype=torch.int64, device=data_tensor.device)\n",
    "\n",
    "    # Multiply random_array by 9 and expand it to shape (num_samples, num_segments)\n",
    "    shifts = (random_array * 9).unsqueeze(-1).expand(-1, num_segments)\n",
    "\n",
    "    # Unfold the data tensor to create segments of length 144\n",
    "    # The result is a tensor of shape (num_samples, num_segments, segment_length)\n",
    "    segments = data_tensor.unfold(1, segment_length, segment_length)\n",
    "\n",
    "    # Create an indices tensor of shape (num_samples, num_segments, segment_length)\n",
    "    indices = torch.arange(segment_length, device=data_tensor.device).repeat(num_samples, num_segments, 1)\n",
    "    # Adjust the indices by adding the shifts and applying modulo operation to wrap around\n",
    "    indices = (indices + shifts.unsqueeze(-1)) % segment_length  # Ensure correct broadcasting\n",
    "\n",
    "    # Gather elements from the segments tensor using the adjusted indices\n",
    "    rotated_segments = torch.gather(segments, 2, indices)\n",
    "\n",
    "    # Reshape the rotated_segments tensor back to the original shape of data_tensor\n",
    "    result_tensor = rotated_segments.view(num_samples, -1)\n",
    "\n",
    "    # Compute the difference between the original data tensor and the result tensor\n",
    "    diffs = data_tensor - result_tensor\n",
    "\n",
    "    return diffs                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae51b22-4d3f-49b2-9975-3dd4e3fca12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data_gran = h5py.File(\"/fast_scratch_1/caloqvae/syn_data/dataset2_synthetic_denim-smoke-166en130.hdf5\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce28732-89ad-4d33-a5f3-82ecfd255a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gran_gen_samples = torch.tensor(gen_data_gran['showers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7947c3-e009-4684-955c-045bdccaf6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xtarget_samples_orig.shape)\n",
    "print(gran_gen_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaf7a7b-b182-4766-a781-1f0a3ec58152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# granularity_target = measure_single_granularity(xtarget_samples,9)\n",
    "# # granularity_sample = measure_single_granularity(xgen_samples,9)\n",
    "# granularity_recon = measure_single_granularity(xrecon_samples,9)\n",
    "\n",
    "granularity_target = measure_stochastic_granularity(xtarget_samples)\n",
    "granularity_recon = measure_stochastic_granularity(xrecon_samples)\n",
    "granularity_sample = measure_stochastic_granularity(xgen_samples)\n",
    "std_target = torch.std(granularity_target, dim =1)\n",
    "std_recon = torch.std(granularity_recon, dim =1)\n",
    "std_sample = torch.std(granularity_sample, dim =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bd3b25-5282-444e-a64d-65c46e9538f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = np.concatenate((std_target, std_recon))\n",
    "bins = np.histogram_bin_edges(combined_data, bins=50)\n",
    "plt.hist(std_target, log=True, color='red', bins=bins, linestyle='--', label='std_target', density=True, histtype='step')\n",
    "plt.hist(std_recon, log=True, color='blue', bins=bins, linestyle='--', label='std_recon', density=True, histtype='step')\n",
    "plt.hist(std_sample, log=True, color='purple', bins=bins, linestyle='--', label='std_sample', density=True, histtype='step')\n",
    "\n",
    "plt.xlabel('Standard Deviation')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.title(\"Granularity STD Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55056815-20ad-477d-91cd-0861b0d2646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = []\n",
    "error_target_list = []\n",
    "error_recon_list = []\n",
    "error_alt_list = []\n",
    "ratio_list = []\n",
    "ratio_list_alt = []\n",
    "for i in range(1, 30):\n",
    "    error_recon = torch.mean(torch.std(measure_single_granularity(xrecon_samples, i),dim=0))\n",
    "    error_target = torch.mean(torch.std(measure_single_granularity(xtarget_samples, i),dim=0))\n",
    "    error_alt = torch.mean(torch.std(measure_single_granularity(xgen_samples, i),dim=0))\n",
    "    ratio = error_target/error_recon\n",
    "    ratio_alt = error_target/error_alt\n",
    "    error_recon_list.append(error_recon)\n",
    "    error_target_list.append(error_target)\n",
    "    error_alt_list.append(error_alt)\n",
    "    ratio_list.append(ratio)\n",
    "    ratio_list_alt.append(ratio_alt)\n",
    "    index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d263c81-3234-4e66-9642-f95ffabf8f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10, 8), gridspec_kw={'height_ratios': [2, 1]})\n",
    "\n",
    "#  error_gen_list  error_target_list\n",
    "ax1.plot(index, error_target_list, label='Target', color='black')\n",
    "ax1.plot(index, error_recon_list, label='Recon', color='red')\n",
    "ax1.plot(index, error_alt_list, label='Samples', color='green')\n",
    "ax1.legend()\n",
    "ax1.set_ylabel('Mean STD')\n",
    "\n",
    "#  ratio_list\n",
    "ax2.plot(index, ratio_list, label='Ratio T/G', color='red')\n",
    "ax2.plot(index, ratio_list_alt, label='Ratio T/A', color='green')\n",
    "ax2.legend()\n",
    "ax2.set_xlabel('Shifting Index')\n",
    "ax2.set_ylabel('Ratio')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddceb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "(((xtarget_samples - xrecon_samples)/(xtarget_samples+1e-5))**2).sum(dim=1).argsort()[-200:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd87e628",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xtarget_samples_orig, xtarget_samples_orig.shape)\n",
    "print(xgen_samples, xgen_samples.shape)\n",
    "print(entarget_samples_orig, entarget_samples_orig.shape)\n",
    "# print(entarget_samples[entarget_samples[9500 < ]])\n",
    "\n",
    "spec_energy = 1000000 #MeV\n",
    "diff_energy = abs(entarget_samples_orig - spec_energy)\n",
    "print(entarget_samples_orig[torch.nonzero(diff_energy == min(diff_energy))[0][0]])\n",
    "idx = torch.nonzero(diff_energy == min(diff_energy))[0][0].item()\n",
    "print(idx)\n",
    "print((xtarget_samples[idx]).sum(dim=0))\n",
    "print(xgen_samples[1000], xgen_samples[1000].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1609073e-5f25-4905-9e35-0a93d3d079ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D PLOTTING\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import Normalize, LogNorm\n",
    "\n",
    "# Generate the array of 6480 values (example data)\n",
    "data = (xtarget_samples_orig[idx]).numpy()\n",
    "print(\"Data sum:\", data.sum())\n",
    "print(\"Data shape:\", data.shape)\n",
    "\n",
    "# Reshape the data to 45 layers x 16 slices x 9 circles\n",
    "count = 0\n",
    "data_reshape = np.zeros((45, 16, 9))\n",
    "print(\"Reshaped data shape:\", data_reshape.shape)\n",
    "for i in range(data_reshape.shape[0]):\n",
    "    for j in range(data_reshape.shape[1]):\n",
    "        for k in range(data_reshape.shape[2]):\n",
    "            data_reshape[i][j][k] = data[count]\n",
    "            count += 1\n",
    "\n",
    "# Check min and max values\n",
    "data_min = data_reshape.min()\n",
    "data_max = data_reshape.max()\n",
    "print(\"Data min:\", data_min, \"Data max:\", data_max)\n",
    "\n",
    "# Normalize the data using logarithmic normalization\n",
    "norm = LogNorm(vmin=1e+0, vmax=1e+4)  # Adding a small value to avoid log(0)\n",
    "\n",
    "print(\"Normalized data shape:\", data_reshape.shape)\n",
    "\n",
    "# Create the cylindrical coordinates\n",
    "theta = np.linspace(0, 2 * np.pi, 17)\n",
    "r = np.linspace(0, 1, 10)\n",
    "theta, r = np.meshgrid(theta, r)\n",
    "\n",
    "# Create the X, Y, Z coordinates for the cylinder rotated on its side\n",
    "Y = r * np.cos(theta) * 40\n",
    "Z = r * np.sin(theta) * 40\n",
    "\n",
    "# Create a figure and 3D axis\n",
    "fig = plt.figure(figsize=(30, 20))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot each layer separately\n",
    "\n",
    "layer_start = 40\n",
    "layer_end = 45\n",
    "for i in range(layer_start, layer_end):\n",
    "    X = np.full_like(Y, i)  # Each layer is at a constant X value\n",
    "    values = data_reshape[i, :, :].T  # Transpose to match the shape\n",
    "    values += 1e-16 # prevent log 0\n",
    "    \n",
    "    # Normalize and apply colormap\n",
    "    norm_values = norm(values)+1e-16\n",
    "    print(f\"Layer {i}, normalized values min: {norm_values.min()}, max: {norm_values.max()}\")\n",
    "    facecolors = plt.cm.rainbow(norm_values)  # Apply colormap to normalized values\n",
    "    \n",
    "    facecolors[..., -1] = 1 # Set the alpha channel to 0.3 for transparency\n",
    "    \n",
    "    surf = ax.plot_surface(X, Y, Z, facecolors=facecolors, rstride=1, cstride=1, shade=False, edgecolor='k')\n",
    "    # surf = ax.plot_surface(X, Y, Z, facecolors=facecolors+1e-16, rstride=1, cstride=1, shade=False)\n",
    "\n",
    "# Add a color bar\n",
    "mappable = plt.cm.ScalarMappable(cmap=plt.cm.rainbow, norm=norm)\n",
    "mappable.set_array(data)\n",
    "cbar = fig.colorbar(mappable, ax=ax, shrink=0.5, aspect=10, pad=-0.05)\n",
    "cbar.set_label('Energy (MeV)')\n",
    "\n",
    "# Add labels\n",
    "ax.set_xlabel('Layers')\n",
    "ax.set_ylabel('x (mm)')\n",
    "ax.set_zlabel('y (mm)', rotation=90)\n",
    "ax.set_xlim(44, 0)\n",
    "# ax.set_ylim(-40, 40)\n",
    "# ax.set_zlim(-40, 40)\n",
    "\n",
    "ax.set_box_aspect([44, 44, 36])\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "# Set the view angle\n",
    "ax.view_init(elev=0, azim=70)  # Adjust these values to change the view\n",
    "ax.grid(False)\n",
    "ax.axis('off')\n",
    "\n",
    "\n",
    "plt.title('EM Shower in Calorimeter')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaabbf4-341f-4419-b4fa-43f9d07d5918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import Normalize, LogNorm\n",
    "\n",
    "# Generate the array of 6480 values (example data)\n",
    "data = (xtarget_samples_orig[idx]).numpy()\n",
    "print(\"Data sum:\", data.sum())\n",
    "print(\"Data shape:\", data.shape)\n",
    "\n",
    "# Reshape the data to 45 layers x 16 slices x 9 circles\n",
    "count = 0\n",
    "data_reshape = np.zeros((45, 16, 9))\n",
    "print(\"Reshaped data shape:\", data_reshape.shape)\n",
    "for i in range(data_reshape.shape[0]):\n",
    "    for j in range(data_reshape.shape[1]):\n",
    "        for k in range(data_reshape.shape[2]):\n",
    "            data_reshape[i][j][k] = data[count]\n",
    "            count += 1\n",
    "\n",
    "# Check min and max values\n",
    "data_min = data_reshape.min()\n",
    "data_max = data_reshape.max()\n",
    "print(\"Data min:\", data_min, \"Data max:\", data_max)\n",
    "\n",
    "# Normalize the data using logarithmic normalization\n",
    "norm = LogNorm(vmin=1e+0, vmax=1e+4)  # Adding a small value to avoid log(0)\n",
    "\n",
    "print(\"Normalized data shape:\", data_reshape.shape)\n",
    "\n",
    "# Create the rectangular coordinates\n",
    "x = np.linspace(-1, 1, 17)\n",
    "y = np.linspace(-1, 1, 10)\n",
    "x, y = np.meshgrid(x, y)\n",
    "\n",
    "# Create the X, Y, Z coordinates for the rectangular prism\n",
    "Y = y * 40\n",
    "Z = x * 40\n",
    "\n",
    "# Create a figure and 3D axis\n",
    "fig = plt.figure(figsize=(32, 32))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot each layer separately\n",
    "layer_start = 10\n",
    "for i in range(layer_start, 45):\n",
    "    X = np.full_like(Y, i)  # Each layer is at a constant X value\n",
    "    values = data_reshape[i, :, :].T  # Transpose to match the shape\n",
    "    values += 1e-16  # Prevent log 0\n",
    "    \n",
    "    # Normalize and apply colormap\n",
    "    norm_values = norm(values)\n",
    "    print(f\"Layer {i}, normalized values min: {norm_values.min()}, max: {norm_values.max()}\")\n",
    "    facecolors = plt.cm.rainbow(norm_values)  # Apply colormap to normalized values\n",
    "    \n",
    "    facecolors[..., -1] = 1  # Set the alpha channel to 0.9 for transparency\n",
    "    \n",
    "    surf = ax.plot_surface(X, Y, Z, facecolors=facecolors, rstride=1, cstride=1, shade=False, edgecolor='k')\n",
    "\n",
    "# Add a color bar\n",
    "mappable = plt.cm.ScalarMappable(cmap=plt.cm.rainbow, norm=norm)\n",
    "mappable.set_array(data)\n",
    "cbar = fig.colorbar(mappable, ax=ax, shrink=0.5, aspect=10, pad=0.05)\n",
    "cbar.set_label('Energy (MeV)')\n",
    "\n",
    "# Add labels\n",
    "ax.set_xlabel('Layers')\n",
    "ax.set_ylabel('x (mm)')\n",
    "ax.set_zlabel('y (mm)', rotation=90)\n",
    "ax.set_xlim(44, 0)\n",
    "# ax.set_ylim(-40, 40)\n",
    "# ax.set_zlim(-40, 40)\n",
    "\n",
    "ax.set_box_aspect([45, 9, 16])\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "# Set the view angle\n",
    "ax.view_init(elev=-5, azim=4, roll = 89.2\n",
    "            )  # Adjust these values to change the view\n",
    "ax.grid(False)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.title('EM Shower in Calorimeter')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedaae2b-4e5c-4794-8d80-5c7a520f81fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import Normalize, LogNorm\n",
    "\n",
    "# Generate the array of 6480 values (example data)\n",
    "data = (xtarget_samples_orig[idx]).numpy()\n",
    "\n",
    "# Reshape the data to 45 layers x 16 slices x 9 circles\n",
    "count = 0\n",
    "data_reshape = np.zeros((45, 16, 9))\n",
    "print(\"Reshaped data shape:\", data_reshape.shape)\n",
    "for i in range(data_reshape.shape[0]):\n",
    "    for j in range(data_reshape.shape[1]):\n",
    "        for k in range(data_reshape.shape[2]):\n",
    "            data_reshape[i][j][k] = data[count]\n",
    "            count += 1\n",
    "\n",
    "data = data_reshape.copy()\n",
    "print(data.shape)\n",
    "mid = data.shape[-2] // 2\n",
    "shift = np.concatenate((data[:,mid:, [0]], data[:,:mid, [0]]), axis=-2)\n",
    "print(shift.shape)\n",
    "data = np.concatenate((shift, data), axis=-1)\n",
    "print(data.shape)\n",
    "data = np.pad(data, ((0, 0), (1,1), (0, 0)), mode='wrap')\n",
    "print(\"Data sum:\", data.sum())\n",
    "print(\"Data shape:\", data.shape)\n",
    "data_reshape = data\n",
    "\n",
    "# Check min and max values\n",
    "data_min = data_reshape.min()\n",
    "data_max = data_reshape.max()\n",
    "print(\"Data min:\", data_min, \"Data max:\", data_max)\n",
    "\n",
    "# Normalize the data using logarithmic normalization\n",
    "norm = LogNorm(vmin=1e+0, vmax=1e+4)  # Adding a small value to avoid log(0)\n",
    "\n",
    "print(\"Normalized data shape:\", data_reshape.shape)\n",
    "\n",
    "# Create the rectangular coordinates\n",
    "x = np.linspace(-1, 1, 19)\n",
    "y = np.linspace(-1, 1, 11)\n",
    "x, y = np.meshgrid(x, y)\n",
    "\n",
    "# Create the X, Y, Z coordinates for the rectangular prism\n",
    "Y = y * 40\n",
    "Z = x * 40\n",
    "\n",
    "# Create a figure and 3D axis\n",
    "fig = plt.figure(figsize=(32, 32))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot each layer separately\n",
    "layer_start = 10\n",
    "for i in range(layer_start, 45):\n",
    "    X = np.full_like(Y, i)  # Each layer is at a constant X value\n",
    "    values = data_reshape[i, :, :].T  # Transpose to match the shape\n",
    "    values += 1e-16  # Prevent log 0\n",
    "    \n",
    "    # Normalize and apply colormap\n",
    "    norm_values = norm(values)\n",
    "    print(f\"Layer {i}, normalized values min: {norm_values.min()}, max: {norm_values.max()}\")\n",
    "    facecolors = plt.cm.rainbow(norm_values)  # Apply colormap to normalized values\n",
    "    \n",
    "    facecolors[..., -1] = 1  # Set the alpha channel to 0.9 for transparency\n",
    "    \n",
    "    surf = ax.plot_surface(X, Y, Z, facecolors=facecolors, rstride=1, cstride=1, shade=False, edgecolor='k')\n",
    "\n",
    "# Add a color bar\n",
    "mappable = plt.cm.ScalarMappable(cmap=plt.cm.rainbow, norm=norm)\n",
    "mappable.set_array(data.reshape(-1))\n",
    "cbar = fig.colorbar(mappable, ax=ax, shrink=0.5, aspect=10, pad=0.05)\n",
    "cbar.set_label('Energy (MeV)')\n",
    "\n",
    "# Add labels\n",
    "ax.set_xlabel('Layers')\n",
    "ax.set_ylabel('x (mm)')\n",
    "ax.set_zlabel('y (mm)', rotation=90)\n",
    "ax.set_xlim(44, 0)\n",
    "# ax.set_ylim(-40, 40)\n",
    "# ax.set_zlim(-40, 40)\n",
    "\n",
    "ax.set_box_aspect([45, 30, 54])\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "# Set the view angle\n",
    "ax.view_init(elev=-0, azim=0, roll = 90\n",
    "            )  # Adjust these values to change the view\n",
    "ax.grid(False)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.title('EM Shower in Calorimeter')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee090273-9811-4de7-bad6-0de02efb61e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "# Generate the array of 6480 values (example data)\n",
    "data = np.random.random(6480) * 100  # Example data\n",
    "print(\"Data sum:\", data.sum())\n",
    "print(\"Data shape:\", data.shape)\n",
    "\n",
    "# Reshape the data to 45 layers x 16 slices x 9 circles\n",
    "data_reshape = data.reshape((45, 16, 9))\n",
    "print(\"Reshaped data shape:\", data_reshape.shape)\n",
    "\n",
    "# Check min and max values\n",
    "data_min = data_reshape.min()\n",
    "data_max = data_reshape.max()\n",
    "print(\"Data min:\", data_min, \"Data max:\", data_max)\n",
    "\n",
    "# Normalize the data using logarithmic normalization\n",
    "norm = LogNorm(vmin=1e-1, vmax=data_max)\n",
    "\n",
    "# Flatten the data for normalization\n",
    "flattened_data = data_reshape.flatten()\n",
    "\n",
    "# Normalize data for color mapping\n",
    "norm_data = norm(flattened_data)\n",
    "\n",
    "# Reshape normalized data to match the original shape\n",
    "norm_data_reshaped = norm_data.reshape(data_reshape.shape)\n",
    "\n",
    "# Create cylindrical coordinates\n",
    "theta = np.linspace(0, 2 * np.pi, 16, endpoint=False)\n",
    "r = np.linspace(0, 1, 9, endpoint=False)\n",
    "theta, r = np.meshgrid(theta, r)\n",
    "\n",
    "# Convert cylindrical to Cartesian coordinates\n",
    "x = r * np.cos(theta)\n",
    "y = r * np.sin(theta)\n",
    "z = np.arange(45)\n",
    "\n",
    "# Repeat coordinates for each layer\n",
    "x = np.repeat(x[np.newaxis, :, :], 45, axis=0)\n",
    "y = np.repeat(y[np.newaxis, :, :], 45, axis=0)\n",
    "z = z[:, np.newaxis, np.newaxis] * np.ones((1, 16, 9))\n",
    "\n",
    "# Create a figure and 3D axis\n",
    "fig = plt.figure(figsize=(30, 20))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Create a boolean array for where the data is non-zero\n",
    "filled = data_reshape > 0\n",
    "\n",
    "# Create the voxel plot\n",
    "colors = plt.cm.viridis(norm_data_reshaped)\n",
    "colors[..., -1] = 0.5  # Set alpha (transparency)\n",
    "\n",
    "# Plot the voxels\n",
    "ax.voxels(filled, facecolors=colors, edgecolor='k')\n",
    "\n",
    "# Add a color bar\n",
    "mappable = plt.cm.ScalarMappable(norm=norm, cmap=plt.cm.viridis)\n",
    "mappable.set_array(flattened_data)\n",
    "cbar = fig.colorbar(mappable, ax=ax, shrink=0.5, aspect=10, pad=-0.05)\n",
    "cbar.set_label('Energy (MeV)')\n",
    "\n",
    "# Add labels\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "ax.set_box_aspect([1, 1, 1])\n",
    "\n",
    "# Set the view angle\n",
    "ax.view_init(elev=30, azim=30)\n",
    "ax.grid(False)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.title('EM Shower in Calorimeter')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3fae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir = f'/home/luian1/CaloQVAE/figs/{modelname}'\n",
    "HLF_1_electron.relevantLayers = [0,5,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,30,35,40,44]\n",
    "\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "\n",
    "HLF_1_electron.DrawSingleShower(xtarget_samples[idx,:].detach().cpu().numpy(), filename=f'/home/luian1/CaloQVAE/figs/{modelname}/target_a_{idx}_{modelname}_{arch}_{datascaled}_{part}.png')\n",
    "HLF_1_electron.DrawSingleShower(xtarget_samples[idx,:].detach().cpu().numpy(), filename=None, vmax=1e+4, vmin=1e+0, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6c295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HLF_1_electron.DrawSingleShower(xrecon_samples[idx,:].detach().cpu().numpy(), filename=f'/home/luian1/Projects/CaloQVAE/figs/{modelname}/target_a_{idx}_{modelname}_{arch}_{datascaled}_{part}.png')\n",
    "HLF_1_electron.DrawSingleShower(xrecon_samples[idx,:].detach().cpu().numpy(), filename=None, vmax=1e+4, vmin=1e+0, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cd386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "HLF_1_electron.DrawSingleShower(xgen_samples[idx,:].detach().cpu().numpy(), filename=f'/home/luian1/CaloQVAE/figs/{modelname}/target_a_{idx}_{modelname}_{arch}_{datascaled}_{part}.png')\n",
    "HLF_1_electron.DrawSingleShower(xgen_samples[idx,:].detach().cpu().numpy(), filename=None, vmax=1e+4, vmin=1e+0, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f497784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use test dataset /fast_scratch/QVAE/test_data/dataset_2_2.hdf5\n",
    "# array from hdf5 -> convert to torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4a6029",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtarget_samples = []\n",
    "xrecon_samples = []\n",
    "xgen_samples = []\n",
    "scaled = True\n",
    "entarget_samples = []\n",
    "\n",
    "def itr_merge(*itrs):\n",
    "    for itr in itrs:\n",
    "        for v in itr:\n",
    "            yield v\n",
    "            \n",
    "# for xx in train_loader:\n",
    "for xx in new_test_loader:\n",
    "    in_data, true_energy, in_data_flat = engine._preprocess(xx[0],xx[1])\n",
    "    \n",
    "#     fwd_output = engine.model((in_data, true_energy), False)\n",
    "    \n",
    "#     if scaled:\n",
    "#         in_data = torch.tensor(engine._data_mgr.inv_transform(in_data.detach().cpu().numpy()))\n",
    "        \n",
    "        \n",
    "#         recon_data = torch.tensor(engine._data_mgr.inv_transform(fwd_output.output_activations.detach().cpu().numpy()))\n",
    "        \n",
    "        ## This is how youi generate data using model\n",
    "        # ================================\n",
    "        engine._model.sampler._batch_size = true_energy.shape[0]\n",
    "        sample_energies, sample_data = engine._model.generate_samples(num_samples=true_energy.shape[0], true_energy=true_energy) #engine._model.generate_samples_qpu(num_samples=128, true_energy=true_energy[:128])\n",
    "        engine._model.sampler._batch_size = engine._config.engine.rbm_batch_size\n",
    "        sample_data = torch.tensor(engine._data_mgr.inv_transform(sample_data.detach().cpu().numpy()))\n",
    "        # ================================\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"Script only supports scaled dataset at the moment...\")\n",
    "\n",
    "\n",
    "    xtarget_samples.append(in_data.detach().cpu())\n",
    "    xrecon_samples.append( recon_data.detach().cpu())\n",
    "    xgen_samples.append( sample_data.detach().cpu())\n",
    "    entarget_samples.append(true_energy.detach().cpu())\n",
    "    \n",
    "    \n",
    "xtarget_samples = torch.cat(xtarget_samples, dim=0)\n",
    "xrecon_samples = torch.cat(xrecon_samples, dim=0)\n",
    "xgen_samples = torch.cat(xgen_samples, dim=0)\n",
    "entarget_samples = torch.cat(entarget_samples, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f784e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a31808",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b771c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########Create Synth data\n",
    "print(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc02cd9e-a450-4249-931e-b16b28c88bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_loader)\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f4bf0e-8f44-46a6-ae68-746118e22172",
   "metadata": {},
   "outputs": [],
   "source": [
    "engen_samples2 = []\n",
    "xgen_samples2 = []\n",
    "with torch.no_grad():\n",
    "    for i,xx in enumerate(train_loader):\n",
    "        in_data, true_energy, in_data_flat = engine._preprocess(xx[0],xx[1])\n",
    "        if reducedata:\n",
    "            engine._model.sampler._batch_size = true_energy.shape[0]\n",
    "            sample_energies, sample_data = engine._model.generate_samples_cond(num_samples=true_energy.shape[0], true_energy=true_energy)\n",
    "            engine._model.sampler._batch_size = engine._config.engine.rbm_batch_size\n",
    "            sample_data = engine._reduceinv(sample_data, sample_energies, R=R)\n",
    "        elif scaled:\n",
    "            engine._model.sampler._batch_size = true_energy.shape[0]\n",
    "            sample_energies, sample_data = engine._model.generate_samples(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True)\n",
    "            engine._model.sampler._batch_size = engine._config.engine.rbm_batch_size\n",
    "            sample_data = torch.tensor(engine._data_mgr.inv_transform(sample_data.detach().cpu().numpy()))\n",
    "        else:\n",
    "            engine._model.sampler._batch_size = true_energy.shape[0]\n",
    "            sample_energies, sample_data = engine._model.generate_samples(num_samples=true_energy.shape[0], true_energy=true_energy)\n",
    "            engine._model.sampler._batch_size = engine._config.engine.rbm_batch_size\n",
    "            sample_data = sample_data.detach().cpu()*1000\n",
    "\n",
    "        xgen_samples2.append(sample_data.detach().cpu())\n",
    "        engen_samples2.append(true_energy.detach().cpu())\n",
    "        \n",
    "    for i,xx in enumerate(val_loader):\n",
    "        in_data, true_energy, in_data_flat = engine._preprocess(xx[0],xx[1])\n",
    "        if reducedata:\n",
    "            engine._model.sampler._batch_size = true_energy.shape[0]\n",
    "            sample_energies, sample_data = engine._model.generate_samples_cond(num_samples=true_energy.shape[0], true_energy=true_energy)\n",
    "            engine._model.sampler._batch_size = engine._config.engine.rbm_batch_size\n",
    "            sample_data = engine._reduceinv(sample_data, sample_energies, R=R)\n",
    "        elif scaled:\n",
    "            engine._model.sampler._batch_size = true_energy.shape[0]\n",
    "            sample_energies, sample_data = engine._model.generate_samples(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True)\n",
    "            engine._model.sampler._batch_size = engine._config.engine.rbm_batch_size\n",
    "            sample_data = torch.tensor(engine._data_mgr.inv_transform(sample_data.detach().cpu().numpy()))\n",
    "        else:\n",
    "            engine._model.sampler._batch_size = true_energy.shape[0]\n",
    "            sample_energies, sample_data = engine._model.generate_samples(num_samples=true_energy.shape[0], true_energy=true_energy)\n",
    "            engine._model.sampler._batch_size = engine._config.engine.rbm_batch_size\n",
    "            sample_data = sample_data.detach().cpu()*1000\n",
    "\n",
    "        xgen_samples2.append( sample_data.detach().cpu())\n",
    "        engen_samples2.append(true_energy.detach().cpu())\n",
    "\n",
    "    for i,xx in enumerate(test_loader):\n",
    "        in_data, true_energy, in_data_flat = engine._preprocess(xx[0],xx[1])\n",
    "        if reducedata:\n",
    "            engine._model.sampler._batch_size = true_energy.shape[0]\n",
    "            sample_energies, sample_data = engine._model.generate_samples_cond(num_samples=true_energy.shape[0], true_energy=true_energy)\n",
    "            engine._model.sampler._batch_size = engine._config.engine.rbm_batch_size\n",
    "            sample_data = engine._reduceinv(sample_data, sample_energies, R=R)\n",
    "        elif scaled:\n",
    "            engine._model.sampler._batch_size = true_energy.shape[0]\n",
    "            sample_energies, sample_data = engine._model.generate_samples(num_samples=true_energy.shape[0], true_energy=true_energy, measure_time=True)\n",
    "            engine._model.sampler._batch_size = engine._config.engine.rbm_batch_size\n",
    "            sample_data = torch.tensor(engine._data_mgr.inv_transform(sample_data.detach().cpu().numpy()))\n",
    "        else:\n",
    "            engine._model.sampler._batch_size = true_energy.shape[0]\n",
    "            sample_energies, sample_data = engine._model.generate_samples(num_samples=true_energy.shape[0], true_energy=true_energy)\n",
    "            engine._model.sampler._batch_size = engine._config.engine.rbm_batch_size\n",
    "            sample_data = sample_data.detach().cpu()*1000\n",
    "\n",
    "        xgen_samples2.append( sample_data.detach().cpu())\n",
    "        engen_samples2.append(true_energy.detach().cpu())\n",
    "        \n",
    "        # if i > 30:\n",
    "        #     break\n",
    "\n",
    "xgen_samples2 = torch.cat(xgen_samples2, dim=0)\n",
    "engen_samples2 = torch.cat(engen_samples2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1389d475-a987-4a70-86d7-18d70dbf7250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "import h5py\n",
    "\n",
    "# Convert tensors to numpy arrays as h5py does not support PyTorch tensors directly\n",
    "tensor1_np = xgen_samples2[:100000,:].numpy()\n",
    "tensor2_np = engen_samples2[:100000,:].numpy()\n",
    "\n",
    "# Create a new HDF5 file\n",
    "with h5py.File(f'/fast_scratch_1/caloqvae/syn_data/dataset2_synthetic_{modelname}en130.hdf5', 'w') as f:\n",
    "    # Create datasets for your tensors\n",
    "    f.create_dataset('showers', data=tensor1_np)\n",
    "    f.create_dataset('incidence energy', data=tensor2_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5278d9-6b6e-40ba-8032-3937ad89e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jetnet\n",
    "# Refer to Documentation: https://jetnet.readthedocs.io/en/latest/pages/metrics.html#jetnet.evaluation.fpd\n",
    "# Metrics will be implemented once dataset # and sample # matches\n",
    "# jetnet.evaluation.fpd(real_features: Tensor | np.ndarray, gen_features: Tensor | np.ndarray, min_samples: int = 20000, max_samples: int = 50000, num_batches: int = 20, num_points: int = 10, normalise: bool = True, seed: int = 42) tuple[float, float]\n",
    "# jetnet.evaluation.kpd(real_features: Tensor | np.ndarray, gen_features: Tensor | np.ndarray, num_batches: int = 10, batch_size: int = 5000, normalise: bool = True, seed: int = 42, num_threads: int | None = None) tuple[float, float]\n",
    "\n",
    "def get_HLData(voxels, e_inc):\n",
    "    \n",
    "\n",
    "fpd_met = jetnet.evaluation.fpd(xtarget_samples, xgen_samples_1429)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
