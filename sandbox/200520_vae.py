
"""200520_VAE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VXx24D7qE2U4kR064qMiIcoaq5wbYVPH

# Variational Autoencoders

Code adapted from https://github.com/pytorch/examples/blob/master/vae/main.py
"""

#pyTorch: Open source ML library dev. mainly by Facebook's AI research lab
import torch
#torchvision contains popular datasets, model architectures for computer vision
from torchvision import datasets, transforms
#autograd: package for automatic differentiation (necessary for gradients)
from torch.autograd import Variable

import torch.nn as nn
import torch.nn.functional as F

from torch.distributions import Normal

import numpy as np

import matplotlib.pyplot as plt

#@title Helper Functions
def plot_autoencoder_outputs(model, n, dims):
    decoded_imgs = model.decode(x_test)

    # number of example digits to show
    n = 5
    plt.figure(figsize=(10, 4.5))
    for i in range(n):
        # plot original image
        ax = plt.subplot(2, n, i + 1)
        plt.imshow(x_test[i].reshape(*dims))
        plt.gray()
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
        if i == n/2:
            ax.set_title('Original Images')

        # plot reconstruction 
        ax = plt.subplot(2, n, i + 1 + n)
        plt.imshow(decoded_imgs[i].reshape(*dims))
        plt.gray()
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
        if i == n/2:
            ax.set_title('Reconstructed Images')
    plt.show()

def plot_loss(history):
    historydf = pd.DataFrame(history.history, index=history.epoch)
    plt.figure(figsize=(8, 6))
    historydf.plot(ylim=(0, historydf.values.max()))
    plt.title('Loss: %.3f' % history.history['loss'][-1])
    
def plot_compare_histories(history_list, name_list, plot_accuracy=True):
    dflist = []
    min_epoch = len(history_list[0].epoch)
    losses = []
    for history in history_list:
        h = {key: val for key, val in history.history.items() if not key.startswith('val_')}
        dflist.append(pd.DataFrame(h, index=history.epoch))
        min_epoch = min(min_epoch, len(history.epoch))
        losses.append(h['loss'][-1])

    historydf = pd.concat(dflist, axis=1)

    metrics = dflist[0].columns
    idx = pd.MultiIndex.from_product([name_list, metrics], names=['model', 'metric'])
    historydf.columns = idx
    
    plt.figure(figsize=(6, 8))

    ax = plt.subplot(211)
    historydf.xs('loss', axis=1, level='metric').plot(ylim=(0,1), ax=ax)
    plt.title("Training Loss: " + ' vs '.join([str(round(x, 3)) for x in losses]))
    
    if plot_accuracy:
        ax = plt.subplot(212)
        historydf.xs('acc', axis=1, level='metric').plot(ylim=(0,1), ax=ax)
        plt.title("Accuracy")
        plt.xlabel("Epochs")
    
    plt.xlim(0, min_epoch-1)
    plt.tight_layout()

n_batch_samples = 128
n_epochs = 10
learning_rate = 1e-3
LATENT_DIMS = 32

torch.manual_seed(1);

"""Load up the MNIST data."""

train_loader = torch.utils.data.DataLoader(
    datasets.MNIST(root='./data', train=True, download=True, 
                       transform=transforms.Compose([
                           transforms.ToTensor()
                       ])),
    batch_size=n_batch_samples, shuffle=True)

test_loader = torch.utils.data.DataLoader(
    datasets.MNIST(root='./data', train=False, 
                       transform=transforms.Compose([
                           transforms.ToTensor()
                       ])),
    batch_size=n_batch_samples, shuffle=True)

"""Let's design a class for a VAE. We will need to define:
 - an encoder network
 - a decoder network
 - a reparameterization method
"""

class VAE(nn.Module):
    def __init__(self):
        super(VAE, self).__init__()

        #define network architecture
        # self._encoderNodes=[(784,400),
        #                     (400,200),
        #                     (200,200),
        #                     (200,100)]

        # self._reparamNodes=(100,LATENT_DIMS)  

        # self._decoderNodes=[(LATENT_DIMS,100),
        #                     (100,200),
        #                     (200,200),
        #                     (200,400)
        #                     ]

        # self._outputNodes=(400,784)  
        self._encoderNodes=[(784,128),]

        self._reparamNodes=(128,LATENT_DIMS)  

        self._decoderNodes=[(LATENT_DIMS,128),]

        self._outputNodes=(128,784)     

        self._encoderLayers=nn.ModuleList([])
        self._decoderLayers=nn.ModuleList([])
        self._reparamLayers=nn.ModuleDict({'mu':nn.Linear(self._reparamNodes[0],self._reparamNodes[1]),
                             'var':nn.Linear(self._reparamNodes[0],self._reparamNodes[1])
        })
        self._outputLayer=nn.Linear(self._outputNodes[0],self._outputNodes[1])

        for node in self._encoderNodes:
          self._encoderLayers.append(
              nn.Linear(node[0],node[1])
              )
        for node in self._decoderNodes:
          self._decoderLayers.append(
              nn.Linear(node[0],node[1])
              )        

        # activation functions per layer
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
        
    def encode(self, x):
        for layer in self._encoderLayers:
          x=self.relu(layer(x))
        # Split for reparameterization
        mu = self._reparamLayers['mu'](x)
        logvar = self._reparamLayers['var'](x)
        return mu, logvar
    
    def reparameterize(self, mu, logvar):
        """ Sample from the normal distributions corres and return var * samples + mu
        """
        eps = torch.randn_like(mu)
        return mu + eps*torch.exp(0.5 * logvar)
        
    def decode(self, z):
        for layer in self._decoderLayers:
          z=self.relu(layer(z))
        x_prime = self.sigmoid(self._outputLayer(z))                 
        return x_prime
                            
    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, 784))
        z = self.reparameterize(mu, logvar)
        x_prime = self.decode(z)
        return x_prime, mu, logvar

"""The loss function consists of two terms: 
 - the autoencoding term, which quantifies the quality of the reconstruction
 - the KL term, which tries to keep the distribution of the latent variables close to Gaussian
"""

def loss(x, output_data, mu, logvar):
    # Autoencoding term
    auto_loss = F.binary_cross_entropy(output_data, x.view(-1, 784), reduction='sum')
     
    # KL loss term assuming Gaussian-distributed latent variables
    kl_loss = 0.5 * torch.sum(1 + logvar - mu.pow(2) - torch.exp(logvar))
    return auto_loss - kl_loss

"""Methods for training and testing."""

def train(model, train_loader, optimizer, epoch):
    model.train()
    total_train_loss = 0
    for batch_idx, (input_data, label) in enumerate(train_loader):
        optimizer.zero_grad()
        input_data = Variable(input_data)
        output_data, mu, logvar = model(input_data)
        train_loss = loss(input_data, output_data, mu, logvar)
        train_loss.backward()
        total_train_loss += train_loss.item()
        optimizer.step()
        
        # Output logging
        if batch_idx % 100 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(input_data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), train_loss.data.item() / len(input_data)))

def test(model, test_loader):
    model.eval()
    test_loss = 0
    with torch.no_grad():
        for batch_idx, (input_data, label) in enumerate(test_loader):
            output_data, mu, logvar = model(input_data)
            test_loss += loss(input_data, output_data, mu, logvar)
        
    test_loss /= len(test_loader.dataset)

"""Run the training sequence."""

model = VAE()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

for epoch in range(1, n_epochs+1):
    train(model, train_loader, optimizer, epoch)
    #test(model, test_loader)

batch_mu = np.zeros((n_batch_samples, LATENT_DIMS))
batch_logvar = np.zeros((n_batch_samples, LATENT_DIMS))

with torch.no_grad():
    for batch_idx, (input_data, label) in enumerate(test_loader):
        output_data, mu, logvar = model(input_data)
        batch_mu = mu
        batch_logvar = logvar

#trained with list-like code
n_samples=5

plt.figure(figsize=(10, 4.5))
for i in range(n_samples):

      # plot original image
    ax = plt.subplot(2, 5, i + 1)
    plt.imshow(input_data[i].reshape((28, 28)))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)

    ax = plt.subplot(2, n_samples, i + 1 + n_samples)
    decImg=output_data[i].reshape((28, 28))
    plt.imshow(decImg)
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()

#trained with original code and loss of ~130
n_samples=5

plt.figure(figsize=(10, 4.5))
for i in range(n_samples):

      # plot original image
    ax = plt.subplot(2, 5, i + 1)
    plt.imshow(input_data[i].reshape((28, 28)))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)

    ax = plt.subplot(2, n_samples, i + 1 + n_samples)
    decImg=output_data[i].reshape((28, 28))
    plt.imshow(decImg)
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()

plt.hist(batch_mu[:, 4])

plt.hist(np.exp(batch_logvar[:, 4]))

