{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3b91d7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83c62d78-3fe1-4f9d-9410-15a2760c16ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5f31300-2b9f-4536-a783-b84407852f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ef[\"layer_0\"].astype('float32')\n",
    "# type(ef[\"layer_0\"].astype('float32')[1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "400c50e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ef = h5py.File('/raid/javier/Datasets/CaloVAE/data/calo/eplus.hdf5','r')\n",
    "gf = h5py.File('/raid/javier/Datasets/CaloVAE/data/calo/gamma.hdf5','r')\n",
    "pf = h5py.File('/raid/javier/Datasets/CaloVAE/data/calo/piplus.hdf5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2457aed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs = [ef, gf, pf]\n",
    "nplcats = []\n",
    "\n",
    "for hdf in hdfs:\n",
    "    npl0 = np.array(hdf['layer_0']).astype('float32')\n",
    "    npl1 = np.array(hdf['layer_1']).astype('float32')\n",
    "    npl2 = np.array(hdf['layer_2']).astype('float32')\n",
    "    \n",
    "    npl0 = npl0.reshape(npl0.shape[0], -1)\n",
    "    npl1 = npl1.reshape(npl1.shape[0], -1)\n",
    "    npl2 = npl2.reshape(npl2.shape[0], -1)\n",
    "    \n",
    "    nplcats.append(np.concatenate([npl0, npl1, npl2], axis=1))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "423ccda6",
   "metadata": {},
   "source": [
    "# Fit and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3837c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "nplcatscaled = []\n",
    "transformers = []\n",
    "arrmins = [[], [], []]\n",
    "epsilon = 1e-2\n",
    "\n",
    "for i in range(len(nplcats)):\n",
    "    nparr = nplcats[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    transformer = StandardScaler().fit(nparr)\n",
    "    nparr = transformer.transform(nparr)\n",
    "    transformers.append(transformer)\n",
    "    \n",
    "    nparr = np.where(np.isnan(nparr), np.inf, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        \n",
    "        if arrmin < 0 and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= arrmin\n",
    "            nparr[:, j] += epsilon\n",
    "            arrmins[i].append(arrmin)\n",
    "        else:\n",
    "            arrmins[i].append(0.)\n",
    "            \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        if arrmin < 0:\n",
    "            print(j, arrmin)\n",
    "            \n",
    "    nplcatscaled.append(nparr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4517385b-e8e4-4763-8c9a-eadfa34c96b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 504)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nplcats[2].shape\n",
    "nplcatscaled[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2f88b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "ef_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/calo_scaled/eplus.hdf5','w')\n",
    "gf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/calo_scaled/gamma.hdf5','w')\n",
    "pf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/calo_scaled/piplus.hdf5','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02bed298",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_scaled = [ef_scaled, gf_scaled, pf_scaled]\n",
    "layer_shapes = {}\n",
    "for key in hdf.keys():\n",
    "    if key == \"energy\" or key == \"overflow\":\n",
    "        pass\n",
    "    else:\n",
    "        layer_shapes[key] = hdf[key].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9a14a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer_0': (100000, 3, 96),\n",
       " 'layer_1': (100000, 12, 12),\n",
       " 'layer_2': (100000, 12, 6)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9e9190-11ae-4045-aaa7-c673658a0925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd57184b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 504)\n",
      "(100000, 288)\n",
      "(100000, 504)\n",
      "(100000, 144)\n",
      "(100000, 504)\n",
      "(100000, 72)\n",
      "(100000, 504)\n",
      "(100000, 288)\n",
      "(100000, 504)\n",
      "(100000, 144)\n",
      "(100000, 504)\n",
      "(100000, 72)\n",
      "(100000, 504)\n",
      "(100000, 288)\n",
      "(100000, 504)\n",
      "(100000, 144)\n",
      "(100000, 504)\n",
      "(100000, 72)\n"
     ]
    }
   ],
   "source": [
    "for hdf, hdf_scaled, scaled_data in zip(hdfs, hdfs_scaled, nplcatscaled):\n",
    "    offset = 0\n",
    "    for key in hdf.keys():\n",
    "        if key == \"energy\" or key == \"overflow\":\n",
    "            hdf_scaled.create_dataset(key, data=hdf[key])\n",
    "        else:\n",
    "            layer_shape = layer_shapes[key]\n",
    "            print(scaled_data.shape)\n",
    "            layer_data = scaled_data[:, offset:offset+(layer_shape[1]*layer_shape[2])]\n",
    "            print(layer_data.shape)\n",
    "            layer_data = layer_data.reshape(layer_shape)\n",
    "            hdf_scaled.create_dataset(key, data=layer_data)\n",
    "            offset += layer_shape[1]*layer_shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00374e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy (100000, 1) float64\n",
      "layer_0 (100000, 3, 96) float32\n",
      "layer_1 (100000, 12, 12) float32\n",
      "layer_2 (100000, 12, 6) float32\n",
      "overflow (100000, 3) float64\n",
      "energy (100000, 1) float64\n",
      "layer_0 (100000, 3, 96) float32\n",
      "layer_1 (100000, 12, 12) float32\n",
      "layer_2 (100000, 12, 6) float32\n",
      "overflow (100000, 3) float64\n",
      "energy (100000, 1) float64\n",
      "layer_0 (100000, 3, 96) float32\n",
      "layer_1 (100000, 12, 12) float32\n",
      "layer_2 (100000, 12, 6) float32\n",
      "overflow (100000, 3) float64\n"
     ]
    }
   ],
   "source": [
    "for hdf_scaled in hdfs_scaled:\n",
    "    for key in hdf_scaled.keys():\n",
    "        print(key, hdf_scaled[key].shape, hdf_scaled[key].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a86e25a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hdf_scaled in hdfs_scaled:\n",
    "    hdf_scaled.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48612961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 504)\n",
      "(100000, 504)\n",
      "(100000, 504)\n"
     ]
    }
   ],
   "source": [
    "for nplcat in nplcatscaled:\n",
    "    print(nplcat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "896840f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504\n"
     ]
    }
   ],
   "source": [
    "print(len(arrmins[0]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a934db67",
   "metadata": {},
   "source": [
    "Inverse transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fcc908f-5409-47d9-9b9d-78cd3f79cfe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(nplcatscaled[2] == np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63b75d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = transformers[i]\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1617be2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 142 -0.3723908\n",
      "0 143 0.5914335\n",
      "0 144 1.3928509\n",
      "0 353 9.338834\n",
      "0 354 9.172087\n",
      "0 365 -0.5450735\n",
      "0 366 0.036711693\n",
      "1 354 7.1371098\n",
      "1 365 -9.951949\n",
      "1 366 10.38685\n",
      "2 144 -0.120242506\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if np.abs(diff) > 0:\n",
    "            print(i, j, diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c979807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = transformers[0].get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6774b743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'copy': True, 'with_mean': True, 'with_std': True}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1965a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5db7472",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(transformers[0], 'scaler.gz')\n",
    "transformer = joblib.load('scaler.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d5dd6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(transformers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "467558d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/raid/javier/Datasets/CaloVAE/data/calo_scaled/piplus_scaler.gz']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(transformers[0], '/raid/javier/Datasets/CaloVAE/data/calo_scaled/eplus_scaler.gz')\n",
    "joblib.dump(transformers[1], '/raid/javier/Datasets/CaloVAE/data/calo_scaled/gamma_scaler.gz')\n",
    "joblib.dump(transformers[2], '/raid/javier/Datasets/CaloVAE/data/calo_scaled/piplus_scaler.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b758880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_transformers = []\n",
    "ld_transformers.append(joblib.load('/raid/javier/Datasets/CaloVAE/data/calo_scaled/eplus_scaler.gz'))\n",
    "ld_transformers.append(joblib.load('/raid/javier/Datasets/CaloVAE/data/calo_scaled/gamma_scaler.gz'))\n",
    "ld_transformers.append(joblib.load('/raid/javier/Datasets/CaloVAE/data/calo_scaled/piplus_scaler.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "347ee94d-dd08-4bd7-baab-1e175ccc5fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(nparr == np.inf)\n",
    "# nplcatscaled[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b905d268",
   "metadata": {},
   "outputs": [],
   "source": [
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.inf)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = ld_transformers[i]\n",
    "    # try:\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    # except:\n",
    "        # print(nparr)\n",
    "    \n",
    "    nparr = np.where(np.isnan(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13598718-3b19-44fd-bd5a-ae3d7a4834cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "429b88a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 143 0.5914335\n",
      "0 144 1.3928509\n",
      "0 353 9.338834\n",
      "0 354 9.172087\n",
      "0 366 0.036711693\n",
      "1 354 7.1371098\n",
      "1 366 10.38685\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if diff > 1e-4:\n",
    "            print(i, j, diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19abaec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(arrmins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d377d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(arrmins[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0626a935",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ptype in enumerate([\"eplus\", \"gamma\", \"piplus\"]):\n",
    "    filepath = \"/raid/javier/Datasets/CaloVAE/data/calo_scaled/\" + ptype + \"_amin.npy\"\n",
    "    with open(filepath, 'wb') as f:\n",
    "        np.save(f, arrmins[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0e544b-9a86-427e-8509-5d23415bb8dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0dadf758-7227-4257-800f-d68d54cd2f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(nplcatinv[0][1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89554d4b-83a5-485b-b270-5beb19abb7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATLAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40de26d2-3088-4361-97cd-d0e517cdcb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "gf = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas/photons_samples_highStat_En_5.hdf5', 'r')\n",
    "pf = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas/pions_samples_highStat_En_5.hdf5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd3286c6-b86a-471c-a658-de21ca7f4683",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs = [gf,pf]\n",
    "nplcats = []\n",
    "\n",
    "for hdf in hdfs:\n",
    "    npl0 = np.array(hdf['voxels'])\n",
    "    \n",
    "    npl0 = npl0.reshape(npl0.shape[0], -1)\n",
    "    \n",
    "    nplcats.append(np.concatenate([npl0], axis=1))\n",
    "    \n",
    "nplcatscaled = []\n",
    "transformers = []\n",
    "arrmins = [[], [], []]\n",
    "epsilon = 1e-2\n",
    "\n",
    "for i in range(len(nplcats)):\n",
    "    nparr = nplcats[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    transformer = StandardScaler().fit(nparr)\n",
    "    nparr = transformer.transform(nparr)\n",
    "    transformers.append(transformer)\n",
    "    \n",
    "    nparr = np.where(np.isnan(nparr), np.inf, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        \n",
    "        if arrmin < 0 and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= arrmin\n",
    "            nparr[:, j] += epsilon\n",
    "            arrmins[i].append(arrmin)\n",
    "        else:\n",
    "            arrmins[i].append(0.)\n",
    "            \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        if arrmin < 0:\n",
    "            print(j, arrmin)\n",
    "            \n",
    "    nplcatscaled.append(nparr)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6cae609d-f5c2-4dcd-a47e-b2d9fc423f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "gf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas_scaled/photons_samples_highStat_En_5.hdf5', 'w')\n",
    "pf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas_scaled/pions_samples_highStat_En_5.hdf5', 'w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f04a7562-a508-4b2f-a261-ee47621e8e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########\n",
      "(99933, 368)\n",
      "(99933, 368)\n",
      "(99933, 368)\n",
      "###########\n",
      "(99999, 533)\n",
      "(99999, 533)\n",
      "(99999, 533)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hdfs_scaled = [gf_scaled, pf_scaled]\n",
    "# hdfs_scaled = [ef_scaled, gf_scaled, pf_scaled]\n",
    "layer_shapes = {}\n",
    "for hdf in hdfs:\n",
    "    for key in hdf.keys():\n",
    "        if key == \"energy\" or key == \"overflow\" or key == \"energy_from_voxels\":\n",
    "            pass\n",
    "        else:\n",
    "#             layer_shapes[key] = hdf[key].shape\n",
    "            layer_shapes[hdf] = {key : hdf[key].shape}\n",
    "        \n",
    "        \n",
    "\n",
    "for hdf, hdf_scaled, scaled_data in zip(hdfs, hdfs_scaled, nplcatscaled):\n",
    "    offset = 0\n",
    "    for key in hdf.keys():\n",
    "        if key == \"energy\" or key == \"overflow\" or key == \"energy_from_voxels\":\n",
    "            hdf_scaled.create_dataset(key, data=hdf[key])\n",
    "        else:\n",
    "            layer_shape = layer_shapes[hdf][key]\n",
    "            print(\"###########\")\n",
    "            print(layer_shape)\n",
    "            print(scaled_data.shape)\n",
    "#             layer_data = scaled_data[:, offset:offset+(layer_shape[1]*layer_shape[2])]\n",
    "            layer_data = scaled_data[:, offset:offset+layer_shape[1]]\n",
    "            print(layer_data.shape)\n",
    "            layer_data = layer_data.reshape(layer_shape)\n",
    "            hdf_scaled.create_dataset(key, data=layer_data)\n",
    "#             offset += layer_shape[1]*layer_shape[2]\n",
    "            offset += layer_shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a123718-52cd-4860-8d3e-6dc7e1732ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy (99933, 1) float64\n",
      "energy_from_voxels (99933, 1) float64\n",
      "voxels (99933, 368) float64\n",
      "energy (99999, 1) float64\n",
      "energy_from_voxels (99999, 1) float64\n",
      "voxels (99999, 533) float64\n"
     ]
    }
   ],
   "source": [
    "for hdf_scaled in hdfs_scaled:\n",
    "    for key in hdf_scaled.keys():\n",
    "        print(key, hdf_scaled[key].shape, hdf_scaled[key].dtype)\n",
    "        \n",
    "for hdf_scaled in hdfs_scaled:\n",
    "    hdf_scaled.close()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79f49c8c-49d3-4ebb-8306-8b3a7c3749f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform\n",
    "\n",
    "\n",
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = transformers[i]\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)\n",
    "\n",
    "\n",
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if diff > 0:\n",
    "            print(i, j, diff)\n",
    "            \n",
    "params = transformers[0].get_params()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3d7ec3-d5ba-4162-b87d-ebdc160d5b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "gf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas_scaled/photons_samples_highStat_En_5.hdf5', 'w')\n",
    "pf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas_scaled/pions_samples_highStat_En_5.hdf5', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19e061e8-21bd-4824-b887-7d1b3da4d9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/raid/javier/Datasets/CaloVAE/data/atlas_scaled/pions_samples_highStat_En_5_scaler.gz']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# joblib.dump(transformers[0], 'scaler.gz')\n",
    "# transformer = joblib.load('scaler.gz')\n",
    "\n",
    "joblib.dump(transformers[0], '/raid/javier/Datasets/CaloVAE/data/atlas_scaled/photons_samples_highStat_En_5_scaler.gz')\n",
    "joblib.dump(transformers[1], '/raid/javier/Datasets/CaloVAE/data/atlas_scaled/pions_samples_highStat_En_5_scaler.gz')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5eafd19-49fe-4b2a-92e8-eb7a0357e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_transformers = []\n",
    "ld_transformers.append(joblib.load('/raid/javier/Datasets/CaloVAE/data/atlas_scaled/photons_samples_highStat_En_5_scaler.gz'))\n",
    "ld_transformers.append(joblib.load('/raid/javier/Datasets/CaloVAE/data/atlas_scaled/pions_samples_highStat_En_5_scaler.gz'))\n",
    "\n",
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.inf)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = ld_transformers[i]\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    \n",
    "    nparr = np.where(np.isnan(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5984977e-834a-435d-8126-6f7f6417a5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if diff > 1e-4:\n",
    "            print(i, j, diff)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0485c9a8-c9f1-4f26-a216-8cd8f453e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ptype in enumerate([\"photons_samples_highStat_En_5\", \"pions_samples_highStat_En_5\"]):\n",
    "    filepath = \"/raid/javier/Datasets/CaloVAE/data/atlas_scaled/\" + ptype + \"_amin.npy\"\n",
    "    with open(filepath, 'wb') as f:\n",
    "        np.save(f, arrmins[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3286a739-1038-427e-864a-9daab66b60a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATLAS ds 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62609fbe-45bb-4930-a39a-43ef6c5fa7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "033a9323-327a-4968-b1fc-b2d1fa985f85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gf = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas_dataset2and3/dataset_2_1.hdf5', 'r')\n",
    "# pf = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas/pions_samples_highStat_En_5.hdf5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abf53fcf-8527-4d2c-ada9-bb393657d0d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hdfs = [gf]\n",
    "nplcats = []\n",
    "\n",
    "for hdf in hdfs:\n",
    "    npl0 = np.array(hdf['showers'])\n",
    "    \n",
    "    npl0 = npl0.reshape(npl0.shape[0], -1)\n",
    "    \n",
    "    nplcats.append(np.concatenate([npl0], axis=1))\n",
    "    \n",
    "nplcatscaled = []\n",
    "transformers = []\n",
    "arrmins = [[], [], []]\n",
    "epsilon = 1e-2\n",
    "\n",
    "for i in range(len(nplcats)):\n",
    "    nparr = nplcats[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    transformer = StandardScaler().fit(nparr)\n",
    "    nparr = transformer.transform(nparr)\n",
    "    transformers.append(transformer)\n",
    "    \n",
    "    nparr = np.where(np.isnan(nparr), np.inf, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        \n",
    "        if arrmin < 0 and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= arrmin\n",
    "            nparr[:, j] += epsilon\n",
    "            arrmins[i].append(arrmin)\n",
    "        else:\n",
    "            arrmins[i].append(0.)\n",
    "            \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        if arrmin < 0:\n",
    "            print(j, arrmin)\n",
    "            \n",
    "    nplcatscaled.append(nparr)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a42a4b14-73d7-4a9a-a13d-31c5d7afc4f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas_dataset2and3_scaled/dataset_2_1.hdf5', 'w')\n",
    "# pf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas_scaled/pions_samples_highStat_En_5.hdf5', 'w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e72b951-1f55-4bce-bd58-9f41f0555ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########\n",
      "(100000, 6480)\n",
      "(100000, 6480)\n",
      "(100000, 6480)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hdfs_scaled = [gf_scaled]\n",
    "# hdfs_scaled = [ef_scaled, gf_scaled, pf_scaled]\n",
    "layer_shapes = {}\n",
    "for hdf in hdfs:\n",
    "    for key in hdf.keys():\n",
    "        if key == \"energy\" or key == \"overflow\" or key == \"energy_from_voxels\":\n",
    "            pass\n",
    "        else:\n",
    "#             layer_shapes[key] = hdf[key].shape\n",
    "            layer_shapes[hdf] = {key : hdf[key].shape}\n",
    "        \n",
    "        \n",
    "\n",
    "for hdf, hdf_scaled, scaled_data in zip(hdfs, hdfs_scaled, nplcatscaled):\n",
    "    offset = 0\n",
    "    for key in hdf.keys():\n",
    "        if key == \"energy\" or key == \"overflow\" or key == \"energy_from_voxels\" or key == \"incident_energies\":\n",
    "            hdf_scaled.create_dataset(key, data=hdf[key])\n",
    "        else:\n",
    "            layer_shape = layer_shapes[hdf][key]\n",
    "            print(\"###########\")\n",
    "            print(layer_shape)\n",
    "            print(scaled_data.shape)\n",
    "#             layer_data = scaled_data[:, offset:offset+(layer_shape[1]*layer_shape[2])]\n",
    "            layer_data = scaled_data[:, offset:offset+layer_shape[1]]\n",
    "            print(layer_data.shape)\n",
    "            layer_data = layer_data.reshape(layer_shape)\n",
    "            hdf_scaled.create_dataset(key, data=layer_data)\n",
    "#             offset += layer_shape[1]*layer_shape[2]\n",
    "            offset += layer_shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a63a2eac-4e53-4b65-9c4d-0bd1e443c9ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incident_energies (100000, 1) float64\n",
      "showers (100000, 6480) float64\n"
     ]
    }
   ],
   "source": [
    "for hdf_scaled in hdfs_scaled:\n",
    "    for key in hdf_scaled.keys():\n",
    "        print(key, hdf_scaled[key].shape, hdf_scaled[key].dtype)\n",
    "        \n",
    "for hdf_scaled in hdfs_scaled:\n",
    "    hdf_scaled.close()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfc416f7-0982-4ea5-ad65-4a64f85e6da2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inverse transform\n",
    "\n",
    "\n",
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = transformers[i]\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)\n",
    "\n",
    "\n",
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if diff > 0:\n",
    "            print(i, j, diff)\n",
    "            \n",
    "params = transformers[0].get_params()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8aa3c5f-b901-4d5c-9782-391258b1a3f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas_dataset2and3_scaled/dataset_2_1.hdf5', 'w')\n",
    "# pf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas_scaled/pions_samples_highStat_En_5.hdf5', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f85459cc-9b75-4a05-9d30-ecb06609640f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/raid/javier/Datasets/CaloVAE/data/atlas_dataset2and3_scaled/dataset_2_1_scaler.gz']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# joblib.dump(transformers[0], 'scaler.gz')\n",
    "# transformer = joblib.load('scaler.gz')\n",
    "\n",
    "joblib.dump(transformers[0], '/raid/javier/Datasets/CaloVAE/data/atlas_dataset2and3_scaled/dataset_2_1_scaler.gz')\n",
    "# joblib.dump(transformers[1], '/raid/javier/Datasets/CaloVAE/data/atlas_scaled/pions_samples_highStat_En_5_scaler.gz')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7668fa4-a610-4f24-b70b-ca6070fe451e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ld_transformers = []\n",
    "ld_transformers.append(joblib.load('/raid/javier/Datasets/CaloVAE/data/atlas_dataset2and3_scaled/dataset_2_1_scaler.gz'))\n",
    "# ld_transformers.append(joblib.load('/raid/javier/Datasets/CaloVAE/data/atlas_scaled/pions_samples_highStat_En_5_scaler.gz'))\n",
    "\n",
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    # nparr = np.where(nparr > 0., nparr, np.inf)\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = ld_transformers[i]\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    \n",
    "    # nparr = np.where(np.isnan(nparr), 0, nparr)\n",
    "    nparr = np.where(np.isnan(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75063bfa-653e-4127-bdb4-113e11ca50af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if diff > 1e-4:\n",
    "            print(i, j, diff)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "284aa357-e289-4db7-ad9c-1c6fdabe6799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, ptype in enumerate([\"dataset_2_1\"]):\n",
    "    filepath = \"/raid/javier/Datasets/CaloVAE/data/atlas_dataset2and3_scaled/\" + ptype + \"_amin.npy\"\n",
    "    with open(filepath, 'wb') as f:\n",
    "        np.save(f, arrmins[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0033a180-7afc-4c0e-8a08-0b13cbeeb12d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
