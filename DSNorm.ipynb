{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "608b5ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fddb16d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e2de7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ef[\"layer_0\"].astype('float32')\n",
    "# type(ef[\"layer_0\"].astype('float32')[1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e46d504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ef = h5py.File('/raid/javier/Datasets/CaloVAE/data/calo/eplus.hdf5','r')\n",
    "gf = h5py.File('/raid/javier/Datasets/CaloVAE/data/calo/gamma.hdf5','r')\n",
    "pf = h5py.File('/raid/javier/Datasets/CaloVAE/data/calo/piplus.hdf5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "443444fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs = [ef, gf, pf]\n",
    "nplcats = []\n",
    "\n",
    "for hdf in hdfs:\n",
    "    npl0 = np.array(hdf['layer_0']).astype('float32')\n",
    "    npl1 = np.array(hdf['layer_1']).astype('float32')\n",
    "    npl2 = np.array(hdf['layer_2']).astype('float32')\n",
    "    \n",
    "    npl0 = npl0.reshape(npl0.shape[0], -1)\n",
    "    npl1 = npl1.reshape(npl1.shape[0], -1)\n",
    "    npl2 = npl2.reshape(npl2.shape[0], -1)\n",
    "    \n",
    "    nplcats.append(np.concatenate([npl0, npl1, npl2], axis=1))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1a9c95d7",
   "metadata": {},
   "source": [
    "# Fit and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e5121cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nplcatscaled = []\n",
    "transformers = []\n",
    "arrmins = [[], [], []]\n",
    "epsilon = 1e-2\n",
    "\n",
    "for i in range(len(nplcats)):\n",
    "    nparr = nplcats[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    transformer = StandardScaler().fit(nparr)\n",
    "    nparr = transformer.transform(nparr)\n",
    "    transformers.append(transformer)\n",
    "    \n",
    "    nparr = np.where(np.isnan(nparr), np.inf, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        \n",
    "        if arrmin < 0 and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= arrmin\n",
    "            nparr[:, j] += epsilon\n",
    "            arrmins[i].append(arrmin)\n",
    "        else:\n",
    "            arrmins[i].append(0.)\n",
    "            \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        if arrmin < 0:\n",
    "            print(j, arrmin)\n",
    "            \n",
    "    nplcatscaled.append(nparr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7771685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 504)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nplcats[2].shape\n",
    "nplcatscaled[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "231c73f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ef_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/calo_scaled/eplus.hdf5','w')\n",
    "gf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/calo_scaled/gamma.hdf5','w')\n",
    "pf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/calo_scaled/piplus.hdf5','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e45292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_scaled = [ef_scaled, gf_scaled, pf_scaled]\n",
    "layer_shapes = {}\n",
    "for key in hdf.keys():\n",
    "    if key == \"energy\" or key == \"overflow\":\n",
    "        pass\n",
    "    else:\n",
    "        layer_shapes[key] = hdf[key].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8712a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer_0': (100000, 3, 96),\n",
       " 'layer_1': (100000, 12, 12),\n",
       " 'layer_2': (100000, 12, 6)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe08bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b1be736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 504)\n",
      "(100000, 288)\n",
      "(100000, 504)\n",
      "(100000, 144)\n",
      "(100000, 504)\n",
      "(100000, 72)\n",
      "(100000, 504)\n",
      "(100000, 288)\n",
      "(100000, 504)\n",
      "(100000, 144)\n",
      "(100000, 504)\n",
      "(100000, 72)\n",
      "(100000, 504)\n",
      "(100000, 288)\n",
      "(100000, 504)\n",
      "(100000, 144)\n",
      "(100000, 504)\n",
      "(100000, 72)\n"
     ]
    }
   ],
   "source": [
    "for hdf, hdf_scaled, scaled_data in zip(hdfs, hdfs_scaled, nplcatscaled):\n",
    "    offset = 0\n",
    "    for key in hdf.keys():\n",
    "        if key == \"energy\" or key == \"overflow\":\n",
    "            hdf_scaled.create_dataset(key, data=hdf[key])\n",
    "        else:\n",
    "            layer_shape = layer_shapes[key]\n",
    "            print(scaled_data.shape)\n",
    "            layer_data = scaled_data[:, offset:offset+(layer_shape[1]*layer_shape[2])]\n",
    "            print(layer_data.shape)\n",
    "            layer_data = layer_data.reshape(layer_shape)\n",
    "            hdf_scaled.create_dataset(key, data=layer_data)\n",
    "            offset += layer_shape[1]*layer_shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "760492ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy (100000, 1) float64\n",
      "layer_0 (100000, 3, 96) float32\n",
      "layer_1 (100000, 12, 12) float32\n",
      "layer_2 (100000, 12, 6) float32\n",
      "overflow (100000, 3) float64\n",
      "energy (100000, 1) float64\n",
      "layer_0 (100000, 3, 96) float32\n",
      "layer_1 (100000, 12, 12) float32\n",
      "layer_2 (100000, 12, 6) float32\n",
      "overflow (100000, 3) float64\n",
      "energy (100000, 1) float64\n",
      "layer_0 (100000, 3, 96) float32\n",
      "layer_1 (100000, 12, 12) float32\n",
      "layer_2 (100000, 12, 6) float32\n",
      "overflow (100000, 3) float64\n"
     ]
    }
   ],
   "source": [
    "for hdf_scaled in hdfs_scaled:\n",
    "    for key in hdf_scaled.keys():\n",
    "        print(key, hdf_scaled[key].shape, hdf_scaled[key].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a49ac389",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hdf_scaled in hdfs_scaled:\n",
    "    hdf_scaled.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab48ec43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 504)\n",
      "(100000, 504)\n",
      "(100000, 504)\n"
     ]
    }
   ],
   "source": [
    "for nplcat in nplcatscaled:\n",
    "    print(nplcat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac47324e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504\n"
     ]
    }
   ],
   "source": [
    "print(len(arrmins[0]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0279838",
   "metadata": {},
   "source": [
    "Inverse transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b2ee3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(nplcatscaled[2] == np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "765c6ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = transformers[i]\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67d8c8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 142 -0.3723908\n",
      "0 143 0.5914335\n",
      "0 144 1.3928509\n",
      "0 353 9.338834\n",
      "0 354 9.172087\n",
      "0 365 -0.5450735\n",
      "0 366 0.036711693\n",
      "1 354 7.1371098\n",
      "1 365 -9.951949\n",
      "1 366 10.38685\n",
      "2 144 -0.120242506\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if np.abs(diff) > 0:\n",
    "            print(i, j, diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "feed2ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = transformers[0].get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "699a9129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'copy': True, 'with_mean': True, 'with_std': True}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48b8957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2422381",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(transformers[0], 'scaler.gz')\n",
    "transformer = joblib.load('scaler.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c2d8996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(transformers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "427e6faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/raid/javier/Datasets/CaloVAE/data/calo_scaled/piplus_scaler.gz']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(transformers[0], '/raid/javier/Datasets/CaloVAE/data/calo_scaled/eplus_scaler.gz')\n",
    "joblib.dump(transformers[1], '/raid/javier/Datasets/CaloVAE/data/calo_scaled/gamma_scaler.gz')\n",
    "joblib.dump(transformers[2], '/raid/javier/Datasets/CaloVAE/data/calo_scaled/piplus_scaler.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18c030a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_transformers = []\n",
    "ld_transformers.append(joblib.load('/raid/javier/Datasets/CaloVAE/data/calo_scaled/eplus_scaler.gz'))\n",
    "ld_transformers.append(joblib.load('/raid/javier/Datasets/CaloVAE/data/calo_scaled/gamma_scaler.gz'))\n",
    "ld_transformers.append(joblib.load('/raid/javier/Datasets/CaloVAE/data/calo_scaled/piplus_scaler.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91ae3560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(nparr == np.inf)\n",
    "# nplcatscaled[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88b467c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.inf)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = ld_transformers[i]\n",
    "    # try:\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    # except:\n",
    "        # print(nparr)\n",
    "    \n",
    "    nparr = np.where(np.isnan(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "954c2cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89771fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 143 0.5914335\n",
      "0 144 1.3928509\n",
      "0 353 9.338834\n",
      "0 354 9.172087\n",
      "0 366 0.036711693\n",
      "1 354 7.1371098\n",
      "1 366 10.38685\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if diff > 1e-4:\n",
    "            print(i, j, diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f642322b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(arrmins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f251f65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(arrmins[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e5dd257",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ptype in enumerate([\"eplus\", \"gamma\", \"piplus\"]):\n",
    "    filepath = \"/raid/javier/Datasets/CaloVAE/data/calo_scaled/\" + ptype + \"_amin.npy\"\n",
    "    with open(filepath, 'wb') as f:\n",
    "        np.save(f, arrmins[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc7ea7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6c25f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(nplcatinv[0][1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4cf75270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATLAS\n",
    "#i: file number\n",
    "file_num = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6ada2646",
   "metadata": {},
   "outputs": [],
   "source": [
    "gf = h5py.File(f'/fast_scratch/QVAE/data/atlas/photons_samples_highStat_En_{file_num}.hdf5', 'r')\n",
    "pf = h5py.File(f'/fast_scratch/QVAE/data/atlas/pions_samples_highStat_En_{file_num}.hdf5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "d1260940",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs = [gf,pf]\n",
    "nplcats = []\n",
    "\n",
    "for hdf in hdfs:\n",
    "    npl0 = np.array(hdf['voxels'])\n",
    "    \n",
    "    npl0 = npl0.reshape(npl0.shape[0], -1)\n",
    "    \n",
    "    nplcats.append(np.concatenate([npl0], axis=1))\n",
    "    \n",
    "nplcatscaled = []\n",
    "transformers = []\n",
    "arrmins = [[], [], []]\n",
    "epsilon = 1e-2\n",
    "\n",
    "for i in range(len(nplcats)):\n",
    "    nparr = nplcats[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    transformer = StandardScaler().fit(nparr)\n",
    "    nparr = transformer.transform(nparr)\n",
    "    transformers.append(transformer)\n",
    "    \n",
    "    nparr = np.where(np.isnan(nparr), np.inf, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        \n",
    "        if arrmin < 0 and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= arrmin\n",
    "            nparr[:, j] += epsilon\n",
    "            arrmins[i].append(arrmin)\n",
    "        else:\n",
    "            arrmins[i].append(0.)\n",
    "            \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        if arrmin < 0:\n",
    "            print(j, arrmin)\n",
    "            \n",
    "    nplcatscaled.append(nparr)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f757796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gf_scaled = h5py.File(f'/fast_scratch/QVAE/data/atlas_scaled/photons_samples_highStat_En_{file_num}.hdf5', 'w')\n",
    "pf_scaled = h5py.File(f'/fast_scratch/QVAE/data/atlas_scaled/pions_samples_highStat_En_{file_num}.hdf5', 'w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "fc3bc797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########\n",
      "(9874, 368)\n",
      "(9874, 368)\n",
      "(9874, 368)\n",
      "###########\n",
      "(9574, 533)\n",
      "(9574, 533)\n",
      "(9574, 533)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hdfs_scaled = [gf_scaled, pf_scaled]\n",
    "# hdfs_scaled = [ef_scaled, gf_scaled, pf_scaled]\n",
    "layer_shapes = {}\n",
    "for hdf in hdfs:\n",
    "    for key in hdf.keys():\n",
    "        if key == \"energy\" or key == \"overflow\" or key == \"energy_from_voxels\":\n",
    "            pass\n",
    "        else:\n",
    "#             layer_shapes[key] = hdf[key].shape\n",
    "            layer_shapes[hdf] = {key : hdf[key].shape}\n",
    "        \n",
    "        \n",
    "\n",
    "for hdf, hdf_scaled, scaled_data in zip(hdfs, hdfs_scaled, nplcatscaled):\n",
    "    offset = 0\n",
    "    for key in hdf.keys():\n",
    "        if key == \"energy\" or key == \"overflow\" or key == \"energy_from_voxels\":\n",
    "            hdf_scaled.create_dataset(key, data=hdf[key])\n",
    "        else:\n",
    "            layer_shape = layer_shapes[hdf][key]\n",
    "            print(\"###########\")\n",
    "            print(layer_shape)\n",
    "            print(scaled_data.shape)\n",
    "#             layer_data = scaled_data[:, offset:offset+(layer_shape[1]*layer_shape[2])]\n",
    "            layer_data = scaled_data[:, offset:offset+layer_shape[1]]\n",
    "            print(layer_data.shape)\n",
    "            layer_data = layer_data.reshape(layer_shape)\n",
    "            hdf_scaled.create_dataset(key, data=layer_data)\n",
    "#             offset += layer_shape[1]*layer_shape[2]\n",
    "            offset += layer_shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "9bb22017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy (9874, 1) float64\n",
      "energy_from_voxels (9874, 1) float64\n",
      "voxels (9874, 368) float64\n",
      "energy (9574, 1) float64\n",
      "energy_from_voxels (9574, 1) float64\n",
      "voxels (9574, 533) float64\n"
     ]
    }
   ],
   "source": [
    "for hdf_scaled in hdfs_scaled:\n",
    "    for key in hdf_scaled.keys():\n",
    "        print(key, hdf_scaled[key].shape, hdf_scaled[key].dtype)\n",
    "        \n",
    "for hdf_scaled in hdfs_scaled:\n",
    "    hdf_scaled.close()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "fb38b949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 87 8.456568778569817e-13\n",
      "0 172 8.29913915367797e-11\n",
      "0 173 5.343281372915953e-12\n",
      "0 180 1.9184653865522705e-11\n",
      "0 181 2.5011104298755527e-12\n",
      "0 182 2.0747847884194925e-12\n",
      "0 183 9.627854069549358e-13\n",
      "0 186 3.3850700020821023e-13\n",
      "0 190 4.160938260611147e-11\n",
      "0 191 1.3949374988442287e-10\n",
      "0 192 2.0236257114447653e-11\n",
      "0 195 5.002220859751105e-12\n",
      "0 201 5.400124791776761e-13\n",
      "0 202 4.554578936222242e-12\n",
      "0 204 1.5152323840084136e-12\n",
      "0 205 7.809308755213351e-13\n",
      "0 209 3.3242031349800527e-10\n",
      "0 210 7.003109203651547e-11\n",
      "0 212 5.911715561524034e-11\n",
      "0 215 7.219114195322618e-12\n",
      "0 219 9.833911462919787e-12\n",
      "0 220 7.958078640513122e-13\n",
      "0 224 2.0121682098306337e-12\n",
      "0 228 6.320988177321851e-11\n",
      "0 230 8.071765478234738e-11\n",
      "0 234 1.3812950783176348e-11\n",
      "0 237 1.0800249583553523e-11\n",
      "0 240 1.2789769243681803e-12\n",
      "0 241 6.963318810448982e-13\n",
      "0 243 4.312106227644108e-13\n",
      "0 246 5.4569682106375694e-12\n",
      "0 248 2.594333636807278e-10\n",
      "0 253 3.0524915928253904e-11\n",
      "0 255 1.3528733688872308e-11\n",
      "0 257 6.480149750132114e-12\n",
      "0 259 8.071765478234738e-12\n",
      "0 262 1.1932677068671182e-12\n",
      "0 265 2.4419932742603123e-10\n",
      "0 267 6.934897101018578e-12\n",
      "0 268 7.753442332614213e-11\n",
      "0 273 7.332801033044234e-12\n",
      "0 274 1.9895196601282805e-12\n",
      "0 286 7.185008144006133e-11\n",
      "0 290 2.3646862246096134e-11\n",
      "0 299 1.7692514120426495e-12\n",
      "0 300 1.2962964035523328e-12\n",
      "0 302 1.0469763944698229e-10\n",
      "0 303 3.8835423765704036e-10\n",
      "0 306 6.332356861094013e-11\n",
      "0 307 1.2937562132719904e-10\n",
      "0 312 9.890754881780595e-12\n",
      "0 314 7.958078640513122e-12\n",
      "0 317 6.004086117172847e-13\n",
      "0 318 1.3997691894473974e-12\n",
      "0 323 2.1304913389030844e-10\n",
      "0 327 3.9335645851679146e-11\n",
      "0 330 2.5011104298755527e-11\n",
      "0 332 2.7853275241795927e-12\n",
      "0 336 1.6200374375330284e-12\n",
      "0 337 7.442935157087049e-13\n",
      "0 338 1.0969003483296547e-13\n",
      "0 343 2.1657342585967854e-10\n",
      "0 345 9.094947017729282e-12\n",
      "0 351 2.5579538487363607e-13\n",
      "0 353 4.277467269275803e-12\n",
      "0 357 4.3154368967179835e-13\n",
      "0 362 5.555556015224283e-13\n",
      "0 363 4.567482392303646e-09\n",
      "0 366 1.0606981959426776e-10\n"
     ]
    }
   ],
   "source": [
    "# Inverse transform\n",
    "\n",
    "\n",
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = transformers[i]\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)\n",
    "\n",
    "\n",
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if diff > 0:\n",
    "            print(i, j, diff)\n",
    "            \n",
    "params = transformers[0].get_params()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "46943b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "gf_scaled = h5py.File(f'/fast_scratch/QVAE/data/atlas_scaled/photons_samples_highStat_En_{file_num}.hdf5', 'w')\n",
    "pf_scaled = h5py.File(f'/fast_scratch/QVAE/data/atlas_scaled/pions_samples_highStat_En_{file_num}.hdf5', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "ed44ee44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/fast_scratch/QVAE/data/atlas_scaled/pions_samples_highStat_En_14_scaler.gz']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# joblib.dump(transformers[0], 'scaler.gz')\n",
    "# transformer = joblib.load('scaler.gz')\n",
    "\n",
    "joblib.dump(transformers[0], f'/fast_scratch/QVAE/data/atlas_scaled/photons_samples_highStat_En_{file_num}_scaler.gz')\n",
    "joblib.dump(transformers[1], f'/fast_scratch/QVAE/data/atlas_scaled/pions_samples_highStat_En_{file_num}_scaler.gz')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "1bee2479",
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_transformers = []\n",
    "ld_transformers.append(joblib.load(f'/fast_scratch/QVAE/data/atlas_scaled/photons_samples_highStat_En_{file_num}_scaler.gz'))\n",
    "ld_transformers.append(joblib.load(f'/fast_scratch/QVAE/data/atlas_scaled/pions_samples_highStat_En_{file_num}_scaler.gz'))\n",
    "\n",
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.inf)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = ld_transformers[i]\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    \n",
    "    nparr = np.where(np.isnan(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "161b50f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if diff > 1e-4:\n",
    "            print(i, j, diff)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "88029e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ptype in enumerate([f'photons_samples_highStat_En_{file_num}', f'pions_samples_highStat_En_{file_num}']):\n",
    "    filepath = f'/fast_scratch/QVAE/data/atlas_scaled/' + ptype + \"_amin.npy\"\n",
    "    with open(filepath, 'wb') as f:\n",
    "        np.save(f, arrmins[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3e6262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATLAS ds 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f66d6ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af35cd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gf = h5py.File(f'/fast_scratch/QVAE/data/atlas_dataset2and3/dataset_2_1.hdf5', 'r')\n",
    "# pf = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas/pions_samples_highStat_En_5.hdf5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75d68814",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hdfs = [gf]\n",
    "nplcats = []\n",
    "\n",
    "for hdf in hdfs:\n",
    "    npl0 = np.array(hdf['showers'])\n",
    "    \n",
    "    npl0 = npl0.reshape(npl0.shape[0], -1)\n",
    "    \n",
    "    nplcats.append(np.concatenate([npl0], axis=1))\n",
    "    \n",
    "nplcatscaled = []\n",
    "transformers = []\n",
    "arrmins = [[], [], []]\n",
    "epsilon = 1e-2\n",
    "\n",
    "for i in range(len(nplcats)):\n",
    "    nparr = nplcats[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    transformer = StandardScaler().fit(nparr)\n",
    "    nparr = transformer.transform(nparr)\n",
    "    transformers.append(transformer)\n",
    "    \n",
    "    nparr = np.where(np.isnan(nparr), np.inf, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        \n",
    "        if arrmin < 0 and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= arrmin\n",
    "            nparr[:, j] += epsilon\n",
    "            arrmins[i].append(arrmin)\n",
    "        else:\n",
    "            arrmins[i].append(0.)\n",
    "            \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        if arrmin < 0:\n",
    "            print(j, arrmin)\n",
    "            \n",
    "    nplcatscaled.append(nparr)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75885c50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas_dataset2and3_scaled/dataset_2_1.hdf5', 'w')\n",
    "# pf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas_scaled/pions_samples_highStat_En_5.hdf5', 'w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35e8466b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########\n",
      "(100000, 6480)\n",
      "(100000, 6480)\n",
      "(100000, 6480)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hdfs_scaled = [gf_scaled]\n",
    "# hdfs_scaled = [ef_scaled, gf_scaled, pf_scaled]\n",
    "layer_shapes = {}\n",
    "for hdf in hdfs:\n",
    "    for key in hdf.keys():\n",
    "        if key == \"energy\" or key == \"overflow\" or key == \"energy_from_voxels\":\n",
    "            pass\n",
    "        else:\n",
    "#             layer_shapes[key] = hdf[key].shape\n",
    "            layer_shapes[hdf] = {key : hdf[key].shape}\n",
    "        \n",
    "        \n",
    "\n",
    "for hdf, hdf_scaled, scaled_data in zip(hdfs, hdfs_scaled, nplcatscaled):\n",
    "    offset = 0\n",
    "    for key in hdf.keys():\n",
    "        if key == \"energy\" or key == \"overflow\" or key == \"energy_from_voxels\" or key == \"incident_energies\":\n",
    "            hdf_scaled.create_dataset(key, data=hdf[key])\n",
    "        else:\n",
    "            layer_shape = layer_shapes[hdf][key]\n",
    "            print(\"###########\")\n",
    "            print(layer_shape)\n",
    "            print(scaled_data.shape)\n",
    "#             layer_data = scaled_data[:, offset:offset+(layer_shape[1]*layer_shape[2])]\n",
    "            layer_data = scaled_data[:, offset:offset+layer_shape[1]]\n",
    "            print(layer_data.shape)\n",
    "            layer_data = layer_data.reshape(layer_shape)\n",
    "            hdf_scaled.create_dataset(key, data=layer_data)\n",
    "#             offset += layer_shape[1]*layer_shape[2]\n",
    "            offset += layer_shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51a4d593",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incident_energies (100000, 1) float64\n",
      "showers (100000, 6480) float64\n"
     ]
    }
   ],
   "source": [
    "for hdf_scaled in hdfs_scaled:\n",
    "    for key in hdf_scaled.keys():\n",
    "        print(key, hdf_scaled[key].shape, hdf_scaled[key].dtype)\n",
    "        \n",
    "for hdf_scaled in hdfs_scaled:\n",
    "    hdf_scaled.close()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17faf39a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inverse transform\n",
    "\n",
    "\n",
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = transformers[i]\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)\n",
    "\n",
    "\n",
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if diff > 0:\n",
    "            print(i, j, diff)\n",
    "            \n",
    "params = transformers[0].get_params()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3822c62f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas_dataset2and3_scaled/dataset_2_1.hdf5', 'w')\n",
    "# pf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas_scaled/pions_samples_highStat_En_5.hdf5', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fe80efb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/raid/javier/Datasets/CaloVAE/data/atlas_dataset2and3_scaled/dataset_2_1_scaler.gz']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# joblib.dump(transformers[0], 'scaler.gz')\n",
    "# transformer = joblib.load('scaler.gz')\n",
    "\n",
    "joblib.dump(transformers[0], '/raid/javier/Datasets/CaloVAE/data/atlas_dataset2and3_scaled/dataset_2_1_scaler.gz')\n",
    "# joblib.dump(transformers[1], '/raid/javier/Datasets/CaloVAE/data/atlas_scaled/pions_samples_highStat_En_5_scaler.gz')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02ec865b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ld_transformers = []\n",
    "ld_transformers.append(joblib.load('/raid/javier/Datasets/CaloVAE/data/atlas_dataset2and3_scaled/dataset_2_1_scaler.gz'))\n",
    "# ld_transformers.append(joblib.load('/raid/javier/Datasets/CaloVAE/data/atlas_scaled/pions_samples_highStat_En_5_scaler.gz'))\n",
    "\n",
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    # nparr = np.where(nparr > 0., nparr, np.inf)\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = ld_transformers[i]\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    \n",
    "    # nparr = np.where(np.isnan(nparr), 0, nparr)\n",
    "    nparr = np.where(np.isnan(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dee9f95d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if diff > 1e-4:\n",
    "            print(i, j, diff)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ab95f31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, ptype in enumerate([\"dataset_2_1\"]):\n",
    "    filepath = \"/raid/javier/Datasets/CaloVAE/data/atlas_dataset2and3_scaled/\" + ptype + \"_amin.npy\"\n",
    "    with open(filepath, 'wb') as f:\n",
    "        np.save(f, arrmins[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ed88f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
