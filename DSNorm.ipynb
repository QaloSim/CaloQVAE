{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7afcfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ac54cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ef[\"layer_0\"].astype('float32')\n",
    "# type(ef[\"layer_0\"].astype('float32')[1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e159dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ef = h5py.File('/fast_scratch/QVAE/data/atlas/pions1.hdf5','r')\n",
    "gf = h5py.File('/fast_scratch/QVAE/data/atlas/photons1.hdf5','r')\n",
    "pf = h5py.File('/fast_scratch/QVAE/data/atlas/photons2.hdf5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "542f7ae6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Unable to open object (object 'layer_0' doesn't exist)\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m nplcats \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hdf \u001b[38;5;129;01min\u001b[39;00m hdfs:\n\u001b[0;32m----> 5\u001b[0m     npl0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mhdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlayer_0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m     npl1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(hdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer_1\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m     npl2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(hdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer_2\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/h5py/_hl/group.py:357\u001b[0m, in \u001b[0;36mGroup.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid HDF5 object reference\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m)):\n\u001b[0;32m--> 357\u001b[0m     oid \u001b[38;5;241m=\u001b[39m \u001b[43mh5o\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessing a group is done with bytes or str, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(name)))\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5o.pyx:190\u001b[0m, in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Unable to open object (object 'layer_0' doesn't exist)\""
     ]
    }
   ],
   "source": [
    "hdfs = [ef, gf, pf]\n",
    "nplcats = []\n",
    "\n",
    "for hdf in hdfs:\n",
    "    npl0 = np.array(hdf['layer_0']).astype('float32')\n",
    "    npl1 = np.array(hdf['layer_1']).astype('float32')\n",
    "    npl2 = np.array(hdf['layer_2']).astype('float32')\n",
    "    \n",
    "    npl0 = npl0.reshape(npl0.shape[0], -1)\n",
    "    npl1 = npl1.reshape(npl1.shape[0], -1)\n",
    "    npl2 = npl2.reshape(npl2.shape[0], -1)\n",
    "    \n",
    "    nplcats.append(np.concatenate([npl0, npl1, npl2], axis=1))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "16c19c37",
   "metadata": {},
   "source": [
    "# Fit and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aba5c967",
   "metadata": {},
   "outputs": [],
   "source": [
    "nplcatscaled = []\n",
    "transformers = []\n",
    "arrmins = [[], [], []]\n",
    "epsilon = 1e-2\n",
    "\n",
    "for i in range(len(nplcats)):\n",
    "    nparr = nplcats[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    transformer = StandardScaler().fit(nparr)\n",
    "    nparr = transformer.transform(nparr)\n",
    "    transformers.append(transformer)\n",
    "    \n",
    "    nparr = np.where(np.isnan(nparr), np.inf, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        \n",
    "        if arrmin < 0 and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= arrmin\n",
    "            nparr[:, j] += epsilon\n",
    "            arrmins[i].append(arrmin)\n",
    "        else:\n",
    "            arrmins[i].append(0.)\n",
    "            \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        if arrmin < 0:\n",
    "            print(j, arrmin)\n",
    "            \n",
    "    nplcatscaled.append(nparr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b13e0e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 504)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nplcats[2].shape\n",
    "nplcatscaled[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae06b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ef_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/calo_scaled/eplus.hdf5','w')\n",
    "gf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/calo_scaled/gamma.hdf5','w')\n",
    "pf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/calo_scaled/piplus.hdf5','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e1ba0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_scaled = [ef_scaled, gf_scaled, pf_scaled]\n",
    "layer_shapes = {}\n",
    "for key in hdf.keys():\n",
    "    if key == \"energy\" or key == \"overflow\":\n",
    "        pass\n",
    "    else:\n",
    "        layer_shapes[key] = hdf[key].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e456f4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer_0': (100000, 3, 96),\n",
       " 'layer_1': (100000, 12, 12),\n",
       " 'layer_2': (100000, 12, 6)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdcbf38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbca745b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 504)\n",
      "(100000, 288)\n",
      "(100000, 504)\n",
      "(100000, 144)\n",
      "(100000, 504)\n",
      "(100000, 72)\n",
      "(100000, 504)\n",
      "(100000, 288)\n",
      "(100000, 504)\n",
      "(100000, 144)\n",
      "(100000, 504)\n",
      "(100000, 72)\n",
      "(100000, 504)\n",
      "(100000, 288)\n",
      "(100000, 504)\n",
      "(100000, 144)\n",
      "(100000, 504)\n",
      "(100000, 72)\n"
     ]
    }
   ],
   "source": [
    "for hdf, hdf_scaled, scaled_data in zip(hdfs, hdfs_scaled, nplcatscaled):\n",
    "    offset = 0\n",
    "    for key in hdf.keys():\n",
    "        if key == \"energy\" or key == \"overflow\":\n",
    "            hdf_scaled.create_dataset(key, data=hdf[key])\n",
    "        else:\n",
    "            layer_shape = layer_shapes[key]\n",
    "            print(scaled_data.shape)\n",
    "            layer_data = scaled_data[:, offset:offset+(layer_shape[1]*layer_shape[2])]\n",
    "            print(layer_data.shape)\n",
    "            layer_data = layer_data.reshape(layer_shape)\n",
    "            hdf_scaled.create_dataset(key, data=layer_data)\n",
    "            offset += layer_shape[1]*layer_shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35b028fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy (100000, 1) float64\n",
      "layer_0 (100000, 3, 96) float32\n",
      "layer_1 (100000, 12, 12) float32\n",
      "layer_2 (100000, 12, 6) float32\n",
      "overflow (100000, 3) float64\n",
      "energy (100000, 1) float64\n",
      "layer_0 (100000, 3, 96) float32\n",
      "layer_1 (100000, 12, 12) float32\n",
      "layer_2 (100000, 12, 6) float32\n",
      "overflow (100000, 3) float64\n",
      "energy (100000, 1) float64\n",
      "layer_0 (100000, 3, 96) float32\n",
      "layer_1 (100000, 12, 12) float32\n",
      "layer_2 (100000, 12, 6) float32\n",
      "overflow (100000, 3) float64\n"
     ]
    }
   ],
   "source": [
    "for hdf_scaled in hdfs_scaled:\n",
    "    for key in hdf_scaled.keys():\n",
    "        print(key, hdf_scaled[key].shape, hdf_scaled[key].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f0a471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hdf_scaled in hdfs_scaled:\n",
    "    hdf_scaled.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dbd8534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 504)\n",
      "(100000, 504)\n",
      "(100000, 504)\n"
     ]
    }
   ],
   "source": [
    "for nplcat in nplcatscaled:\n",
    "    print(nplcat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c67f309a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504\n"
     ]
    }
   ],
   "source": [
    "print(len(arrmins[0]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b906dbad",
   "metadata": {},
   "source": [
    "Inverse transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66d6c48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(nplcatscaled[2] == np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53a93b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = transformers[i]\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "903802e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 142 -0.3723908\n",
      "0 143 0.5914335\n",
      "0 144 1.3928509\n",
      "0 353 9.338834\n",
      "0 354 9.172087\n",
      "0 365 -0.5450735\n",
      "0 366 0.036711693\n",
      "1 354 7.1371098\n",
      "1 365 -9.951949\n",
      "1 366 10.38685\n",
      "2 144 -0.120242506\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if np.abs(diff) > 0:\n",
    "            print(i, j, diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eba53829",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = transformers[0].get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab81d605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'copy': True, 'with_mean': True, 'with_std': True}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4854117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41205bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(transformers[0], 'scaler.gz')\n",
    "transformer = joblib.load('scaler.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52147c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(transformers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d0ff33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/raid/javier/Datasets/CaloVAE/data/calo_scaled/piplus_scaler.gz']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(transformers[0], '/raid/javier/Datasets/CaloVAE/data/calo_scaled/eplus_scaler.gz')\n",
    "joblib.dump(transformers[1], '/raid/javier/Datasets/CaloVAE/data/calo_scaled/gamma_scaler.gz')\n",
    "joblib.dump(transformers[2], '/raid/javier/Datasets/CaloVAE/data/calo_scaled/piplus_scaler.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8998c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_transformers = []\n",
    "ld_transformers.append(joblib.load('/raid/javier/Datasets/CaloVAE/data/calo_scaled/eplus_scaler.gz'))\n",
    "ld_transformers.append(joblib.load('/raid/javier/Datasets/CaloVAE/data/calo_scaled/gamma_scaler.gz'))\n",
    "ld_transformers.append(joblib.load('/raid/javier/Datasets/CaloVAE/data/calo_scaled/piplus_scaler.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d4d03e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(nparr == np.inf)\n",
    "# nplcatscaled[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad04c30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.inf)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = ld_transformers[i]\n",
    "    # try:\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    # except:\n",
    "        # print(nparr)\n",
    "    \n",
    "    nparr = np.where(np.isnan(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96160571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2d6ba83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 143 0.5914335\n",
      "0 144 1.3928509\n",
      "0 353 9.338834\n",
      "0 354 9.172087\n",
      "0 366 0.036711693\n",
      "1 354 7.1371098\n",
      "1 366 10.38685\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if diff > 1e-4:\n",
    "            print(i, j, diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca933b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(arrmins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "153bb37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(arrmins[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9923803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ptype in enumerate([\"eplus\", \"gamma\", \"piplus\"]):\n",
    "    filepath = \"/raid/javier/Datasets/CaloVAE/data/calo_scaled/\" + ptype + \"_amin.npy\"\n",
    "    with open(filepath, 'wb') as f:\n",
    "        np.save(f, arrmins[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819c1e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e75f561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(nplcatinv[0][1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eee9b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATLAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c846fcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gf = h5py.File('/fast_scratch/QVAE/data/atlas_dataset2and3/dataset_2_1.hdf5', 'r')\n",
    "pf = h5py.File('/fast_scratch/QVAE/data/atlas_dataset2and3/dataset_3_1.hdf5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35ba0cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 6480)\n",
      "(50000, 40500)\n",
      "<KeysViewHDF5 ['incident_energies', 'showers']>\n"
     ]
    }
   ],
   "source": [
    "print(gf['showers'].shape)\n",
    "print(pf['showers'].shape)\n",
    "print(gf.keys())\n",
    "#gf = torch.tensor(gf[\"showers\"][:])\n",
    "#print(gf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dc5ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs = [gf,pf]\n",
    "nplcats = []\n",
    "nplcats_energies = []\n",
    "\n",
    "for hdf in hdfs:\n",
    "    npl0 = np.array(hdf['showers'])\n",
    "    npl1 = np.array(hdf['incident_energies'])\n",
    "    \n",
    "    npl0 = npl0.reshape(npl0.shape[0], -1)\n",
    "    npl1 = npl1.reshape(npl1.shape[0], -1)\n",
    "    \n",
    "    nplcats.append(np.concatenate([npl0], axis=1))\n",
    "    nplcats_energies.append(np.concatenate([npl1], axis=1))\n",
    "    \n",
    "nplcatscaled = []\n",
    "transformers = []\n",
    "arrmins = [[], [], []]\n",
    "epsilon = 1e-2\n",
    "\n",
    "for i in range(len(nplcats)):\n",
    "    nparr = nplcats[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    transformer = StandardScaler().fit(nparr)\n",
    "    nparr = transformer.transform(nparr)\n",
    "    transformers.append(transformer)\n",
    "    \n",
    "    nparr = np.where(np.isnan(nparr), np.inf, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        \n",
    "        if arrmin < 0 and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= arrmin\n",
    "            nparr[:, j] += epsilon\n",
    "            arrmins[i].append(arrmin)\n",
    "        else:\n",
    "            arrmins[i].append(0.)\n",
    "            \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        if arrmin < 0:\n",
    "            print(j, arrmin)\n",
    "            \n",
    "    nplcatscaled.append(nparr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76f7c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_scaled = [gf_scaled, pf_scaled]\n",
    "# hdfs_scaled = [ef_scaled, gf_scaled, pf_scaled]\n",
    "layer_shapes = {}\n",
    "for hdf in hdfs:\n",
    "    for key in hdf.keys():\n",
    "        if key == \"energy\" or key == \"overflow\" or key == \"energy_from_voxels\":\n",
    "            pass\n",
    "        else:\n",
    "   #         layer_shapes[key] = hdf[key].shape\n",
    "             layer_shapes[hdf] = {key : hdf[key].shape}\n",
    "\n",
    "for hdf, hdf_scaled, scaled_data in zip(hdfs, hdfs_scaled, nplcatscaled):\n",
    "    offset = 0\n",
    "    for key in hdf.keys():\n",
    "        if key == \"energy\" or key == \"overflow\" or key == \"energy_from_voxels\":\n",
    "            hdf_scaled.create_dataset(key, data=hdf[key])\n",
    "        else:\n",
    "            layer_shape = layer_shapes[hdf][key]\n",
    "            print(\"###########\")\n",
    "            print(layer_shape)\n",
    "            print(scaled_data.shape)\n",
    "#             layer_data = scaled_data[:, offset:offset+(layer_shape[1]*layer_shape[2])]\n",
    "            layer_data = scaled_data[:, offset:offset+layer_shape[1]]\n",
    "            print(layer_data.shape)\n",
    "            layer_data = layer_data.reshape(layer_shape)\n",
    "            hdf_scaled.create_dataset(key, data=layer_data)\n",
    "#             offset += layer_shape[1]*layer_shape[2]\n",
    "            offset += layer_shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d1d563",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hdf_scaled in hdfs_scaled:\n",
    "    for key in hdf_scaled.keys():\n",
    "        print(key, hdf_scaled[key].shape, hdf_scaled[key].dtype)\n",
    "        \n",
    "for hdf_scaled in hdfs_scaled:\n",
    "    hdf_scaled.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f784249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform\n",
    "\n",
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = transformers[i]\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)\n",
    "\n",
    "\n",
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if diff > 0:\n",
    "            print(i, j, diff)\n",
    "            \n",
    "params = transformers[0].get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0c557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gf_scaled = h5py.File('/fast_scratch/QVAE/data/atlas_scaled/photons1_all_E.hdf5', 'w')\n",
    "#pf_scaled = h5py.File('/fast_scratch/QVAE/data/atlas_scaled/pions1_all_E.hdf5', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1fc39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# joblib.dump(transformers[0], 'scaler.gz')\n",
    "# transformer = joblib.load('scaler.gz')\n",
    "\n",
    "joblib.dump(transformers[0], '/fast_scratch/QVAE/data/atlas_dataset2and3/dataset_2_1_scaler.gz')\n",
    "joblib.dump(transformers[1], '/fast_scratch/QVAE/data/atlas_dataset2and3/dataset_3_1_scaler.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116c4e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_transformers = []\n",
    "ld_transformers.append(joblib.load('/fast_scratch/QVAE/data/atlas_dataset2and3/dataset_2_1_scaler.gz'))\n",
    "ld_transformers.append(joblib.load('/fast_scratch/QVAE/data/atlas_dataset2and3/dataset_3_1_scaler.gz'))\n",
    "\n",
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.inf)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = ld_transformers[i]\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    \n",
    "    nparr = np.where(np.isnan(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbe0c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if diff > 1e-4:\n",
    "            print(i, j, diff)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f922ab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ptype in enumerate([\"dataset_2_1\", \"dataset_3_1\"]):\n",
    "    filepath = \"/fast_scratch/QVAE/data/atlas_dataset2and3/\" + ptype + \"_amin.npy\"\n",
    "    with open(filepath, 'wb') as f:\n",
    "        np.save(f, arrmins[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aa16a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
