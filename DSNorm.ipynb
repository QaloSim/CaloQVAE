{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c40add99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4160b743",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14fca42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ef[\"layer_0\"].astype('float32')\n",
    "# type(ef[\"layer_0\"].astype('float32')[1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48079e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "ef = h5py.File('/raid/javier/Datasets/CaloVAE/data/calo/eplus.hdf5','r')\n",
    "gf = h5py.File('/raid/javier/Datasets/CaloVAE/data/calo/gamma.hdf5','r')\n",
    "pf = h5py.File('/raid/javier/Datasets/CaloVAE/data/calo/piplus.hdf5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb237c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs = [ef, gf, pf]\n",
    "nplcats = []\n",
    "\n",
    "for hdf in hdfs:\n",
    "    npl0 = np.array(hdf['layer_0']).astype('float32')\n",
    "    npl1 = np.array(hdf['layer_1']).astype('float32')\n",
    "    npl2 = np.array(hdf['layer_2']).astype('float32')\n",
    "    \n",
    "    npl0 = npl0.reshape(npl0.shape[0], -1)\n",
    "    npl1 = npl1.reshape(npl1.shape[0], -1)\n",
    "    npl2 = npl2.reshape(npl2.shape[0], -1)\n",
    "    \n",
    "    nplcats.append(np.concatenate([npl0, npl1, npl2], axis=1))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53529e1e",
   "metadata": {},
   "source": [
    "# Fit and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "751c2f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "nplcatscaled = []\n",
    "transformers = []\n",
    "arrmins = [[], [], []]\n",
    "epsilon = 1e-2\n",
    "\n",
    "for i in range(len(nplcats)):\n",
    "    nparr = nplcats[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    transformer = StandardScaler().fit(nparr)\n",
    "    nparr = transformer.transform(nparr)\n",
    "    transformers.append(transformer)\n",
    "    \n",
    "    nparr = np.where(np.isnan(nparr), np.inf, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        \n",
    "        if arrmin < 0 and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= arrmin\n",
    "            nparr[:, j] += epsilon\n",
    "            arrmins[i].append(arrmin)\n",
    "        else:\n",
    "            arrmins[i].append(0.)\n",
    "            \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        if arrmin < 0:\n",
    "            print(j, arrmin)\n",
    "            \n",
    "    nplcatscaled.append(nparr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14120568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 504)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nplcats[2].shape\n",
    "nplcatscaled[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4d4caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ef_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/calo_scaled/eplus.hdf5','w')\n",
    "gf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/calo_scaled/gamma.hdf5','w')\n",
    "pf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/calo_scaled/piplus.hdf5','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67e57f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_scaled = [ef_scaled, gf_scaled, pf_scaled]\n",
    "layer_shapes = {}\n",
    "for key in hdf.keys():\n",
    "    if key == \"energy\" or key == \"overflow\":\n",
    "        pass\n",
    "    else:\n",
    "        layer_shapes[key] = hdf[key].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15abed40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer_0': (100000, 3, 96),\n",
       " 'layer_1': (100000, 12, 12),\n",
       " 'layer_2': (100000, 12, 6)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2af13f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c68e354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 504)\n",
      "(100000, 288)\n",
      "(100000, 504)\n",
      "(100000, 144)\n",
      "(100000, 504)\n",
      "(100000, 72)\n",
      "(100000, 504)\n",
      "(100000, 288)\n",
      "(100000, 504)\n",
      "(100000, 144)\n",
      "(100000, 504)\n",
      "(100000, 72)\n",
      "(100000, 504)\n",
      "(100000, 288)\n",
      "(100000, 504)\n",
      "(100000, 144)\n",
      "(100000, 504)\n",
      "(100000, 72)\n"
     ]
    }
   ],
   "source": [
    "for hdf, hdf_scaled, scaled_data in zip(hdfs, hdfs_scaled, nplcatscaled):\n",
    "    offset = 0\n",
    "    for key in hdf.keys():\n",
    "        if key == \"energy\" or key == \"overflow\":\n",
    "            hdf_scaled.create_dataset(key, data=hdf[key])\n",
    "        else:\n",
    "            layer_shape = layer_shapes[key]\n",
    "            print(scaled_data.shape)\n",
    "            layer_data = scaled_data[:, offset:offset+(layer_shape[1]*layer_shape[2])]\n",
    "            print(layer_data.shape)\n",
    "            layer_data = layer_data.reshape(layer_shape)\n",
    "            hdf_scaled.create_dataset(key, data=layer_data)\n",
    "            offset += layer_shape[1]*layer_shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee0d2856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy (100000, 1) float64\n",
      "layer_0 (100000, 3, 96) float32\n",
      "layer_1 (100000, 12, 12) float32\n",
      "layer_2 (100000, 12, 6) float32\n",
      "overflow (100000, 3) float64\n",
      "energy (100000, 1) float64\n",
      "layer_0 (100000, 3, 96) float32\n",
      "layer_1 (100000, 12, 12) float32\n",
      "layer_2 (100000, 12, 6) float32\n",
      "overflow (100000, 3) float64\n",
      "energy (100000, 1) float64\n",
      "layer_0 (100000, 3, 96) float32\n",
      "layer_1 (100000, 12, 12) float32\n",
      "layer_2 (100000, 12, 6) float32\n",
      "overflow (100000, 3) float64\n"
     ]
    }
   ],
   "source": [
    "for hdf_scaled in hdfs_scaled:\n",
    "    for key in hdf_scaled.keys():\n",
    "        print(key, hdf_scaled[key].shape, hdf_scaled[key].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cb66aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hdf_scaled in hdfs_scaled:\n",
    "    hdf_scaled.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5e47c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 504)\n",
      "(100000, 504)\n",
      "(100000, 504)\n"
     ]
    }
   ],
   "source": [
    "for nplcat in nplcatscaled:\n",
    "    print(nplcat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff3abd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504\n"
     ]
    }
   ],
   "source": [
    "print(len(arrmins[0]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6dc33696",
   "metadata": {},
   "source": [
    "Inverse transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34448639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(nplcatscaled[2] == np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d0d5ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = transformers[i]\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3eceda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 142 -0.3723908\n",
      "0 143 0.5914335\n",
      "0 144 1.3928509\n",
      "0 353 9.338834\n",
      "0 354 9.172087\n",
      "0 365 -0.5450735\n",
      "0 366 0.036711693\n",
      "1 354 7.1371098\n",
      "1 365 -9.951949\n",
      "1 366 10.38685\n",
      "2 144 -0.120242506\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if np.abs(diff) > 0:\n",
    "            print(i, j, diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c583a8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = transformers[0].get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0f71c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'copy': True, 'with_mean': True, 'with_std': True}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8858efb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e03d790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(transformers[0], 'scaler.gz')\n",
    "transformer = joblib.load('scaler.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ce81084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(transformers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09c33ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/raid/javier/Datasets/CaloVAE/data/calo_scaled/piplus_scaler.gz']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(transformers[0], '/raid/javier/Datasets/CaloVAE/data/calo_scaled/eplus_scaler.gz')\n",
    "joblib.dump(transformers[1], '/raid/javier/Datasets/CaloVAE/data/calo_scaled/gamma_scaler.gz')\n",
    "joblib.dump(transformers[2], '/raid/javier/Datasets/CaloVAE/data/calo_scaled/piplus_scaler.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46721968",
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_transformers = []\n",
    "ld_transformers.append(joblib.load('/raid/javier/Datasets/CaloVAE/data/calo_scaled/eplus_scaler.gz'))\n",
    "ld_transformers.append(joblib.load('/raid/javier/Datasets/CaloVAE/data/calo_scaled/gamma_scaler.gz'))\n",
    "ld_transformers.append(joblib.load('/raid/javier/Datasets/CaloVAE/data/calo_scaled/piplus_scaler.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea18c014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(nparr == np.inf)\n",
    "# nplcatscaled[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7696d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.inf)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = ld_transformers[i]\n",
    "    # try:\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    # except:\n",
    "        # print(nparr)\n",
    "    \n",
    "    nparr = np.where(np.isnan(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a18e0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e704f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 143 0.5914335\n",
      "0 144 1.3928509\n",
      "0 353 9.338834\n",
      "0 354 9.172087\n",
      "0 366 0.036711693\n",
      "1 354 7.1371098\n",
      "1 366 10.38685\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if diff > 1e-4:\n",
    "            print(i, j, diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da10a950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(arrmins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8051c705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(arrmins[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3eb00199",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ptype in enumerate([\"eplus\", \"gamma\", \"piplus\"]):\n",
    "    filepath = \"/raid/javier/Datasets/CaloVAE/data/calo_scaled/\" + ptype + \"_amin.npy\"\n",
    "    with open(filepath, 'wb') as f:\n",
    "        np.save(f, arrmins[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fafae13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fcdbf111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(nplcatinv[0][1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26d20d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATLAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bcc799c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gf = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas/photons_samples_highStat_En_5.hdf5', 'r')\n",
    "pf = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas/pions_samples_highStat_En_5.hdf5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cbf43605",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs = [gf,pf]\n",
    "nplcats = []\n",
    "\n",
    "for hdf in hdfs:\n",
    "    npl0 = np.array(hdf['voxels'])\n",
    "    \n",
    "    npl0 = npl0.reshape(npl0.shape[0], -1)\n",
    "    \n",
    "    nplcats.append(np.concatenate([npl0], axis=1))\n",
    "    \n",
    "nplcatscaled = []\n",
    "transformers = []\n",
    "arrmins = [[], [], []]\n",
    "epsilon = 1e-2\n",
    "\n",
    "for i in range(len(nplcats)):\n",
    "    nparr = nplcats[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    transformer = StandardScaler().fit(nparr)\n",
    "    nparr = transformer.transform(nparr)\n",
    "    transformers.append(transformer)\n",
    "    \n",
    "    nparr = np.where(np.isnan(nparr), np.inf, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        \n",
    "        if arrmin < 0 and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= arrmin\n",
    "            nparr[:, j] += epsilon\n",
    "            arrmins[i].append(arrmin)\n",
    "        else:\n",
    "            arrmins[i].append(0.)\n",
    "            \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        if arrmin < 0:\n",
    "            print(j, arrmin)\n",
    "            \n",
    "    nplcatscaled.append(nparr)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79d52fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas_scaled/photons_samples_highStat_En_5.hdf5', 'w')\n",
    "pf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas_scaled/pions_samples_highStat_En_5.hdf5', 'w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9894250c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########\n",
      "(99933, 368)\n",
      "(99933, 368)\n",
      "(99933, 368)\n",
      "###########\n",
      "(99999, 533)\n",
      "(99999, 533)\n",
      "(99999, 533)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hdfs_scaled = [gf_scaled, pf_scaled]\n",
    "# hdfs_scaled = [ef_scaled, gf_scaled, pf_scaled]\n",
    "layer_shapes = {}\n",
    "for hdf in hdfs:\n",
    "    for key in hdf.keys():\n",
    "        if key == \"energy\" or key == \"overflow\" or key == \"energy_from_voxels\":\n",
    "            pass\n",
    "        else:\n",
    "#             layer_shapes[key] = hdf[key].shape\n",
    "            layer_shapes[hdf] = {key : hdf[key].shape}\n",
    "        \n",
    "        \n",
    "\n",
    "for hdf, hdf_scaled, scaled_data in zip(hdfs, hdfs_scaled, nplcatscaled):\n",
    "    offset = 0\n",
    "    for key in hdf.keys():\n",
    "        if key == \"energy\" or key == \"overflow\" or key == \"energy_from_voxels\":\n",
    "            hdf_scaled.create_dataset(key, data=hdf[key])\n",
    "        else:\n",
    "            layer_shape = layer_shapes[hdf][key]\n",
    "            print(\"###########\")\n",
    "            print(layer_shape)\n",
    "            print(scaled_data.shape)\n",
    "#             layer_data = scaled_data[:, offset:offset+(layer_shape[1]*layer_shape[2])]\n",
    "            layer_data = scaled_data[:, offset:offset+layer_shape[1]]\n",
    "            print(layer_data.shape)\n",
    "            layer_data = layer_data.reshape(layer_shape)\n",
    "            hdf_scaled.create_dataset(key, data=layer_data)\n",
    "#             offset += layer_shape[1]*layer_shape[2]\n",
    "            offset += layer_shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31e7fd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy (99933, 1) float64\n",
      "energy_from_voxels (99933, 1) float64\n",
      "voxels (99933, 368) float64\n",
      "energy (99999, 1) float64\n",
      "energy_from_voxels (99999, 1) float64\n",
      "voxels (99999, 533) float64\n"
     ]
    }
   ],
   "source": [
    "for hdf_scaled in hdfs_scaled:\n",
    "    for key in hdf_scaled.keys():\n",
    "        print(key, hdf_scaled[key].shape, hdf_scaled[key].dtype)\n",
    "        \n",
    "for hdf_scaled in hdfs_scaled:\n",
    "    hdf_scaled.close()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1ed3828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform\n",
    "\n",
    "\n",
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = transformers[i]\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)\n",
    "\n",
    "\n",
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if diff > 0:\n",
    "            print(i, j, diff)\n",
    "            \n",
    "params = transformers[0].get_params()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e57027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas_scaled/photons_samples_highStat_En_5.hdf5', 'w')\n",
    "pf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas_scaled/pions_samples_highStat_En_5.hdf5', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8390f23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/raid/javier/Datasets/CaloVAE/data/atlas_scaled/pions_samples_highStat_En_5_scaler.gz']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# joblib.dump(transformers[0], 'scaler.gz')\n",
    "# transformer = joblib.load('scaler.gz')\n",
    "\n",
    "joblib.dump(transformers[0], '/raid/javier/Datasets/CaloVAE/data/atlas_scaled/photons_samples_highStat_En_5_scaler.gz')\n",
    "joblib.dump(transformers[1], '/raid/javier/Datasets/CaloVAE/data/atlas_scaled/pions_samples_highStat_En_5_scaler.gz')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b15ed41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_transformers = []\n",
    "ld_transformers.append(joblib.load('/raid/javier/Datasets/CaloVAE/data/atlas_scaled/photons_samples_highStat_En_5_scaler.gz'))\n",
    "ld_transformers.append(joblib.load('/raid/javier/Datasets/CaloVAE/data/atlas_scaled/pions_samples_highStat_En_5_scaler.gz'))\n",
    "\n",
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.inf)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = ld_transformers[i]\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    \n",
    "    nparr = np.where(np.isnan(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "807fa849",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if diff > 1e-4:\n",
    "            print(i, j, diff)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45353d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ptype in enumerate([\"photons_samples_highStat_En_5\", \"pions_samples_highStat_En_5\"]):\n",
    "    filepath = \"/raid/javier/Datasets/CaloVAE/data/atlas_scaled/\" + ptype + \"_amin.npy\"\n",
    "    with open(filepath, 'wb') as f:\n",
    "        np.save(f, arrmins[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb489e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATLAS ds 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aab46d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74f8e5bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gf = h5py.File(\"/fast_scratch/QVAE/data/atlas_dataset2and3_uniform/dataset_2_1.hdf5\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1a6b227",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hdfs = [gf]\n",
    "nplcats = []\n",
    "\n",
    "for hdf in hdfs:\n",
    "    npl0 = np.array(hdf['showers'])\n",
    "    \n",
    "    npl0 = npl0.reshape(npl0.shape[0], -1)\n",
    "    \n",
    "    nplcats.append(np.concatenate([npl0], axis=1))\n",
    "    \n",
    "nplcatscaled = []\n",
    "transformers = []\n",
    "arrmins = [[], [], []]\n",
    "epsilon = 1e-2\n",
    "\n",
    "for i in range(len(nplcats)):\n",
    "    nparr = nplcats[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    transformer = StandardScaler().fit(nparr)\n",
    "    nparr = transformer.transform(nparr)\n",
    "    transformers.append(transformer)\n",
    "    \n",
    "    nparr = np.where(np.isnan(nparr), np.inf, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        \n",
    "        if arrmin < 0 and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= arrmin\n",
    "            nparr[:, j] += epsilon\n",
    "            arrmins[i].append(arrmin)\n",
    "        else:\n",
    "            arrmins[i].append(0.)\n",
    "            \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        if arrmin < 0:\n",
    "            print(j, arrmin)\n",
    "            \n",
    "    nplcatscaled.append(nparr)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2106de7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gf_scaled = h5py.File(\"/fast_scratch/QVAE/data/atlas_dataset2and3_scaled/uniform_dataset_2_1.hdf5\", 'w')\n",
    "# pf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas_scaled/pions_samples_highStat_En_5.hdf5', 'w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a6767e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########\n",
      "(15924, 6480)\n",
      "(15924, 6480)\n",
      "(15924, 6480)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hdfs_scaled = [gf_scaled]\n",
    "# hdfs_scaled = [ef_scaled, gf_scaled, pf_scaled]\n",
    "layer_shapes = {}\n",
    "for hdf in hdfs:\n",
    "    for key in hdf.keys():\n",
    "        if key == \"energy\" or key == \"overflow\" or key == \"energy_from_voxels\":\n",
    "            pass\n",
    "        else:\n",
    "#             layer_shapes[key] = hdf[key].shape\n",
    "            layer_shapes[hdf] = {key : hdf[key].shape}\n",
    "        \n",
    "        \n",
    "\n",
    "for hdf, hdf_scaled, scaled_data in zip(hdfs, hdfs_scaled, nplcatscaled):\n",
    "    offset = 0\n",
    "    for key in hdf.keys():\n",
    "        if key == \"energy\" or key == \"overflow\" or key == \"energy_from_voxels\" or key == \"incident_energies\":\n",
    "            hdf_scaled.create_dataset(key, data=hdf[key])\n",
    "        else:\n",
    "            layer_shape = layer_shapes[hdf][key]\n",
    "            print(\"###########\")\n",
    "            print(layer_shape)\n",
    "            print(scaled_data.shape)\n",
    "#             layer_data = scaled_data[:, offset:offset+(layer_shape[1]*layer_shape[2])]\n",
    "            layer_data = scaled_data[:, offset:offset+layer_shape[1]]\n",
    "            print(layer_data.shape)\n",
    "            layer_data = layer_data.reshape(layer_shape)\n",
    "            hdf_scaled.create_dataset(key, data=layer_data)\n",
    "#             offset += layer_shape[1]*layer_shape[2]\n",
    "            offset += layer_shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1120dd52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incident_energies (15924, 1) float64\n",
      "showers (15924, 6480) float64\n"
     ]
    }
   ],
   "source": [
    "for hdf_scaled in hdfs_scaled:\n",
    "    for key in hdf_scaled.keys():\n",
    "        print(key, hdf_scaled[key].shape, hdf_scaled[key].dtype)\n",
    "        \n",
    "for hdf_scaled in hdfs_scaled:\n",
    "    hdf_scaled.close()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0155a7d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inverse transform\n",
    "\n",
    "\n",
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = transformers[i]\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)\n",
    "\n",
    "\n",
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if diff > 0:\n",
    "            print(i, j, diff)\n",
    "            \n",
    "params = transformers[0].get_params()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0848142",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas_dataset2and3_scaled/dataset_2_1.hdf5', 'w')\n",
    "# pf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas_scaled/pions_samples_highStat_En_5.hdf5', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcd1f1c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/fast_scratch/QVAE/data/atlas_dataset2and3_scaled/uniform_dataset_2_1_scaler.gz']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# joblib.dump(transformers[0], 'scaler.gz')\n",
    "# transformer = joblib.load('scaler.gz')\n",
    "\n",
    "joblib.dump(transformers[0], '/fast_scratch/QVAE/data/atlas_dataset2and3_scaled/uniform_dataset_2_1_scaler.gz')\n",
    "# joblib.dump(transformers[1], '/raid/javier/Datasets/CaloVAE/data/atlas_scaled/pions_samples_highStat_En_5_scaler.gz')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b936dc5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ld_transformers = []\n",
    "ld_transformers.append(joblib.load('/fast_scratch/QVAE/data/atlas_dataset2and3_scaled/uniform_dataset_2_1_scaler.gz'))\n",
    "# ld_transformers.append(joblib.load('/raid/javier/Datasets/CaloVAE/data/atlas_scaled/pions_samples_highStat_En_5_scaler.gz'))\n",
    "\n",
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    # nparr = np.where(nparr > 0., nparr, np.inf)\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = ld_transformers[i]\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    \n",
    "    # nparr = np.where(np.isnan(nparr), 0, nparr)\n",
    "    nparr = np.where(np.isnan(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b209ead0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if diff > 1e-4:\n",
    "            print(i, j, diff)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7030102e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, ptype in enumerate([\"dataset_2_1\"]):\n",
    "    filepath = \"/fast_scratch/QVAE/data/atlas_dataset2and3_scaled/uniform_\" + ptype + \"_amin.npy\"\n",
    "    with open(filepath, 'wb') as f:\n",
    "        np.save(f, arrmins[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cdfa49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
