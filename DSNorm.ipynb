{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "cb85988e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "86a0b931",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e20cad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ef[\"layer_0\"].astype('float32')\n",
    "# type(ef[\"layer_0\"].astype('float32')[1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f66ea5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ef = h5py.File('/raid/javier/Datasets/CaloVAE/data/calo/eplus.hdf5','r')\n",
    "gf = h5py.File('/raid/javier/Datasets/CaloVAE/data/calo/gamma.hdf5','r')\n",
    "pf = h5py.File('/raid/javier/Datasets/CaloVAE/data/calo/piplus.hdf5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17f3f412",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs = [ef, gf, pf]\n",
    "nplcats = []\n",
    "\n",
    "for hdf in hdfs:\n",
    "    npl0 = np.array(hdf['layer_0']).astype('float32')\n",
    "    npl1 = np.array(hdf['layer_1']).astype('float32')\n",
    "    npl2 = np.array(hdf['layer_2']).astype('float32')\n",
    "    \n",
    "    npl0 = npl0.reshape(npl0.shape[0], -1)\n",
    "    npl1 = npl1.reshape(npl1.shape[0], -1)\n",
    "    npl2 = npl2.reshape(npl2.shape[0], -1)\n",
    "    \n",
    "    nplcats.append(np.concatenate([npl0, npl1, npl2], axis=1))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1a1478b",
   "metadata": {},
   "source": [
    "# Fit and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5302fed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nplcatscaled = []\n",
    "transformers = []\n",
    "arrmins = [[], [], []]\n",
    "epsilon = 1e-2\n",
    "\n",
    "for i in range(len(nplcats)):\n",
    "    nparr = nplcats[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    transformer = StandardScaler().fit(nparr)\n",
    "    nparr = transformer.transform(nparr)\n",
    "    transformers.append(transformer)\n",
    "    \n",
    "    nparr = np.where(np.isnan(nparr), np.inf, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        \n",
    "        if arrmin < 0 and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= arrmin\n",
    "            nparr[:, j] += epsilon\n",
    "            arrmins[i].append(arrmin)\n",
    "        else:\n",
    "            arrmins[i].append(0.)\n",
    "            \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        if arrmin < 0:\n",
    "            print(j, arrmin)\n",
    "            \n",
    "    nplcatscaled.append(nparr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4cac89a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 504)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nplcats[2].shape\n",
    "nplcatscaled[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66b30197",
   "metadata": {},
   "outputs": [],
   "source": [
    "ef_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/calo_scaled/eplus.hdf5','w')\n",
    "gf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/calo_scaled/gamma.hdf5','w')\n",
    "pf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/calo_scaled/piplus.hdf5','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cfc3995",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_scaled = [ef_scaled, gf_scaled, pf_scaled]\n",
    "layer_shapes = {}\n",
    "for key in hdf.keys():\n",
    "    if key == \"energy\" or key == \"overflow\":\n",
    "        pass\n",
    "    else:\n",
    "        layer_shapes[key] = hdf[key].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eea838e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer_0': (100000, 3, 96),\n",
       " 'layer_1': (100000, 12, 12),\n",
       " 'layer_2': (100000, 12, 6)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8726bc50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98c504da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 504)\n",
      "(100000, 288)\n",
      "(100000, 504)\n",
      "(100000, 144)\n",
      "(100000, 504)\n",
      "(100000, 72)\n",
      "(100000, 504)\n",
      "(100000, 288)\n",
      "(100000, 504)\n",
      "(100000, 144)\n",
      "(100000, 504)\n",
      "(100000, 72)\n",
      "(100000, 504)\n",
      "(100000, 288)\n",
      "(100000, 504)\n",
      "(100000, 144)\n",
      "(100000, 504)\n",
      "(100000, 72)\n"
     ]
    }
   ],
   "source": [
    "for hdf, hdf_scaled, scaled_data in zip(hdfs, hdfs_scaled, nplcatscaled):\n",
    "    offset = 0\n",
    "    for key in hdf.keys():\n",
    "        if key == \"energy\" or key == \"overflow\":\n",
    "            hdf_scaled.create_dataset(key, data=hdf[key])\n",
    "        else:\n",
    "            layer_shape = layer_shapes[key]\n",
    "            print(scaled_data.shape)\n",
    "            layer_data = scaled_data[:, offset:offset+(layer_shape[1]*layer_shape[2])]\n",
    "            print(layer_data.shape)\n",
    "            layer_data = layer_data.reshape(layer_shape)\n",
    "            hdf_scaled.create_dataset(key, data=layer_data)\n",
    "            offset += layer_shape[1]*layer_shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "839f7ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy (100000, 1) float64\n",
      "layer_0 (100000, 3, 96) float32\n",
      "layer_1 (100000, 12, 12) float32\n",
      "layer_2 (100000, 12, 6) float32\n",
      "overflow (100000, 3) float64\n",
      "energy (100000, 1) float64\n",
      "layer_0 (100000, 3, 96) float32\n",
      "layer_1 (100000, 12, 12) float32\n",
      "layer_2 (100000, 12, 6) float32\n",
      "overflow (100000, 3) float64\n",
      "energy (100000, 1) float64\n",
      "layer_0 (100000, 3, 96) float32\n",
      "layer_1 (100000, 12, 12) float32\n",
      "layer_2 (100000, 12, 6) float32\n",
      "overflow (100000, 3) float64\n"
     ]
    }
   ],
   "source": [
    "for hdf_scaled in hdfs_scaled:\n",
    "    for key in hdf_scaled.keys():\n",
    "        print(key, hdf_scaled[key].shape, hdf_scaled[key].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6c0adb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hdf_scaled in hdfs_scaled:\n",
    "    hdf_scaled.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdb31e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 504)\n",
      "(100000, 504)\n",
      "(100000, 504)\n"
     ]
    }
   ],
   "source": [
    "for nplcat in nplcatscaled:\n",
    "    print(nplcat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d24e55f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504\n"
     ]
    }
   ],
   "source": [
    "print(len(arrmins[0]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6fa95bd",
   "metadata": {},
   "source": [
    "Inverse transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55d356ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(nplcatscaled[2] == np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4beec5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = transformers[i]\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7583c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 142 -0.3723908\n",
      "0 143 0.5914335\n",
      "0 144 1.3928509\n",
      "0 353 9.338834\n",
      "0 354 9.172087\n",
      "0 365 -0.5450735\n",
      "0 366 0.036711693\n",
      "1 354 7.1371098\n",
      "1 365 -9.951949\n",
      "1 366 10.38685\n",
      "2 144 -0.120242506\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if np.abs(diff) > 0:\n",
    "            print(i, j, diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41b9b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = transformers[0].get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d77d680c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'copy': True, 'with_mean': True, 'with_std': True}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1654895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "847e10b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(transformers[0], 'scaler.gz')\n",
    "transformer = joblib.load('scaler.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d8a8a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(transformers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "536c43bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/raid/javier/Datasets/CaloVAE/data/calo_scaled/piplus_scaler.gz']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(transformers[0], '/raid/javier/Datasets/CaloVAE/data/calo_scaled/eplus_scaler.gz')\n",
    "joblib.dump(transformers[1], '/raid/javier/Datasets/CaloVAE/data/calo_scaled/gamma_scaler.gz')\n",
    "joblib.dump(transformers[2], '/raid/javier/Datasets/CaloVAE/data/calo_scaled/piplus_scaler.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97ba3de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_transformers = []\n",
    "ld_transformers.append(joblib.load('/raid/javier/Datasets/CaloVAE/data/calo_scaled/eplus_scaler.gz'))\n",
    "ld_transformers.append(joblib.load('/raid/javier/Datasets/CaloVAE/data/calo_scaled/gamma_scaler.gz'))\n",
    "ld_transformers.append(joblib.load('/raid/javier/Datasets/CaloVAE/data/calo_scaled/piplus_scaler.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a44d5128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(nparr == np.inf)\n",
    "# nplcatscaled[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f02d4084",
   "metadata": {},
   "outputs": [],
   "source": [
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.inf)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = ld_transformers[i]\n",
    "    # try:\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    # except:\n",
    "        # print(nparr)\n",
    "    \n",
    "    nparr = np.where(np.isnan(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1202926b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64837aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 143 0.5914335\n",
      "0 144 1.3928509\n",
      "0 353 9.338834\n",
      "0 354 9.172087\n",
      "0 366 0.036711693\n",
      "1 354 7.1371098\n",
      "1 366 10.38685\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if diff > 1e-4:\n",
    "            print(i, j, diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24f80582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(arrmins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d60561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(arrmins[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5df952b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ptype in enumerate([\"eplus\", \"gamma\", \"piplus\"]):\n",
    "    filepath = \"/raid/javier/Datasets/CaloVAE/data/calo_scaled/\" + ptype + \"_amin.npy\"\n",
    "    with open(filepath, 'wb') as f:\n",
    "        np.save(f, arrmins[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e00d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5574dcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(nplcatinv[0][1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "bbacae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATLAS\n",
    "#i: file number\n",
    "file_num = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "8aa177d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gf = h5py.File(f'/fast_scratch/QVAE/data/atlas/photons_samples_highStat_En_{file_num}.hdf5', 'r')\n",
    "pf = h5py.File(f'/fast_scratch/QVAE/data/atlas/pions_samples_highStat_En_{file_num}.hdf5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "1eb4cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs = [gf,pf]\n",
    "nplcats = []\n",
    "\n",
    "for hdf in hdfs:\n",
    "    npl0 = np.array(hdf['voxels'])\n",
    "    \n",
    "    npl0 = npl0.reshape(npl0.shape[0], -1)\n",
    "    \n",
    "    nplcats.append(np.concatenate([npl0], axis=1))\n",
    "    \n",
    "nplcatscaled = []\n",
    "transformers = []\n",
    "arrmins = [[], [], []]\n",
    "epsilon = 1e-2\n",
    "\n",
    "for i in range(len(nplcats)):\n",
    "    nparr = nplcats[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    transformer = StandardScaler().fit(nparr)\n",
    "    nparr = transformer.transform(nparr)\n",
    "    transformers.append(transformer)\n",
    "    \n",
    "    nparr = np.where(np.isnan(nparr), np.inf, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        \n",
    "        if arrmin < 0 and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= arrmin\n",
    "            nparr[:, j] += epsilon\n",
    "            arrmins[i].append(arrmin)\n",
    "        else:\n",
    "            arrmins[i].append(0.)\n",
    "            \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        if arrmin < 0:\n",
    "            print(j, arrmin)\n",
    "            \n",
    "    nplcatscaled.append(nparr)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "b1a0b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "gf_scaled = h5py.File(f'/fast_scratch/QVAE/data/atlas_scaled/photons_samples_highStat_En_{file_num}.hdf5', 'w')\n",
    "pf_scaled = h5py.File(f'/fast_scratch/QVAE/data/atlas_scaled/pions_samples_highStat_En_{file_num}.hdf5', 'w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "fbdb0405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########\n",
      "(99933, 368)\n",
      "(99933, 368)\n",
      "(99933, 368)\n",
      "###########\n",
      "(99999, 533)\n",
      "(99999, 533)\n",
      "(99999, 533)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hdfs_scaled = [gf_scaled, pf_scaled]\n",
    "# hdfs_scaled = [ef_scaled, gf_scaled, pf_scaled]\n",
    "layer_shapes = {}\n",
    "for hdf in hdfs:\n",
    "    for key in hdf.keys():\n",
    "        if key == \"energy\" or key == \"overflow\" or key == \"energy_from_voxels\":\n",
    "            pass\n",
    "        else:\n",
    "#             layer_shapes[key] = hdf[key].shape\n",
    "            layer_shapes[hdf] = {key : hdf[key].shape}\n",
    "        \n",
    "        \n",
    "\n",
    "for hdf, hdf_scaled, scaled_data in zip(hdfs, hdfs_scaled, nplcatscaled):\n",
    "    offset = 0\n",
    "    for key in hdf.keys():\n",
    "        if key == \"energy\" or key == \"overflow\" or key == \"energy_from_voxels\":\n",
    "            hdf_scaled.create_dataset(key, data=hdf[key])\n",
    "        else:\n",
    "            layer_shape = layer_shapes[hdf][key]\n",
    "            print(\"###########\")\n",
    "            print(layer_shape)\n",
    "            print(scaled_data.shape)\n",
    "#             layer_data = scaled_data[:, offset:offset+(layer_shape[1]*layer_shape[2])]\n",
    "            layer_data = scaled_data[:, offset:offset+layer_shape[1]]\n",
    "            print(layer_data.shape)\n",
    "            layer_data = layer_data.reshape(layer_shape)\n",
    "            hdf_scaled.create_dataset(key, data=layer_data)\n",
    "#             offset += layer_shape[1]*layer_shape[2]\n",
    "            offset += layer_shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "5726c38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy (99933, 1) float64\n",
      "energy_from_voxels (99933, 1) float64\n",
      "voxels (99933, 368) float64\n",
      "energy (99999, 1) float64\n",
      "energy_from_voxels (99999, 1) float64\n",
      "voxels (99999, 533) float64\n"
     ]
    }
   ],
   "source": [
    "for hdf_scaled in hdfs_scaled:\n",
    "    for key in hdf_scaled.keys():\n",
    "        print(key, hdf_scaled[key].shape, hdf_scaled[key].dtype)\n",
    "        \n",
    "for hdf_scaled in hdfs_scaled:\n",
    "    hdf_scaled.close()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "501554c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform\n",
    "\n",
    "\n",
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = transformers[i]\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)\n",
    "\n",
    "\n",
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if diff > 0:\n",
    "            print(i, j, diff)\n",
    "            \n",
    "params = transformers[0].get_params()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "87516174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gf_scaled = h5py.File(f'/fast_scratch/QVAE/data/atlas_scaled/photons_samples_highStat_En_{file_num}.hdf5', 'w')\n",
    "#pf_scaled = h5py.File(f'/fast_scratch/QVAE/data/atlas_scaled/pions_samples_highStat_En_{file_num}.hdf5', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "04a70918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/fast_scratch/QVAE/data/atlas_scaled/pions_samples_highStat_En_5_scaler.gz']"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# joblib.dump(transformers[0], 'scaler.gz')\n",
    "# transformer = joblib.load('scaler.gz')\n",
    "\n",
    "joblib.dump(transformers[0], f'/fast_scratch/QVAE/data/atlas_scaled/photons_samples_highStat_En_{file_num}_scaler.gz')\n",
    "joblib.dump(transformers[1], f'/fast_scratch/QVAE/data/atlas_scaled/pions_samples_highStat_En_{file_num}_scaler.gz')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "046d55d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_transformers = []\n",
    "ld_transformers.append(joblib.load(f'/fast_scratch/QVAE/data/atlas_scaled/photons_samples_highStat_En_{file_num}_scaler.gz'))\n",
    "ld_transformers.append(joblib.load(f'/fast_scratch/QVAE/data/atlas_scaled/pions_samples_highStat_En_{file_num}_scaler.gz'))\n",
    "\n",
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.inf)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = ld_transformers[i]\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    \n",
    "    nparr = np.where(np.isnan(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "80f668dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if diff > 1e-4:\n",
    "            print(i, j, diff)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "20007145",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ptype in enumerate([f'photons_samples_highStat_En_{file_num}', f'pions_samples_highStat_En_{file_num}']):\n",
    "    filepath = f'/fast_scratch/QVAE/data/atlas_scaled/' + ptype + \"_amin.npy\"\n",
    "    with open(filepath, 'wb') as f:\n",
    "        np.save(f, arrmins[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aadb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATLAS ds 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5691b28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd5dff03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gf = h5py.File(f'/fast_scratch/QVAE/data/atlas_dataset2and3/dataset_2_1.hdf5', 'r')\n",
    "# pf = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas/pions_samples_highStat_En_5.hdf5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97be32ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hdfs = [gf]\n",
    "nplcats = []\n",
    "\n",
    "for hdf in hdfs:\n",
    "    npl0 = np.array(hdf['showers'])\n",
    "    \n",
    "    npl0 = npl0.reshape(npl0.shape[0], -1)\n",
    "    \n",
    "    nplcats.append(np.concatenate([npl0], axis=1))\n",
    "    \n",
    "nplcatscaled = []\n",
    "transformers = []\n",
    "arrmins = [[], [], []]\n",
    "epsilon = 1e-2\n",
    "\n",
    "for i in range(len(nplcats)):\n",
    "    nparr = nplcats[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    transformer = StandardScaler().fit(nparr)\n",
    "    nparr = transformer.transform(nparr)\n",
    "    transformers.append(transformer)\n",
    "    \n",
    "    nparr = np.where(np.isnan(nparr), np.inf, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        \n",
    "        if arrmin < 0 and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= arrmin\n",
    "            nparr[:, j] += epsilon\n",
    "            arrmins[i].append(arrmin)\n",
    "        else:\n",
    "            arrmins[i].append(0.)\n",
    "            \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = np.amin(nparr[:, j])\n",
    "        if arrmin < 0:\n",
    "            print(j, arrmin)\n",
    "            \n",
    "    nplcatscaled.append(nparr)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "686710a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas_dataset2and3_scaled/dataset_2_1.hdf5', 'w')\n",
    "# pf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas_scaled/pions_samples_highStat_En_5.hdf5', 'w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d7553fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########\n",
      "(100000, 6480)\n",
      "(100000, 6480)\n",
      "(100000, 6480)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hdfs_scaled = [gf_scaled]\n",
    "# hdfs_scaled = [ef_scaled, gf_scaled, pf_scaled]\n",
    "layer_shapes = {}\n",
    "for hdf in hdfs:\n",
    "    for key in hdf.keys():\n",
    "        if key == \"energy\" or key == \"overflow\" or key == \"energy_from_voxels\":\n",
    "            pass\n",
    "        else:\n",
    "#             layer_shapes[key] = hdf[key].shape\n",
    "            layer_shapes[hdf] = {key : hdf[key].shape}\n",
    "        \n",
    "        \n",
    "\n",
    "for hdf, hdf_scaled, scaled_data in zip(hdfs, hdfs_scaled, nplcatscaled):\n",
    "    offset = 0\n",
    "    for key in hdf.keys():\n",
    "        if key == \"energy\" or key == \"overflow\" or key == \"energy_from_voxels\" or key == \"incident_energies\":\n",
    "            hdf_scaled.create_dataset(key, data=hdf[key])\n",
    "        else:\n",
    "            layer_shape = layer_shapes[hdf][key]\n",
    "            print(\"###########\")\n",
    "            print(layer_shape)\n",
    "            print(scaled_data.shape)\n",
    "#             layer_data = scaled_data[:, offset:offset+(layer_shape[1]*layer_shape[2])]\n",
    "            layer_data = scaled_data[:, offset:offset+layer_shape[1]]\n",
    "            print(layer_data.shape)\n",
    "            layer_data = layer_data.reshape(layer_shape)\n",
    "            hdf_scaled.create_dataset(key, data=layer_data)\n",
    "#             offset += layer_shape[1]*layer_shape[2]\n",
    "            offset += layer_shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5135663",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incident_energies (100000, 1) float64\n",
      "showers (100000, 6480) float64\n"
     ]
    }
   ],
   "source": [
    "for hdf_scaled in hdfs_scaled:\n",
    "    for key in hdf_scaled.keys():\n",
    "        print(key, hdf_scaled[key].shape, hdf_scaled[key].dtype)\n",
    "        \n",
    "for hdf_scaled in hdfs_scaled:\n",
    "    hdf_scaled.close()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44ac21e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inverse transform\n",
    "\n",
    "\n",
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = transformers[i]\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    \n",
    "    nparr = np.where(np.isinf(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)\n",
    "\n",
    "\n",
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if diff > 0:\n",
    "            print(i, j, diff)\n",
    "            \n",
    "params = transformers[0].get_params()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4666eeda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas_dataset2and3_scaled/dataset_2_1.hdf5', 'w')\n",
    "# pf_scaled = h5py.File('/raid/javier/Datasets/CaloVAE/data/atlas_scaled/pions_samples_highStat_En_5.hdf5', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e33c78d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/raid/javier/Datasets/CaloVAE/data/atlas_dataset2and3_scaled/dataset_2_1_scaler.gz']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# joblib.dump(transformers[0], 'scaler.gz')\n",
    "# transformer = joblib.load('scaler.gz')\n",
    "\n",
    "joblib.dump(transformers[0], '/raid/javier/Datasets/CaloVAE/data/atlas_dataset2and3_scaled/dataset_2_1_scaler.gz')\n",
    "# joblib.dump(transformers[1], '/raid/javier/Datasets/CaloVAE/data/atlas_scaled/pions_samples_highStat_En_5_scaler.gz')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2feddc04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ld_transformers = []\n",
    "ld_transformers.append(joblib.load('/raid/javier/Datasets/CaloVAE/data/atlas_dataset2and3_scaled/dataset_2_1_scaler.gz'))\n",
    "# ld_transformers.append(joblib.load('/raid/javier/Datasets/CaloVAE/data/atlas_scaled/pions_samples_highStat_En_5_scaler.gz'))\n",
    "\n",
    "nplcatinv = []\n",
    "\n",
    "for i in range(len(nplcatscaled)):\n",
    "    nparr = nplcatscaled[i]\n",
    "    # nparr = np.where(nparr > 0., nparr, np.inf)\n",
    "    nparr = np.where(nparr > 0., nparr, np.nan)\n",
    "    \n",
    "    for j in range(nparr.shape[1]):\n",
    "        arrmin = arrmins[i][j]\n",
    "        if arrmin < 0. and not np.isnan(arrmin):\n",
    "            nparr[:, j] -= epsilon\n",
    "            nparr[:, j] += arrmin\n",
    "            \n",
    "    transformer = ld_transformers[i]\n",
    "    nparr = transformer.inverse_transform(nparr)\n",
    "    \n",
    "    # nparr = np.where(np.isnan(nparr), 0, nparr)\n",
    "    nparr = np.where(np.isnan(nparr), 0, nparr)\n",
    "    nplcatinv.append(nparr)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "204880a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(nplcatinv)):\n",
    "    nparrorig = nplcats[i]\n",
    "    nparrinv = nplcatinv[i]\n",
    "    \n",
    "    for j in range(nparrorig.shape[1]):\n",
    "        diff = np.sum(nparrorig[:, j] - nparrinv[:, j])\n",
    "        if diff > 1e-4:\n",
    "            print(i, j, diff)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45ab1e36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, ptype in enumerate([\"dataset_2_1\"]):\n",
    "    filepath = \"/raid/javier/Datasets/CaloVAE/data/atlas_dataset2and3_scaled/\" + ptype + \"_amin.npy\"\n",
    "    with open(filepath, 'wb') as f:\n",
    "        np.save(f, arrmins[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb200e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
