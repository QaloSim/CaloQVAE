#testing

import torch
import torch.nn as nn
import torch.nn.functional as F
from models.networks.basicCoders import BasicDecoderV3

class PeriodicConvTranspose2d(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):
        super(PeriodicConvTranspose2d, self).__init__()
        self.padding = padding
        self.conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=0, dilation=dilation, groups=groups, bias=bias)

    def forward(self, x):
        # Pad input tensor with periodic boundary conditions
        x = F.pad(x, (self.padding, self.padding, self.padding, self.padding), mode='circular')
        # Apply convolution
        x = self.conv(x)
        return x

class sequentialMultiInput(nn.Sequential):
    def forward(self, *inputs):
        for module in self._modules.values():
            if type(inputs) == tuple:
                inputs = module(*inputs)
            else:
                inputs = module(inputs)
        return inputs

class DecoderCNNPBv4_HEMOD(BasicDecoderV3):
    def __init__(self, num_input_nodes, num_output_nodes, output_activation_fct=nn.Identity(), **kwargs):
        super(DecoderCNNPBv4_HEMOD, self).__init__(**kwargs)
        self._output_activation_fct = output_activation_fct
        self.num_input_nodes = num_input_nodes
        self.z = 45
        self.r = 9
        self.phi = 16
        self.hierarchal_outputs = num_output_nodes
        self.output_layers = int(self.hierarchal_outputs / 144)
        
        # self._node_sequence = [(2049, 800), (800, 700), (700, 600), (600, 550), (550, 500), (500, 6480)]
        self._layers =  nn.Sequential(
                   # nn.Unflatten(1, (self._node_sequence[0][0]-1, 1,1)),
                   nn.Unflatten(1, (self.num_input_nodes, 1,1)),

                   PeriodicConvTranspose2d(self.num_input_nodes, 1024, (3,5), 2, 0),
                   nn.BatchNorm2d(1024),
                   nn.PReLU(1024, 0.02),
                   

                   PeriodicConvTranspose2d(1024, 512, (3,5), 1, 0),
                   nn.BatchNorm2d(512),
                   nn.PReLU(512, 0.02),
                                   )
        
        self._layers2 = nn.Sequential(
                   PeriodicConvTranspose2d(513, 128, (3,5), 1, 0),
                   nn.BatchNorm2d(128),
                   nn.PReLU(128, 0.02),

                   PeriodicConvTranspose2d(128, self.output_layers, (3,4), 1, 0),
                   # nn.BatchNorm2d(45),
                   nn.PReLU(self.output_layers, 1.0),
                                   )
        
        self._layers3 = nn.Sequential(
                   PeriodicConvTranspose2d(513, 128, (3,5), 1, 0),
                   nn.BatchNorm2d(128),
                   nn.PReLU(128, 0.02),

                   PeriodicConvTranspose2d(128, self.output_layers, (3,4), 1, 0),
                   # nn.BatchNorm2d(45),
                   nn.PReLU(self.output_layers, 0.02),
                                   )
        
    def forward(self, x, x0):
        print("t1: ", x.shape)
        x = self._layers(x)
        print("t2: ", x.shape)
        x0 = self.trans_energy(x0)
        xx0 = torch.cat((x, x0.unsqueeze(2).unsqueeze(3).repeat(1,1,torch.tensor(x.shape[-2:-1]).item(), torch.tensor(x.shape[-1:]).item())), 1)
        x1 = self._layers2(xx0)
        x2 = self._layers3(xx0)
        # need channels * height * width = self.hierarchal_outputs = 1620
        return x1.reshape(x1.shape[0], self.hierarchal_outputs), x2.reshape(x1.shape[0], self.hierarchal_outputs)
    
    def trans_energy(self, x0, log_e_max=14.0, log_e_min=6.0, s_map = 15 * 1.2812657528661318):
        # s_map = max(scaled voxel energy u_i) * (incidence energy / slope of total energy in shower) of the dataset
        return ((torch.log(x0) - log_e_min)/(log_e_max - log_e_min)) * s_map

class DecoderCNNPB_HEv1(BasicDecoderV3):
    def __init__(self, encArch = 'Large', **kwargs):
        self.encArch = encArch
        super(DecoderCNNPB_HEv1, self).__init__(**kwargs)

    def _create_hierarchy_network(self, level: int = 0):
        self.latent_nodes = 2048
        self.layer_step = 11*144
        self.hierarchiel_lvls = 4

        inp_layers = [self.latent_nodes + i * self.layer_step for i in range(self.hierarchiel_lvls)] 
        out_layers = 4 * [self.layer_step]
        out_layers[3] += (6480 - 4 * self.layer_step)

        self.moduleLayers = nn.ModuleList([])
        for i in range(len(inp_layers)):
            self.moduleLayers.append(DecoderCNNPBv4_HEMOD(inp_layers[i], out_layers[i]))
            
        sequential = sequentialMultiInput(*self.moduleLayers)
        return sequential
    
    def forward(self, x, x0):
        self._create_hierarchy_network()
        self.sub_values = []
        x1, x2 = torch.tensor([]), torch.tensor([])
        for lvl in range(self.hierarchiel_lvls):
            cur_net = self.moduleLayers[lvl]
            hits, acts = cur_net(x, x0)
            beta = torch.tensor(self._config.model.output_smoothing_fct, dtype=torch.float, device=output_hits.device, requires_grad=False)
            out.output_activations = self._energy_activation_fct(output_activations) * self._hit_smoothing_dist_mod(output_hits, beta, is_training)
            z = out.output_activations
            self.sub_values.append([hits, acts])
            if lvl == self.hierarchiel_lvls - 1:
                for vals in self.sub_values:
                    x1 = torch.cat((x1, vals[0]), dim=1)
                    x2 = torch.cat((x2, vals[1]), dim=1)
            else:
                x = torch.cat((x, z), dim=1)
        return x1, x2


class PeriodicConv2d(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):
        super(PeriodicConv2d, self).__init__()
        self.padding = padding
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=0, dilation=dilation, groups=groups, bias=bias)

    def forward(self, x):
        # Pad input tensor with periodic boundary conditions
        x = F.pad(x, (self.padding, self.padding, 0, 0), mode='circular')
        # Apply convolution
        x = self.conv(x)
        return x


class PeriodicConv3d(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):
        super(PeriodicConv3d, self).__init__()
        self.padding = padding
        # try 3x3x3 cubic convolution
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=0, dilation=dilation, groups=groups, bias=bias)
    def forward(self, x):
        # Pad input tensor with periodic boundary and circle-center conditions
        if self.padding == 1:
            mid = x.shape[-1] // 2
            shift = torch.cat((x[..., [-1], mid:], x[..., [-1], :mid]), -1)
            x = torch.cat((x, shift), dim=-2)
        x = F.pad(x, (self.padding, self.padding, 0, 0, 0, 0), mode='circular')
        # Apply convolution
        x = self.conv(x)
        return x


class PeriodicConv3dBlock(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):
        super(PeriodicConv3d, self).__init__()
        self.padding = padding
        # try 3x3x3 cubic convolution
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=0, dilation=dilation, groups=groups, bias=bias)
    def forward(self, x):
        # Pad input tensor with periodic boundary conditions
        x = F.pad(x, (self.padding, self.padding, 0, 0, 0, 0), mode='circular')
        # Apply convolution
        x = self.conv(x)
        
        return x


data = torch.rand([32, 1, 45, 9, 16])
print(data.shape)


# why no pooling
conv1 = nn.Sequential(
    PeriodicConv3d(1, 32, (3,3,3), (2,1,1), 1),
    PeriodicConv3d(32, 64, (3,3,3), (2,1,1), 1),
    # nn.BatchNorm3d(64),
    # nn.PReLU(64, 0.02),
    PeriodicConv3d(64, 128, (3,3,3), (1,1,2), 0),
    PeriodicConv3d(128, 256, (3,3,3), (2,1,2), 0),
    PeriodicConv3d(256, 512, (3,3,3), (1,1,1), 0),
)
# conv2 = PeriodicConv3d(16, 32, 3, 1, 1)
# conv3 = PeriodicConv3d(32, 64, 3, 1, 1)
# xt = conv1(data)
xt = data

print(conv1)
print("Input Shape: ", xt.shape)
for layer in conv1:
    xt = layer(xt)
    print("Cur Shape: ", xt.shape)

# xt = conv3(conv2(conv1(data)))
# shape of 3d data (batch_size, depth, height, width) 
# width gets padded so 3, 3, 5 in this example
# print(xt.shape)
#torch.Size([32, 128, 29, 1, 8])


class PeriodicConvTranspose3d(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):
        super(PeriodicConvTranspose3d, self).__init__()
        self.padding = padding
        self.conv = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=0, dilation=dilation, groups=groups, bias=bias)

    def forward(self, x):
        # Pad input tensor with periodic boundary conditions
        # if self.padding == 1:
        #     mid = x.shape[-1] // 2
        #     shift = torch.cat((x[..., [-1], mid:], x[..., [-1], :mid]), -1)
        #     x = torch.cat((x, shift), dim=-2)
        x = F.pad(x, (self.padding, self.padding, 0, 0, 0, 0), mode='circular')
        # Apply convolution
        x = self.conv(x)
        return x


class DecoderCNNPBv4(BasicDecoderV3):
    def __init__(self, output_activation_fct=nn.Identity(),num_output_nodes=368, **kwargs):
        super(DecoderCNNPBv4, self).__init__(**kwargs)
        self._output_activation_fct=output_activation_fct
        self.num_output_nodes = num_output_nodes
        self.z = 45
        self.r = 9
        self.phi = 16

        # self.n_latent_nodes = self._config.model.n_latent_nodes
        # self.n_latent_nodes = self._config.model.n_latent_nodes_per_p * 4
        self.n_latent_nodes = 302 * 4
        
        # self._node_sequence = [(2049, 800), (800, 700), (700, 600), (600, 550), (550, 500), (500, 6480)]
        self._layers =  nn.Sequential(
                   # nn.Unflatten(1, (self._node_sequence[0][0]-1, 1,1)),
                   nn.Unflatten(1, (self.n_latent_nodes, 1,1)),

                   PeriodicConvTranspose2d(self.n_latent_nodes, 1024, (3,5), 2, 0),
                   nn.BatchNorm2d(1024),
                   nn.PReLU(1024, 0.02),
                   

                   PeriodicConvTranspose2d(1024, 512, (3,4), 1, 0),
                   nn.BatchNorm2d(512),
                   nn.PReLU(512, 0.02),
                                   )
        
        self._layers2 = nn.Sequential(
                   PeriodicConvTranspose2d(513, 128, (3,3), 1, 1),
                   nn.BatchNorm2d(128),
                   nn.PReLU(128, 0.02),

                   PeriodicConvTranspose2d(128, 45, (3,3), 1, 1),
                   # nn.BatchNorm2d(45),
                   nn.PReLU(45, 1.0),
                                   )
        
        self._layers3 = nn.Sequential(
                   PeriodicConvTranspose2d(513, 128, (3,3), 1, 1),
                   nn.BatchNorm2d(128),
                   nn.PReLU(128, 0.02),

                   PeriodicConvTranspose2d(128, 45, (3,3), 1, 1),
                   # nn.BatchNorm2d(45),
                   nn.PReLU(45, 0.02),
                                   )


convt = nn.Sequential(
    # PeriodicConvTranspose3d(1, 64, (5,3,5), (2,1,1), 1),
    # nn.BatchNorm3d(64),
    # nn.PReLU(64, 0.02),
    # PeriodicConvTrnaspose3d(64, 128, (5,3,3), (2,1,2), 1),
    # PeriodicConvTranspose3d(128, 256, (5,3,3), (2,1,2), 0),
    # PeriodicConvTranspose3d(256, 512, (3,3,3), (1,1,1), 0),

    nn.Unflatten(1, (1208, 1, 1, 1)),

    PeriodicConvTranspose3d(1208, 512, (3,2,3), (2,1,1), 0),
   
    PeriodicConvTranspose3d(512, 128, (5,3,3), (2,1,1), 0),

    PeriodicConvTranspose3d(128, 64, (3,2,3), (2,1,1), 0),

    PeriodicConvTranspose3d(64, 32, (5,3,3), (2,1,2), 0),

    PeriodicConvTranspose3d(32, 1, (5,3,2), (1,1,1), 0),

    PeriodicConv3d(1, 1, (45 - 5 + 1, 1, 1), (1,1,1), 0)
    
)

# xx0 = torch.cat((x, x0.unsqueeze(2).unsqueeze(3).repeat(1,1,torch.tensor(x.shape[-2:-1]).item(), torch.tensor(x.shape[-1:]).item())), 1)
xt = torch.rand([32, 1208])

print(convt)
print("Input Shape: ", xt.shape)
for layer in convt:
    xt = layer(xt)
    print("Cur Shape: ", xt.shape)

# xt = conv3(conv2(conv1(data)))
# shape of 3d data (batch_size, depth, height, width) 
# width gets padded so 3, 3, 5 in this example
# print(xt.shape)
#torch.Size([32, 128, 29, 1, 8])


x = torch.tensor(([[[[[0,1,2,3,4,5,6,7,8,9], [10,11,12,13,14,15,16,17,18,19]], [[0,1,2,3,4,5,6,7,8,9], [10,11,12,13,14,15,16,17,18,19]]]]]))
print(x, x.shape)
# x = F.pad(x, (1, 1, 0, 0, 0, 0), mode='circular')
print(x, x.shape)
# x = F.pad(x, (1, 1, 0, 0, 0, 0), mode='circular')
print(x, x.shape)


print(x[...,-1])


print(x)
print(x.shape[-1])
mid = x.shape[-1] // 2
shift = torch.cat((x[..., [-1], mid:], x[..., [-1], :mid]), -1)
testx = torch.cat((x, shift), dim=-2)
print(shift)
print(testx)
testx = F.pad(testx, (1, 1, 0, 0, 0, 0), mode='circular')
print(testx)


testc = F.CircularPad2d(x, (1,1,0,1,0,0))


# need to load config?
testDec = DecoderCNNPB_HEv1()
print(testDec.encArch)
x = torch.rand([1, 2048])
x0 = torch.rand([1, 1])
res = testDec.forward(x, x0)
print(len(res))
print(res[0].shape, res[1].shape)



test = torch.tensor(([[[1,2,3], [4,5,6]]]))
print(test, test.shape)
test = F.pad(test, (0, 0, 1, 1), mode = 'circular')
print(test, test.shape)


inp_layers = [1,2,3,4]
x = [0] + [layers - 1 for layers in inp_layers]
print(x)


test = [1 + 1 * x for x in range(6)]
print(test)


from torchvision import models
from torchsummary import summary
from DecoderCond import DecoderCNNPBv4


enc = DecoderCNNPBv4()
# takes parameters from the forward method
summary(enc, [32, 1208], [32, 1])


!source source.me



