import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

def sample_gumbel(shape, eps=1e-20):
    U = torch.rand(shape)
    return -torch.log(-torch.log(U + eps) + eps)

def gumbel_softmax(logits, temperature=1.0, hard=False):
    g = sample_gumbel(logits.size())
    y = logits + g
    y = y / temperature
    y = F.softmax(y, dim=-1)
    if hard:
        # Straight-through
        max_idx = torch.argmax(y, dim=-1, keepdim=True)
        y_hard = torch.zeros_like(y).scatter_(-1, max_idx, 1.0)
        y = (y_hard - y).detach() + y
    return y

class GumbelBinaryAutoencoder(nn.Module):
    def __init__(self, input_dim=16, hidden_dim=8):
        super().__init__()
        self.hidden_dim = hidden_dim
        
        # Larger encoder: add an extra layer
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, hidden_dim * 2)  # 2 logits per latent dimension
        )
        
        # Larger decoder: add an extra layer
        self.decoder = nn.Sequential(
            nn.Linear(hidden_dim * 2, 32),
            nn.ReLU(),
            nn.Linear(32, 64),
            nn.ReLU(),
            nn.Linear(64, input_dim),
            nn.Sigmoid()
        )

    def forward(self, x, temp=1.0, hard=False):
        logits = self.encoder(x)
        logits = logits.view(-1, self.hidden_dim, 2)
        z = gumbel_softmax(logits, temperature=temp, hard=hard)
        z = z.view(-1, self.hidden_dim * 2)
        return self.decoder(z)


def testing1():
    input_dim, hidden_dim = 16, 8
    epochs, batch_size, lr = 100, 32, 1e-3
    # Define temperature schedule
    temp_start = 5.0    # start with a higher temperature (more “soft”)
    temp_end   = 0.5    # end with a lower temperature (more “discrete”)
    temp_decay = (temp_end / temp_start) ** (1.0 / (epochs - 1))

    X = torch.rand(1000, input_dim)
    data_loader = torch.utils.data.DataLoader(X, batch_size=batch_size, shuffle=True)

    model = GumbelBinaryAutoencoder(input_dim, hidden_dim)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.MSELoss()

    temp = temp_start
    for epoch in range(epochs):
        total_loss = 0
        for batch in data_loader:
            out = model(batch, temp=temp, hard=True)
            loss = criterion(out, batch)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item() * len(batch)
        
        avg_loss = total_loss / len(X)
        print(f"Epoch [{epoch+1}/{epochs}], Temp: {temp:.3f}, Loss: {avg_loss:.6f}")
        
        # Anneal temperature
        temp = max(temp_end, temp * temp_decay)

    # Quick test
    with torch.no_grad():
        test_samples = X[:5]
        reconstructed = model(test_samples, temp=1.0, hard=True)
        print("Original:\n", test_samples)
        print("Reconstructed:\n", reconstructed)



testing1()


def testing2():
    input_dim, hidden_dim = 16, 8
    epochs, batch_size, lr = 10, 32, 1e-3
    # Define temperature schedule
    temp_start = 5.0    # start with a higher temperature (more “soft”)
    temp_end   = 0.5    # end with a lower temperature (more “discrete”)
    temp_decay = (temp_end / temp_start) ** (1.0 / (epochs - 1))

    X = torch.rand(1000, input_dim)
    data_loader = torch.utils.data.DataLoader(X, batch_size=batch_size, shuffle=True)

    model = GumbelBinaryAutoencoder(input_dim, hidden_dim)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.MSELoss()

    temp = temp_start
    for epoch in range(epochs):
        total_loss = 0
        for batch in data_loader:
            out = model(batch, temp=temp, hard=True)
            print(out.shape)
            loss = criterion(out, batch)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item() * len(batch)
        
        avg_loss = total_loss / len(X)
        print(f"Epoch [{epoch+1}/{epochs}], Temp: {temp:.3f}, Loss: {avg_loss:.6f}")
        
        # Anneal temperature
        temp = max(temp_end, temp * temp_decay)

    # Quick test
    with torch.no_grad():
        test_samples = X[:5]
        reconstructed = model(test_samples, temp=1.0, hard=True)
        print("Original:\n", test_samples)
        print("Reconstructed:\n", reconstructed)




